{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 10 code\n",
    "icd10_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd10 = icd10_df['subject_id'].unique()\n",
    "num_patients_with_icd10 = len(unique_patients_with_icd10)\n",
    "print(\"Number of patients with at least one ICD-10 code:\", num_patients_with_icd10)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 10 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd10 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd10 = len(patients_with_only_icd10)\n",
    "print(\"Number of patients with only ICD-10 codes:\", num_patients_only_icd10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id').size()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3]\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "#\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "splits = []\n",
    "\n",
    "for visits in patient_visits:\n",
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "        splits.append(split_index)\n",
    "        features.append([visit for visit in visits[:split_index]])\n",
    "        labels.append([code for sublist in visits[split_index:] for code in sublist])\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Split Index: \", splits[:10])\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"Example Labels: \" , labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "class ICDCodeDataset(Dataset):\n",
    "    def __init__(self, features, labels, tokenizer, max_len=512):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Encode features\n",
    "        feature_encoded = self.tokenizer.encode_plus(\n",
    "            ' '.join(self.features[index]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Encode labels\n",
    "        labels_encoded = self.tokenizer.encode_plus(\n",
    "            ' '.join(self.labels[index]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': feature_encoded['input_ids'].flatten(),\n",
    "            'attention_mask': feature_encoded['attention_mask'].flatten(),\n",
    "            'labels': labels_encoded['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "#initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#create dataset\n",
    "dataset = ICDCodeDataset(features, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "#train and test split\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "test_size = total_samples - train_size \n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "print(\"FULL Dataset: {}\".format(len(dataset)))\n",
    "print(\"TRAIN Dataset: {}\".format(len(train_dataset)))\n",
    "print(\"TEST Dataset: {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 100,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': 1042, # number of disease + symbols for word embedding\n",
    "    'hidden_size': 300, # word embedding and seg embedding hidden size the embeddings we get are 300 in length\n",
    "    # 'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    # 'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.get_embeddings import ICD9Embeddings\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups \n",
    "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        \n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None,):\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "\n",
    "        # TODO how to combine the embeddings of all the words\n",
    "        word_embed = self.icd9_embeddings(word_ids)\n",
    "\n",
    "        # TODO how to pass in positional embeddings?\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "    \n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(input_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### SETUP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
