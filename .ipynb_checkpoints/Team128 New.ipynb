{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b878e2c7-b91a-4349-8fde-50dd70ff6e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lzfmft212pbcxkk1qdsptfpc0000gn/T/ipykernel_50033/2534502054.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mskm\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ff7943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'], dtype='object')\n",
      "Patient ID: 10002769\n",
      "num of visits for patient:  2\n",
      "\t0-th visit id: 28314592\n",
      "\t0-th visit diagnosis codes: ['45342', '70713', '45981', '4019', '2724', 'V1251']\n",
      "\t0-th visit diagnosis short titles: ['Ac DVT/emb distl low ext', 'Ulcer of ankle', 'Venous insufficiency NOS', 'Hypertension NOS', 'Hyperlipidemia NEC/NOS', 'Hx-ven thrombosis/embols']\n",
      "\t1-th visit id: 25681387\n",
      "\t1-th visit diagnosis codes: ['45981', '70713', '4019', '2720', 'V1251', '4928', 'V1582', 'V113']\n",
      "\t1-th visit diagnosis short titles: ['Venous insufficiency NOS', 'Ulcer of ankle', 'Hypertension NOS', 'Pure hypercholesterolem', 'Hx-ven thrombosis/embols', 'Emphysema NEC', 'History of tobacco use', 'Hx of alcoholism']\n"
     ]
    }
   ],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3d9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with icd_version = 10: 1989449\n",
      "Number of rows with icd_version = 9: 2766877\n",
      "Number of unique ICD-9 codes: 9072\n",
      "Number of unique ICD-10 codes: 16757\n",
      "Number of patients with at least one ICD-10 code: 124550\n",
      "Number of patients with both ICD-9 and ICD-10 codes: 24123\n",
      "Number of patients with only ICD-10 codes: 100427\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 10 code\n",
    "icd10_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd10 = icd10_df['subject_id'].unique()\n",
    "num_patients_with_icd10 = len(unique_patients_with_icd10)\n",
    "print(\"Number of patients with at least one ICD-10 code:\", num_patients_with_icd10)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 10 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd10 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd10 = len(patients_with_only_icd10)\n",
    "print(\"Number of patients with only ICD-10 codes:\", num_patients_only_icd10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2eb469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with only ICD-9 codes and more than 3 visits: 83227\n"
     ]
    }
   ],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id').size()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3]\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f6f1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Index:  [2, 2, 2, 2, 6, 2, 2, 2, 2, 2]\n",
      "Example Features:  [['5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582'], ['07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051']]\n",
      "Example Labels:  ['07054', '78959', 'V462', '5715', '2767', '2761', '496', 'V08', '3051', '78791', '45829', '07044', '7994', '2761', '78959', '2767', '3051', 'V08', 'V4986', 'V462', '496', '29680', '5715']\n"
     ]
    }
   ],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "#\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "splits = []\n",
    "\n",
    "for visits in patient_visits:\n",
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "        splits.append(split_index)\n",
    "        features.append([visit for visit in visits[:split_index]])\n",
    "        labels.append([code for sublist in visits[split_index:] for code in sublist])\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Split Index: \", splits[:10])\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"Example Labels: \" , labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef340551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "class ICDCodeDataset(Dataset):\n",
    "    def __init__(self, features, labels, tokenizer, max_len=512):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Encode features\n",
    "        feature_encoded = self.tokenizer.encode_plus(\n",
    "            ' '.join(self.features[index]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Encode labels\n",
    "        labels_encoded = self.tokenizer.encode_plus(\n",
    "            ' '.join(self.labels[index]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': feature_encoded['input_ids'].flatten(),\n",
    "            'attention_mask': feature_encoded['attention_mask'].flatten(),\n",
    "            'labels': labels_encoded['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "#initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#create dataset\n",
    "dataset = ICDCodeDataset(features, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87631706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: 27873\n",
      "TRAIN Dataset: 22298\n",
      "TEST Dataset: 5575\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "#train and test split\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "test_size = total_samples - train_size \n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "print(\"FULL Dataset: {}\".format(len(dataset)))\n",
    "print(\"TRAIN Dataset: {}\".format(len(train_dataset)))\n",
    "print(\"TEST Dataset: {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5873138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7ee9ab-3825-43e3-b698-d21591a26c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb0169a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 100,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': 1042, # number of disease + symbols for word embedding\n",
    "    'hidden_size': 300, # word embedding and seg embedding hidden size the embeddings we get are 300 in length\n",
    "    # 'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    # 'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa11de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "543344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.get_embeddings import ICD9Embeddings\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups \n",
    "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        \n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None,):\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "\n",
    "        # TODO how to combine the embeddings of all the words\n",
    "        word_embed = self.icd9_embeddings(word_ids)\n",
    "\n",
    "        # TODO how to pass in positional embeddings?\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "    \n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(input_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498eae7",
   "metadata": {},
   "source": [
    "### SETUP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba47b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e824f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
