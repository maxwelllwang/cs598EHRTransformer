{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lzfmft212pbcxkk1qdsptfpc0000gn/T/ipykernel_50276/3580728234.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/sinja/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sinja/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_pretrained_bert\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mBert\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import json\n",
    "import pytorch_pretrained_bert as Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'], dtype='object')\n",
      "Patient ID: 10002769\n",
      "num of visits for patient:  2\n",
      "\t0-th visit id: 28314592\n",
      "\t0-th visit diagnosis codes: ['45342', '70713', '45981', '4019', '2724', 'V1251']\n",
      "\t0-th visit diagnosis short titles: ['Ac DVT/emb distl low ext', 'Ulcer of ankle', 'Venous insufficiency NOS', 'Hypertension NOS', 'Hyperlipidemia NEC/NOS', 'Hx-ven thrombosis/embols']\n",
      "\t1-th visit id: 25681387\n",
      "\t1-th visit diagnosis codes: ['45981', '70713', '4019', '2720', 'V1251', '4928', 'V1582', 'V113']\n",
      "\t1-th visit diagnosis short titles: ['Venous insufficiency NOS', 'Ulcer of ankle', 'Hypertension NOS', 'Pure hypercholesterolem', 'Hx-ven thrombosis/embols', 'Emphysema NEC', 'History of tobacco use', 'Hx of alcoholism']\n"
     ]
    }
   ],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with icd_version = 10: 1989449\n",
      "Number of rows with icd_version = 9: 2766877\n",
      "Number of unique ICD-9 codes: 9072\n",
      "Number of unique ICD-10 codes: 16757\n",
      "Number of patients with at least one ICD-9 code: 124550\n",
      "Number of patients with both ICD-9 and ICD-10 codes: 24123\n",
      "Number of patients with only ICD-9 codes: 100427\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 9 code\n",
    "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
    "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
    "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 9 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
    "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with only ICD-9 codes and more than 3 visits: 11073\n"
     ]
    }
   ],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique truncated codes: 1042\n"
     ]
    }
   ],
   "source": [
    "#Creates Token Vocabulary\n",
    "\n",
    "\n",
    "import json\n",
    "# TODO i need full vocabulary     \n",
    "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
    "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
    "\n",
    "# Define special tokens with a specific order\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
    "\n",
    "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
    "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
    "\n",
    "# Print the number of unique codes to verify\n",
    "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
    "\n",
    "# Print token to index mapping\n",
    "#print(\"Token to Index Mapping:\", token2idx)\n",
    "\n",
    "# Save the token2idx dictionary to a JSON file for later use\n",
    "with open('token2idx.json', 'w') as f:\n",
    "    json.dump(token2idx, f)\n",
    "\n",
    "    # for labels just make the full dict with all 1042 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n"
     ]
    }
   ],
   "source": [
    "#create the label vocab\n",
    "#same as token vocab but no SEP, CLS, PAD, MASk (Leave UNK)\n",
    "# DID NOT reorganize indices, may need to\n",
    "\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "labelVocab = {token: idx for token, idx in token2idx.items() if token not in special_tokens}\n",
    "\n",
    "\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    del token2idx['[PAD]']\n",
    "    del token2idx['[SEP]']\n",
    "    del token2idx['[CLS]']\n",
    "    del token2idx['[MASK]']\n",
    "    token = list(token2idx.keys())\n",
    "    labelVocab = {}\n",
    "    for i,x in enumerate(token):\n",
    "        labelVocab[x] = i\n",
    "    return labelVocab\n",
    "\n",
    "labelVocab = format_label_vocab(token2idx)\n",
    "\n",
    "# Print the new dictionary to verify\n",
    "print(len(labelVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset helpers\n",
    "\n",
    "def index_seg(tokens, symbol=2):\n",
    "    flag = 0\n",
    "    seg = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            seg.append(flag)\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "        else:\n",
    "            seg.append(flag)\n",
    "    return seg\n",
    "\n",
    "\n",
    "def position_idx(tokens, symbol=2):\n",
    "    pos = []\n",
    "    flag = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            pos.append(flag)\n",
    "            flag += 1\n",
    "        else:\n",
    "            pos.append(flag)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:  ['CLS', '5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582', 'SEP', '07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051', 'SEP']\n",
      "length Labels:  11073\n",
      "example label 1043\n"
     ]
    }
   ],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "#extract all ICD9 codes but only take first 3 digits\n",
    "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
    "\n",
    "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
    "\n",
    "def encode_labels(label_codes, labelVocab):\n",
    "    # print(len(label_codes), len(labelVocab))\n",
    "    multi_hot = [0] * len(labelVocab)\n",
    "    #print(len)\n",
    "    \n",
    "    for code in label_codes:\n",
    "        if code in labelVocab:\n",
    "            index = labelVocab[code]\n",
    "            multi_hot[index] = 1\n",
    "        else:\n",
    "            print(f\"Warning: Code {code} not found in labelVocab.\")\n",
    "    \n",
    "    return multi_hot\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "labels = {}\n",
    "\n",
    "for subject_id, visits in patient_visits.items():\n",
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "\n",
    "        # Initialize the feature list with 'CLS'\n",
    "        feature_visits = ['CLS']\n",
    "\n",
    "        # Append codes and 'SEP' after each visit up to the split index\n",
    "        for sublist in visits[:split_index]:\n",
    "            visit_codes = [code for code in sublist]\n",
    "            visit_codes.append('SEP')\n",
    "            feature_visits.extend(visit_codes)\n",
    "\n",
    "        # Append the feature list to the features\n",
    "        features.append(feature_visits)\n",
    "\n",
    "        # Gather label codes from visits after the split index\n",
    "        label_codes = [code[:3] for sublist in visits[split_index:] for code in sublist]\n",
    "\n",
    "        # Encode the labels and store using subject_id as key\n",
    "        labels[subject_id] = encode_labels(label_codes, labelVocab)\n",
    "\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"length Labels: \" , len(labels))\n",
    "    print(\"example label\", len(labels[list(patient_visits.items())[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset\n",
    "\n",
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
    "        self.token2idx = token2idx\n",
    "        self.labels = labels\n",
    "        self.patient_visits = patient_visits\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_visits)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # code_idxs, position idx, mask, segment, label\n",
    "        # Retrieve patient data by index\n",
    "        patient_id = list(self.patient_visits.keys())[index]\n",
    "        codes = self.patient_visits[patient_id]\n",
    "        \n",
    "        # Initialize sequence with [CLS] token\n",
    "        sequence = [self.token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "        \n",
    "        # Add each code to the sequence and append [SEP] after each visit\n",
    "        for visit in codes:\n",
    "            #change 'code' to be truncated version!!!!!!\n",
    "            sequence.extend([self.token2idx.get(code[:3], self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
    "\n",
    "    \n",
    "        \n",
    "        # Cut or pad the sequence to the maximum length\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
    "        \n",
    "        position_indices = position_idx(sequence)\n",
    "        segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "        # Create a mask for the sequence\n",
    "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), torch.tensor(segment, dtype=torch.long), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2]\n",
      "512\n",
      "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "512\n",
      "position:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "512\n",
      "segment:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "512\n",
      "mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1043\n",
      "labels:  tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\\nNeed to do version matching before feeding into model.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testint stuff out\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "patient_id = list(patient_visits.keys())[0]\n",
    "codes = patient_visits[patient_id]\n",
    "\n",
    "# Initialize sequence with [CLS] token\n",
    "sequence = [token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code[:3], token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "\n",
    "# Cut or pad the sequence to the maximum length\n",
    "if len(sequence) > max_len:\n",
    "    sequence = sequence[:max_len]\n",
    "else:\n",
    "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
    "\n",
    "position_indices = position_idx(sequence)\n",
    "segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "label = torch.tensor(labels[patient_id], dtype=torch.float)\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "print(len(position_indices))\n",
    "print(\"position: \", position_indices)\n",
    "\n",
    "print(len(segment))\n",
    "print(\"segment: \", segment)\n",
    "\n",
    "print(len(mask))\n",
    "print(\"mask: \", mask)\n",
    "\n",
    "print(len(labels[10000032]))\n",
    "print(\"labels: \" , label)\n",
    "\n",
    "'''\n",
    "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
    "Need to do version matching before feeding into model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertPreTrainedModel , BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cpu',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 512,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "model_config = {\n",
    "    'vocab_size': len(labelVocab), # number of disease + symbols for word embedding 1047\n",
    "    'hidden_size': 300, # word embedding and seg embedding hidden size \n",
    "    #'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    #'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        #self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        #self.age_vocab_size = config.get('age_vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=global_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=global_params['batch_size'], shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ICD9Embeddings:\n",
    "    def __init__(self, filename=\"./ic9_embeddings.txt\"):\n",
    "        self.embedding_filename = filename\n",
    "        # self.icd9_to_embeddings = self._read_embeddings()\n",
    "        self.icd9_to_embeddings = self._read_embeddings_trunc()\n",
    "        self.embedding_size = 300\n",
    "\n",
    "    # this reads the 5 digit code correctly\n",
    "    def _read_embeddings(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "\n",
    "                icd9_to_embeddings[code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    # this does the opposite of greedy it basically just takes the last icd_9 with the first three that match\n",
    "    def _read_embeddings_trunc(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        codes_lost = 0\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "                trunc_code = code[:3]\n",
    "                # print(trunc_code)\n",
    "                if trunc_code in icd9_to_embeddings:\n",
    "                    codes_lost += 1\n",
    "\n",
    "                icd9_to_embeddings[trunc_code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        # print('codes_lost', codes_lost)\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    def get(self, code):\n",
    "        if code in self.icd9_to_embeddings:\n",
    "            return self.icd9_to_embeddings[code]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_idx_to_embedding(self, token2idx):\n",
    "        idx2embedding = {}\n",
    "        for code, idx in token2idx.items():\n",
    "            if code in self.icd9_to_embeddings:\n",
    "                idx2embedding[idx] = self.icd9_to_embeddings[code]\n",
    "            else:\n",
    "                idx2embedding[idx] = torch.zeros(\n",
    "                    self.embedding_size, dtype=torch.float32\n",
    "                )\n",
    "                # print(\"code is not in icd9 embeddings\", code)\n",
    "\n",
    "        return idx2embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups \n",
    "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        self.idx2embedding = self.icd9_embeddings.get_idx_to_embedding(token2idx)\n",
    "\n",
    "\n",
    "        icd9_codes = list(self.idx2embedding.keys())  \n",
    "        # print('length of icd9 codes', len(icd9_codes))\n",
    "\n",
    "        embeddings_matrix = torch.stack([self.idx2embedding[code] for code in icd9_codes], dim=0)\n",
    "        # print('embeddings matrix shape', embeddings_matrix.shape)\n",
    "\n",
    "        # Initialize embeddings for CLS and SEP tokens\n",
    "        additional_embeddings = torch.randn(2, config.hidden_size)\n",
    "\n",
    "        # Combine precomputed and additional embeddings\n",
    "        full_embeddings_matrix = torch.cat([embeddings_matrix, additional_embeddings], dim=0)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=False)\n",
    "\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None, segment_ids=None):\n",
    "        # print('embeddings forward')\n",
    "        if segment_ids is None:\n",
    "            pass\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "        # print('max word id', word_ids.max().item())\n",
    "        # print('embeddings size', self.word_embeddings.weight.shape)\n",
    "        # print(word_ids)\n",
    "        # print('word_ids shape', word_ids.shape)\n",
    "        \n",
    "        embeddings = self.word_embeddings(word_ids)\n",
    "        # print('embeddings',embeddings)\n",
    "\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "    \n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None,segment_ids=None,output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(posi_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, segment_ids=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,segment_ids,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(global_params['device'])\n",
    "model = model.to(\"cpu\")\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, output, label\n",
    "\n",
    "def accuracy(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    # Apply a threshold to convert probabilities to binary predictions\n",
    "    predictions = (output.numpy() > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = sklearn.metrics.accuracy_score(label.numpy(), predictions)\n",
    "    return acc, output, label\n",
    "def print_result(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    predictions = (output.numpy() > 0.5).astype(int)\n",
    "\n",
    "    print('predicted diseases',     np.where(predictions == 1)[:5])\n",
    "    print('actual diseases',     np.where(label == 1)[:5])\n",
    "    # print('label',     label)\n",
    "\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, roc, output, label,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultiLabelBinarizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\">?<span>Documentation for MultiLabelBinarizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "print(len(labelVocab))\n",
    "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        cnt +=1\n",
    "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
    "        # print('targets',targets.shape,targets)\n",
    "\n",
    "        # print('targets', np.argwhere(targets[0] == 1))\n",
    "        # targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        # print('targets after', np.argwhere(targets[0] == 1))\n",
    "        #age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 10==0:\n",
    "            prec, a, b = precision(logits, targets)\n",
    "            acc, a, b = accuracy(logits,targets)\n",
    "            print_result(logits[0], targets[0])\n",
    "\n",
    "\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\\t| Accuracy: {} \".format(e, cnt,temp_loss/500, prec, acc))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        input_ids, posi_ids, attMask, targets, _ = batch\n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "predicted diseases (array([   0,    2,    4,    5,    6,    9,   10,   12,   14,   17,   19,\n",
      "         21,   22,   23,   25,   27,   32,   34,   35,   38,   39,   40,\n",
      "         43,   48,   50,   53,   55,   56,   59,   62,   63,   65,   67,\n",
      "         69,   70,   71,   72,   74,   75,   76,   77,   82,   84,   86,\n",
      "         89,   91,   94,   95,   96,   98,   99,  101,  105,  107,  109,\n",
      "        110,  111,  112,  113,  114,  115,  117,  118,  120,  121,  122,\n",
      "        123,  127,  129,  132,  133,  134,  135,  137,  138,  141,  145,\n",
      "        146,  152,  153,  155,  158,  167,  168,  171,  172,  174,  175,\n",
      "        176,  178,  179,  182,  183,  186,  193,  194,  196,  198,  199,\n",
      "        200,  204,  205,  208,  209,  211,  212,  213,  214,  216,  220,\n",
      "        222,  228,  233,  234,  235,  237,  238,  239,  241,  242,  244,\n",
      "        253,  255,  262,  263,  266,  267,  269,  272,  273,  274,  275,\n",
      "        277,  282,  286,  288,  289,  293,  294,  295,  297,  298,  300,\n",
      "        301,  302,  304,  306,  309,  311,  314,  316,  320,  321,  323,\n",
      "        326,  327,  328,  330,  331,  332,  334,  338,  339,  340,  344,\n",
      "        346,  348,  349,  350,  352,  355,  359,  363,  364,  366,  367,\n",
      "        368,  369,  370,  371,  377,  378,  379,  380,  382,  385,  387,\n",
      "        388,  392,  393,  395,  397,  398,  399,  400,  401,  402,  404,\n",
      "        409,  411,  416,  420,  421,  423,  425,  426,  427,  429,  430,\n",
      "        431,  432,  434,  437,  438,  440,  442,  443,  444,  445,  446,\n",
      "        448,  450,  452,  453,  457,  459,  462,  466,  468,  469,  471,\n",
      "        473,  475,  477,  481,  483,  484,  486,  487,  490,  492,  494,\n",
      "        495,  498,  500,  501,  503,  506,  507,  509,  511,  514,  515,\n",
      "        519,  520,  522,  524,  525,  527,  537,  538,  540,  541,  543,\n",
      "        545,  547,  548,  550,  552,  553,  557,  560,  563,  564,  565,\n",
      "        566,  572,  573,  574,  575,  576,  578,  579,  581,  582,  583,\n",
      "        586,  587,  588,  589,  590,  593,  594,  596,  597,  599,  600,\n",
      "        603,  605,  608,  609,  610,  615,  617,  618,  621,  622,  625,\n",
      "        629,  633,  634,  635,  640,  642,  644,  646,  651,  654,  655,\n",
      "        657,  659,  660,  661,  664,  665,  667,  668,  669,  670,  672,\n",
      "        679,  681,  682,  685,  687,  691,  693,  697,  698,  699,  701,\n",
      "        703,  704,  708,  712,  717,  718,  720,  721,  726,  727,  728,\n",
      "        729,  730,  733,  735,  739,  741,  742,  744,  745,  746,  747,\n",
      "        748,  749,  750,  751,  752,  755,  757,  759,  760,  761,  763,\n",
      "        764,  765,  766,  767,  769,  770,  771,  775,  776,  777,  778,\n",
      "        779,  780,  782,  788,  790,  792,  794,  795,  798,  801,  803,\n",
      "        804,  806,  810,  811,  812,  815,  819,  820,  822,  826,  827,\n",
      "        829,  833,  836,  839,  841,  843,  844,  848,  849,  851,  856,\n",
      "        859,  860,  861,  862,  865,  867,  869,  870,  871,  872,  873,\n",
      "        874,  875,  879,  883,  884,  886,  888,  890,  894,  898,  903,\n",
      "        904,  905,  908,  909,  911,  912,  914,  916,  919,  920,  924,\n",
      "        925,  932,  933,  934,  936,  937,  938,  940,  943,  946,  947,\n",
      "        948,  955,  956,  960,  963,  964,  968,  970,  973,  976,  977,\n",
      "        978,  979,  980,  982,  985,  988,  991,  992,  993,  995,  997,\n",
      "        998,  999, 1004, 1009, 1010, 1013, 1014, 1023, 1024, 1025, 1026,\n",
      "       1029, 1031, 1032, 1033, 1034, 1035, 1042]),)\n",
      "actual diseases (array([  98,  178,  180,  252,  261,  264,  320,  393,  395,  466,  469,\n",
      "        485,  486,  507,  658,  719,  723,  726,  728,  924,  947,  962,\n",
      "       1009]),)\n",
      "epoch: 0\t| Cnt: 1\t| Loss: 0.0013869562149047851\t| precision: 0.029238523586727388\t| Accuracy: 0.0 \n",
      "predicted diseases (array([   5,    9,   10,   11,   12,   13,   14,   15,   16,   25,   27,\n",
      "         35,   36,   38,   39,   40,   41,   44,   48,   56,   59,   61,\n",
      "         63,   66,   72,   74,   76,   77,   87,   89,   94,  101,  107,\n",
      "        108,  109,  110,  111,  112,  113,  118,  121,  129,  132,  134,\n",
      "        136,  137,  138,  141,  143,  146,  153,  154,  155,  160,  161,\n",
      "        163,  164,  167,  168,  171,  172,  174,  175,  178,  179,  183,\n",
      "        186,  188,  194,  198,  204,  205,  209,  211,  216,  222,  227,\n",
      "        229,  234,  235,  237,  238,  239,  241,  249,  253,  257,  260,\n",
      "        261,  267,  270,  272,  273,  282,  288,  289,  293,  295,  297,\n",
      "        302,  304,  306,  310,  311,  320,  321,  326,  328,  331,  339,\n",
      "        340,  346,  350,  352,  362,  366,  367,  369,  370,  378,  381,\n",
      "        382,  388,  395,  398,  399,  400,  402,  404,  406,  409,  410,\n",
      "        411,  418,  419,  420,  425,  430,  432,  434,  435,  437,  438,\n",
      "        442,  443,  445,  446,  448,  451,  452,  456,  462,  463,  465,\n",
      "        466,  469,  471,  481,  483,  486,  487,  488,  491,  494,  495,\n",
      "        500,  503,  509,  510,  513,  515,  520,  524,  525,  526,  527,\n",
      "        529,  532,  534,  537,  541,  542,  543,  548,  550,  551,  557,\n",
      "        560,  564,  566,  567,  569,  572,  573,  574,  575,  577,  579,\n",
      "        580,  583,  589,  593,  594,  595,  596,  597,  599,  604,  605,\n",
      "        610,  617,  618,  621,  622,  625,  633,  642,  650,  654,  655,\n",
      "        656,  657,  659,  660,  661,  662,  665,  668,  670,  671,  679,\n",
      "        680,  682,  684,  685,  688,  691,  697,  698,  701,  703,  704,\n",
      "        707,  708,  709,  717,  719,  720,  726,  727,  728,  730,  733,\n",
      "        735,  737,  739,  749,  751,  760,  762,  764,  768,  770,  772,\n",
      "        775,  777,  787,  788,  798,  800,  803,  804,  806,  807,  809,\n",
      "        811,  812,  814,  817,  819,  820,  821,  822,  823,  825,  826,\n",
      "        829,  833,  836,  838,  839,  841,  844,  845,  848,  849,  851,\n",
      "        854,  857,  859,  860,  865,  868,  869,  870,  871,  873,  878,\n",
      "        879,  884,  889,  890,  893,  894,  902,  904,  905,  909,  912,\n",
      "        915,  919,  920,  924,  925,  926,  929,  932,  933,  934,  936,\n",
      "        940,  941,  943,  946,  948,  955,  956,  963,  964,  968,  973,\n",
      "        978,  980,  984,  991,  993,  995,  997,  998,  999, 1004, 1010,\n",
      "       1016, 1025, 1029, 1034, 1035, 1036, 1041]),)\n",
      "actual diseases (array([ 227,  248,  263,  322,  375,  380,  382,  394,  423,  509,  516,\n",
      "        524,  529,  530,  625,  646,  663,  719,  725,  729,  738,  814,\n",
      "        929,  937,  941,  967,  996, 1000]),)\n",
      "epoch: 0\t| Cnt: 11\t| Loss: 0.013639095544815063\t| precision: 0.03290535373149393\t| Accuracy: 0.0 \n",
      "predicted diseases (array([   9,   11,   13,   16,   18,   25,   27,   35,   48,   70,   72,\n",
      "         76,   77,   78,   87,   89,   94,  101,  102,  107,  108,  109,\n",
      "        110,  118,  121,  122,  123,  126,  127,  129,  132,  133,  135,\n",
      "        137,  154,  155,  156,  164,  171,  174,  178,  182,  183,  184,\n",
      "        186,  190,  192,  193,  194,  196,  198,  203,  205,  211,  214,\n",
      "        218,  222,  227,  228,  233,  234,  235,  239,  263,  267,  272,\n",
      "        273,  282,  293,  294,  295,  297,  298,  302,  306,  311,  314,\n",
      "        316,  320,  326,  328,  331,  339,  340,  342,  346,  350,  352,\n",
      "        365,  366,  367,  369,  373,  382,  384,  388,  395,  396,  398,\n",
      "        402,  406,  409,  416,  420,  421,  425,  431,  435,  438,  443,\n",
      "        446,  452,  455,  463,  465,  481,  484,  487,  488,  490,  494,\n",
      "        495,  496,  499,  503,  511,  519,  529,  532,  539,  540,  541,\n",
      "        543,  544,  548,  550,  551,  560,  566,  569,  572,  573,  576,\n",
      "        583,  589,  593,  596,  597,  599,  607,  608,  615,  617,  622,\n",
      "        625,  627,  633,  637,  642,  644,  646,  651,  655,  657,  659,\n",
      "        662,  665,  666,  668,  670,  671,  677,  679,  680,  682,  685,\n",
      "        693,  697,  698,  708,  718,  726,  727,  728,  729,  730,  741,\n",
      "        742,  746,  748,  749,  751,  755,  760,  764,  766,  767,  771,\n",
      "        772,  777,  785,  788,  793,  800,  803,  806,  812,  813,  814,\n",
      "        819,  820,  822,  823,  825,  829,  833,  836,  839,  843,  844,\n",
      "        848,  849,  856,  865,  868,  870,  871,  873,  875,  876,  879,\n",
      "        884,  885,  890,  892,  894,  904,  905,  909,  911,  912,  920,\n",
      "        924,  925,  926,  928,  929,  932,  933,  934,  936,  940,  941,\n",
      "        943,  945,  946,  956,  959,  963,  964,  968,  973,  977,  978,\n",
      "        979,  980,  991,  993,  997,  999, 1003, 1010, 1021, 1028, 1029,\n",
      "       1033, 1035, 1036, 1041]),)\n",
      "actual diseases (array([   8,   35,  248,  250,  252,  261,  303,  330,  373,  375,  382,\n",
      "        384,  393,  394,  404,  442,  448,  450,  453,  455,  473,  485,\n",
      "        529,  530,  625,  663,  672,  724,  726,  924,  954,  962,  994,\n",
      "        996, 1015]),)\n",
      "epoch: 0\t| Cnt: 21\t| Loss: 0.013202232837677002\t| precision: 0.038471299893489436\t| Accuracy: 0.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     aps, roc, test_loss \u001b[38;5;241m=\u001b[39m evaluation()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aps \u001b[38;5;241m>\u001b[39mbest_pre:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Save a trained model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     19\u001b[0m attMask \u001b[38;5;241m=\u001b[39m attMask\u001b[38;5;241m.\u001b[39mto(global_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(global_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposi_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattMask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_accumulation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m/\u001b[39mglobal_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_accumulation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 118\u001b[0m, in \u001b[0;36mBertForMultiLabelPrediction.forward\u001b[0;34m(self, input_ids, posi_ids, attention_mask, segment_ids, labels)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, posi_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, segment_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 118\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposi_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m    122\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 98\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, posi_ids, attention_mask, segment_ids, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m     95\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m     97\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, posi_ids)\n\u001b[0;32m---> 98\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoded_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    101\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:359\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0.0\n",
    "for e in range(50):\n",
    "    print(\"starting training...\")\n",
    "    train(e)\n",
    "    aps, roc, test_loss = evaluation()\n",
    "    if aps >best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = aps\n",
    "    print('aps : {}'.format(aps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
