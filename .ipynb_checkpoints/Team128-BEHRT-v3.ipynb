{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import json\n",
    "import pytorch_pretrained_bert as Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 9 code\n",
    "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
    "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
    "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 9 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
    "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Token Vocabulary\n",
    "\n",
    "\n",
    "import json\n",
    "# TODO i need full vocabulary     \n",
    "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
    "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
    "\n",
    "# Define special tokens with a specific order\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
    "\n",
    "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
    "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
    "\n",
    "# Print the number of unique codes to verify\n",
    "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
    "\n",
    "# Print token to index mapping\n",
    "#print(\"Token to Index Mapping:\", token2idx)\n",
    "\n",
    "# Save the token2idx dictionary to a JSON file for later use\n",
    "with open('token2idx.json', 'w') as f:\n",
    "    json.dump(token2idx, f)\n",
    "\n",
    "    # for labels just make the full dict with all 1042 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the label vocab\n",
    "#same as token vocab but no SEP, CLS, PAD, MASk (Leave UNK)\n",
    "# DID NOT reorganize indices, may need to\n",
    "\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "labelVocab = {token: idx for token, idx in token2idx.items() if token not in special_tokens}\n",
    "\n",
    "\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    del token2idx['[PAD]']\n",
    "    del token2idx['[SEP]']\n",
    "    del token2idx['[CLS]']\n",
    "    del token2idx['[MASK]']\n",
    "    token = list(token2idx.keys())\n",
    "    labelVocab = {}\n",
    "    for i,x in enumerate(token):\n",
    "        labelVocab[x] = i\n",
    "    return labelVocab\n",
    "\n",
    "labelVocab = format_label_vocab(token2idx)\n",
    "\n",
    "# Print the new dictionary to verify\n",
    "print(len(labelVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset helpers\n",
    "\n",
    "def index_seg(tokens, symbol=2):\n",
    "    flag = 0\n",
    "    seg = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            seg.append(flag)\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "        else:\n",
    "            seg.append(flag)\n",
    "    return seg\n",
    "\n",
    "\n",
    "def position_idx(tokens, symbol=2):\n",
    "    pos = []\n",
    "    flag = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            pos.append(flag)\n",
    "            flag += 1\n",
    "        else:\n",
    "            pos.append(flag)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "#extract all ICD9 codes but only take first 3 digits\n",
    "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
    "\n",
    "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
    "\n",
    "def encode_labels(label_codes, labelVocab):\n",
    "    # print(len(label_codes), len(labelVocab))\n",
    "    multi_hot = [0] * len(labelVocab)\n",
    "    #print(len)\n",
    "    \n",
    "    for code in label_codes:\n",
    "        if code in labelVocab:\n",
    "            index = labelVocab[code]\n",
    "            multi_hot[index] = 1\n",
    "        else:\n",
    "            print(f\"Warning: Code {code} not found in labelVocab.\")\n",
    "    \n",
    "    return multi_hot\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "labels = {}\n",
    "\n",
    "for subject_id, visits in patient_visits.items():\n",
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "\n",
    "        # Initialize the feature list with 'CLS'\n",
    "        feature_visits = ['CLS']\n",
    "\n",
    "        # Append codes and 'SEP' after each visit up to the split index\n",
    "        for sublist in visits[:split_index]:\n",
    "            visit_codes = [code for code in sublist]\n",
    "            visit_codes.append('SEP')\n",
    "            feature_visits.extend(visit_codes)\n",
    "\n",
    "        # Append the feature list to the features\n",
    "        features.append(feature_visits)\n",
    "\n",
    "        # Gather label codes from visits after the split index\n",
    "        label_codes = [code[:3] for sublist in visits[split_index:] for code in sublist]\n",
    "\n",
    "        # Encode the labels and store using subject_id as key\n",
    "        labels[subject_id] = encode_labels(label_codes, labelVocab)\n",
    "\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"length Labels: \" , len(labels))\n",
    "    print(\"example label\", len(labels[list(patient_visits.items())[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset\n",
    "\n",
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
    "        self.token2idx = token2idx\n",
    "        self.labels = labels\n",
    "        self.patient_visits = patient_visits\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_visits)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # code_idxs, position idx, mask, segment, label\n",
    "        # Retrieve patient data by index\n",
    "        patient_id = list(self.patient_visits.keys())[index]\n",
    "        codes = self.patient_visits[patient_id]\n",
    "        \n",
    "        # Initialize sequence with [CLS] token\n",
    "        sequence = [self.token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "        \n",
    "        # Add each code to the sequence and append [SEP] after each visit\n",
    "        for visit in codes:\n",
    "            #change 'code' to be truncated version!!!!!!\n",
    "            sequence.extend([self.token2idx.get(code[:3], self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
    "\n",
    "    \n",
    "        \n",
    "        # Cut or pad the sequence to the maximum length\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
    "        \n",
    "        position_indices = position_idx(sequence)\n",
    "        segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "        # Create a mask for the sequence\n",
    "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), torch.tensor(segment, dtype=torch.long), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testint stuff out\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "patient_id = list(patient_visits.keys())[0]\n",
    "codes = patient_visits[patient_id]\n",
    "\n",
    "# Initialize sequence with [CLS] token\n",
    "sequence = [token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code[:3], token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "\n",
    "# Cut or pad the sequence to the maximum length\n",
    "if len(sequence) > max_len:\n",
    "    sequence = sequence[:max_len]\n",
    "else:\n",
    "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
    "\n",
    "position_indices = position_idx(sequence)\n",
    "segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "label = torch.tensor(labels[patient_id], dtype=torch.float)\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "print(len(position_indices))\n",
    "print(\"position: \", position_indices)\n",
    "\n",
    "print(len(segment))\n",
    "print(\"segment: \", segment)\n",
    "\n",
    "print(len(mask))\n",
    "print(\"mask: \", mask)\n",
    "\n",
    "print(len(labels[10000032]))\n",
    "print(\"labels: \" , label)\n",
    "\n",
    "'''\n",
    "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
    "Need to do version matching before feeding into model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertPreTrainedModel , BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cpu',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 512,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "model_config = {\n",
    "    'vocab_size': len(labelVocab), # number of disease + symbols for word embedding 1047\n",
    "    'hidden_size': 300, # word embedding and seg embedding hidden size \n",
    "    #'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    #'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        #self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        #self.age_vocab_size = config.get('age_vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=global_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=global_params['batch_size'], shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ICD9Embeddings:\n",
    "    def __init__(self, filename=\"./ic9_embeddings.txt\"):\n",
    "        self.embedding_filename = filename\n",
    "        # self.icd9_to_embeddings = self._read_embeddings()\n",
    "        self.icd9_to_embeddings = self._read_embeddings_trunc()\n",
    "        self.embedding_size = 300\n",
    "\n",
    "    # this reads the 5 digit code correctly\n",
    "    def _read_embeddings(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "\n",
    "                icd9_to_embeddings[code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    # this does the opposite of greedy it basically just takes the last icd_9 with the first three that match\n",
    "    def _read_embeddings_trunc(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        codes_lost = 0\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "                trunc_code = code[:3]\n",
    "                # print(trunc_code)\n",
    "                if trunc_code in icd9_to_embeddings:\n",
    "                    codes_lost += 1\n",
    "\n",
    "                icd9_to_embeddings[trunc_code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        # print('codes_lost', codes_lost)\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    def get(self, code):\n",
    "        if code in self.icd9_to_embeddings:\n",
    "            return self.icd9_to_embeddings[code]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_idx_to_embedding(self, token2idx):\n",
    "        idx2embedding = {}\n",
    "        for code, idx in token2idx.items():\n",
    "            if code in self.icd9_to_embeddings:\n",
    "                idx2embedding[idx] = self.icd9_to_embeddings[code]\n",
    "            else:\n",
    "                idx2embedding[idx] = torch.zeros(\n",
    "                    self.embedding_size, dtype=torch.float32\n",
    "                )\n",
    "                # print(\"code is not in icd9 embeddings\", code)\n",
    "\n",
    "        return idx2embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups \n",
    "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        self.idx2embedding = self.icd9_embeddings.get_idx_to_embedding(token2idx)\n",
    "\n",
    "\n",
    "        icd9_codes = list(self.idx2embedding.keys())  \n",
    "        # print('length of icd9 codes', len(icd9_codes))\n",
    "\n",
    "        embeddings_matrix = torch.stack([self.idx2embedding[code] for code in icd9_codes], dim=0)\n",
    "        # print('embeddings matrix shape', embeddings_matrix.shape)\n",
    "\n",
    "        # Initialize embeddings for CLS and SEP tokens\n",
    "        additional_embeddings = torch.randn(2, config.hidden_size)\n",
    "\n",
    "        # Combine precomputed and additional embeddings\n",
    "        full_embeddings_matrix = torch.cat([embeddings_matrix, additional_embeddings], dim=0)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=False)\n",
    "\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None, segment_ids=None):\n",
    "        # print('embeddings forward')\n",
    "        if segment_ids is None:\n",
    "            pass\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "        # print('max word id', word_ids.max().item())\n",
    "        # print('embeddings size', self.word_embeddings.weight.shape)\n",
    "        # print(word_ids)\n",
    "        # print('word_ids shape', word_ids.shape)\n",
    "        \n",
    "        embeddings = self.word_embeddings(word_ids)\n",
    "        # print('embeddings',embeddings)\n",
    "\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "    \n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None,segment_ids=None,output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(posi_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, segment_ids=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,segment_ids,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(global_params['device'])\n",
    "model = model.to(\"cpu\")\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, output, label\n",
    "\n",
    "def accuracy(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    # Apply a threshold to convert probabilities to binary predictions\n",
    "    predictions = (output.numpy() > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = sklearn.metrics.accuracy_score(label.numpy(), predictions)\n",
    "    return acc, output, label\n",
    "def print_result(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    predictions = (output.numpy() > 0.5).astype(int)\n",
    "\n",
    "    print('predicted diseases',     np.where(predictions == 1)[:5])\n",
    "    print('actual diseases',     np.where(label == 1)[:5])\n",
    "    # print('label',     label)\n",
    "\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, roc, output, label,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "print(len(labelVocab))\n",
    "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        cnt +=1\n",
    "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
    "        # print('targets',targets.shape,targets)\n",
    "\n",
    "        # print('targets', np.argwhere(targets[0] == 1))\n",
    "        # targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        # print('targets after', np.argwhere(targets[0] == 1))\n",
    "        #age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 10==0:\n",
    "            prec, a, b = precision(logits, targets)\n",
    "            acc, a, b = accuracy(logits,targets)\n",
    "            print_result(logits[0], targets[0])\n",
    "\n",
    "\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\\t| Accuracy: {} \".format(e, cnt,temp_loss/500, prec, acc))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        input_ids, posi_ids, attMask, targets, _ = batch\n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pre = 0.0\n",
    "for e in range(50):\n",
    "    print(\"starting training...\")\n",
    "    train(e)\n",
    "    aps, roc, test_loss = evaluation()\n",
    "    if aps >best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = aps\n",
    "    print('aps : {}'.format(aps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
