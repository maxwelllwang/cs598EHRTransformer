{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "_OvPWYhA1-jo",
      "metadata": {
        "id": "_OvPWYhA1-jo"
      },
      "source": [
        "# **Project Links**\n",
        "\n",
        "### [Project GitHub Repository](https://github.com/maxwelllwang/cs598EHRTransformer/tree/main)\n",
        "\n",
        "### [Project Video](https://drive.google.com/file/d/1v-igXoba1TicqjT4ozxfhXb0kaP0A9W2/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2NjffeLNwPdH",
      "metadata": {
        "id": "2NjffeLNwPdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ded3ebb-2b79-40b8-fff1-21e66858742d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "g4m0lg-gwnHJ",
      "metadata": {
        "id": "g4m0lg-gwnHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1287aa3c-2c31-4516-a6f8-1f96acb9b326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/10FiI-a1o7z1nzN3cuW8I0SbdVBN4N_OA/cs598ehrupload\n",
            "'Copy of Team128New(newest).ipynb'   mimic_data         sample_data   'Team128New(OLD).ipynb'\n",
            " embeddings\t\t\t     requirements.txt   saved_models   token2idx.json\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/gdrive/MyDrive/CS 598 Project/cs598ehrupload\n",
        "%cd /content/gdrive/MyDrive/cs598ehrupload\n",
        "! ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ohMXONP1a92",
      "metadata": {
        "id": "9ohMXONP1a92"
      },
      "source": [
        "#**Introduction**\n",
        "\n",
        "## *Background:*\n",
        "Current pretraining objectives in predictive EHR-based models are limited to predicting a fraction of ICD codes within a patient’s visit, when in reality, patients usually have multiple, often highly-correlated diseases. In addition, current models are unable to accurately predict the timeline of correlated diagnoses and could lead to missed opportunities in preventative care. Predictive tasks surrounding healthcare data can be challenging due to the complexity of healthcare data, which includes high-dimensional and often incomplete patient data over time. The state of the art methods used to solve similar healthcare related problems have been transformer based deep learning models trained on extensive datasets and fine-tuned for specific tasks. Despite their success, current models are often fine-tuned to focus on predicting a limited set of outcomes, thus overlooking the interconnected nature of various health conditions.\n",
        "\n",
        "## *Original Paper: TransformEHR*\n",
        "The paper presents \"TransformEHR,\" a novel generative encoder-decoder model leveraging transformer architecture, specifically designed for predicting future patient outcomes based on their longitudinal EHRs. The authors utilized techniques like visit-masking and time embedding to achieve results that outperform the other state of the art models. For example, when testing their encoder-decoder model against an encoder only model, the authors were able to achieve an, “improvement of 95%CI: 0.74%–1.16%, p < 0.001 in AUROC across all diseases/outcomes tested.” While the paper boasted strong results on a variety of both common and uncommon diseases, the authors mentioned that their work was related to predictive model studies focused on intentional self-harm. TransformEHR performed exceptionally well within this subspace and esteemed to reduce incremental cost-effective ratio by $109k per quality-adjusted life-years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uz8n1Nmz3Hjf",
      "metadata": {
        "id": "uz8n1Nmz3Hjf"
      },
      "source": [
        "#**Scope of Reproducibility**\n",
        "\n",
        "While trying to recreate the TransformEHR model as per the original paper, we discovered a lot of missing code in the project repo, specifically in the helper class `dataset.py`. This file was meant to contain the DataCollator functions and Tokenizers to be used in the model. Since the model required the input formatted a specific way, missing these functions and not knowing how they were implemented or what the expected input format was made it nearly impossible to get the model running.\n",
        "\n",
        "As a result, we decided to base our model off of BEHRT, another type of transformer model used to predict EHR codes. We also integrated this with prelearned ICD code embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LT-36hWLHq-F",
      "metadata": {
        "id": "LT-36hWLHq-F"
      },
      "source": [
        "## *BEHRT Overview*\n",
        "\n",
        "BEHRT is a transformer model inspired by BERT, which aims to accomplish a similar goal to TransformEHR of predicting patient's future diseases/outcomes based on EHR data of their past visits. The model takes each diagnosis as a \"word,\" each visit as a \"sentence,\" and the entire medical history as a \"document,\" and uses multi-head self-attention and masked language modeling. BEHRT integrates disease embeddings, positional encodings, age, and visit segment information, and uses deep bidirectional representation to make predictions of a patient's medical journey."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bHVnMX-y3fIx",
      "metadata": {
        "id": "bHVnMX-y3fIx"
      },
      "source": [
        "#**Methodology**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vHsHp_Ka3ygR",
      "metadata": {
        "id": "vHsHp_Ka3ygR"
      },
      "source": [
        "##*Environment*\n",
        "\n",
        "Python version: 3.10\n",
        "\n",
        "###Dependencies/Packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0",
      "metadata": {
        "id": "0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3d83fa-a713-4fb3-8cc3-9a896c15a788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.25.2)\n",
            "Collecting boto3 (from pytorch-pretrained-bert)\n",
            "  Downloading boto3-1.34.100-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch-pretrained-bert)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting botocore<1.35.0,>=1.34.100 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading botocore-1.34.100-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.100->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.100->boto3->pytorch-pretrained-bert) (1.16.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.34.100 botocore-1.34.100 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-pretrained-bert-0.6.2 s3transfer-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sklearn.metrics as skm\n",
        "import math\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import transformers\n",
        "import json\n",
        "import pytorch_pretrained_bert as Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MjH2HMCd3_xP",
      "metadata": {
        "id": "MjH2HMCd3_xP"
      },
      "source": [
        "##*Data*\n",
        "\n",
        "The data we use for this model comes from the **[MIMIC-III dataset](https://physionet.org/content/mimiciii/1.4/)**, which can be accessed via PhysioNet to anyone with completed training/permissions. Specifically, we use the files *diagnoses_icd.csv.g* and *D_ICD_DIAGNOSES.csv*.\n",
        "\n",
        "\n",
        "Please note that the filepaths for variables `diagnoses_file_path` (which will read from *diagnoses_icd.csv.gz*) and `map_file_path` (which will read from *D_ICD_DIAGNOSES.csv*) may need to be updated depending on where they are located locally.\n",
        "\n",
        "\n",
        "We have provided the equivalent data files from the publicly available [MIMIC-III demo dataset](https://physionet.org/content/mimiciii-demo/1.4/). While the demo data is mostly similar to the full dataset, there are some differences; for example, the full dataset contains both ICD-9 and ICD-10 codes in patient visits, while the demo data only has ICD-9 codes, so filtering out patients with ICD-10 codes in their visits would not be necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "y8H364z45WNC"
      },
      "id": "y8H364z45WNC"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1",
      "metadata": {
        "id": "1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a4a7af-f15d-4376-fe8e-9887698990a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'], dtype='object')\n",
            "Patient ID: 10002769\n",
            "num of visits for patient:  2\n",
            "\t0-th visit id: 28314592\n",
            "\t0-th visit diagnosis codes: ['45342', '70713', '45981', '4019', '2724', 'V1251']\n",
            "\t0-th visit diagnosis short titles: ['Ac DVT/emb distl low ext', 'Ulcer of ankle', 'Venous insufficiency NOS', 'Hypertension NOS', 'Hyperlipidemia NEC/NOS', 'Hx-ven thrombosis/embols']\n",
            "\t1-th visit id: 25681387\n",
            "\t1-th visit diagnosis codes: ['45981', '70713', '4019', '2720', 'V1251', '4928', 'V1582', 'V113']\n",
            "\t1-th visit diagnosis short titles: ['Venous insufficiency NOS', 'Ulcer of ankle', 'Hypertension NOS', 'Pure hypercholesterolem', 'Hx-ven thrombosis/embols', 'Emphysema NEC', 'History of tobacco use', 'Hx of alcoholism']\n"
          ]
        }
      ],
      "source": [
        "#Data stuff\n",
        "diagnoses_file_path = r'/content/gdrive/MyDrive/cs598ehrupload/mimic_data/diagnoses_icd.csv.gz'\n",
        "map_file_path = r'/content/gdrive/MyDrive/cs598ehrupload/mimic_data/D_ICD_DIAGNOSES.csv'\n",
        "\n",
        "\n",
        "# diagnoses_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/sample_data/DIAGNOSES_ICD.csv'\n",
        "# map_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/sample_data/D_ICD_DIAGNOSES.csv'\n",
        "\n",
        "# diagnoses_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/mimic_data/diagnoses_icd.csv.gz'\n",
        "# map_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/mimic_data/D_ICD_DIAGNOSES.csv'\n",
        "\n",
        "\n",
        "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
        "print(diagnoses_df.columns)\n",
        "map_df = pd.read_csv(map_file_path)\n",
        "\n",
        "icd_code_col_name = 'icd_code'   # NOTE: column name 'icd9_code' in DIAGNOSES_ICD.csv from MIMIC demo dataset is called 'icd_code' in full dataset\n",
        "\n",
        "\n",
        "#list of patient id's that have been diagnosed with something\n",
        "#make everything sequential and not patient_id key based\n",
        "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
        "\n",
        "#2d array where each nested list is the hadm_id for each visit\n",
        "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
        "\n",
        "#3d array contains a list of visits with respective ICD9 code per visit\n",
        "# NOTE: column name 'icd9_code' in DIAGNOSES_ICD.csv from MIMIC demo dataset is called 'icd_code' in full dataset\n",
        "patient_visits = (\n",
        "    diagnoses_df.groupby(['subject_id', 'hadm_id'])[icd_code_col_name].apply(list).groupby(level=0).apply(list).tolist()\n",
        ")\n",
        "\n",
        "#dict of {icd9_code : short_title}\n",
        "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
        "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
        "\n",
        "print(\"Patient ID:\", patient_ids[53])\n",
        "print(\"num of visits for patient: \" , len(visits[53]))\n",
        "for visit in range(len(visits[53])):\n",
        "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
        "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
        "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
        "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zbciMgJuv83B",
      "metadata": {
        "id": "zbciMgJuv83B"
      },
      "source": [
        "The following code outputs some descriptive stats on the complete dataset to determine how many patients have ICD-9 vs ICD-10 codes as part of their visit. This is not applicable to the demo dataset because that only contains ICD-9 codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2",
      "metadata": {
        "id": "2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65b1e08-a9b7-4443-8e49-2e86c9eb12cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with icd_version = 10: 1989449\n",
            "Number of rows with icd_version = 9: 2766877\n",
            "Number of unique ICD-9 codes: 9072\n",
            "Number of unique ICD-10 codes: 16757\n",
            "Number of patients with at least one ICD-9 code: 124550\n",
            "Number of patients with both ICD-9 and ICD-10 codes: 24123\n",
            "Number of patients with only ICD-9 codes: 100427\n"
          ]
        }
      ],
      "source": [
        "#Descriptive Statistics\n",
        "\n",
        "#Total rows with icd 9/10\n",
        "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
        "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
        "\n",
        "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
        "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
        "\n",
        "#Num of unique ICD 9/10 codes\n",
        "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
        "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
        "\n",
        "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
        "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
        "\n",
        "#num patients with atleast 1 ICD 9 code\n",
        "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
        "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
        "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
        "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
        "\n",
        "#num patients with both ICD 9 / 10 codes\n",
        "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
        "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
        "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
        "\n",
        "# num pateients with ONLY ICD 9 codes\n",
        "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
        "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
        "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
        "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only want to consider patients with 4 or more visits so that we can split their visits into label and feature data later on."
      ],
      "metadata": {
        "id": "Fszz0eB_uaf3"
      },
      "id": "Fszz0eB_uaf3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3",
      "metadata": {
        "id": "3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e0b277-ef2d-49b5-ee3c-3547f74c067e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique patients: 180640\n",
            "Number of patients with only ICD-9 codes and more than 3 visits: 11073\n"
          ]
        }
      ],
      "source": [
        "# Total num of usable patients\n",
        "num_unique_patients = diagnoses_df['subject_id'].nunique()\n",
        "print(\"Number of unique patients:\", num_unique_patients)\n",
        "\n",
        "\n",
        "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
        "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
        "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
        "\n",
        "# visit_counts = diagnoses_df.groupby('subject_id')['hadm_id'].nunique()    # for demo data\n",
        "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()  # for main data\n",
        "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
        "num_patients = len(patients_more_than_three_visits)\n",
        "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocab Dicts"
      ],
      "metadata": {
        "id": "smhEyDJBw3QY"
      },
      "id": "smhEyDJBw3QY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a token vocabulary to map each ICD code to a unique index. We also add special tokens as padding and separators."
      ],
      "metadata": {
        "id": "T9pKpeuY3xbD"
      },
      "id": "T9pKpeuY3xbD"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4",
      "metadata": {
        "id": "4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df755bee-37c5-4a5b-ba5b-429d9e6d5fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique truncated codes: 1042\n"
          ]
        }
      ],
      "source": [
        "#Creates Token Vocabulary\n",
        "\n",
        "import json\n",
        "# TODO i need full vocabulary\n",
        "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
        "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
        "\n",
        "# Define special tokens with a specific order\n",
        "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
        "\n",
        "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
        "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
        "\n",
        "# Print the number of unique codes to verify\n",
        "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
        "\n",
        "# Print token to index mapping\n",
        "#print(\"Token to Index Mapping:\", token2idx)\n",
        "\n",
        "# Save the token2idx dictionary to a JSON file for later use\n",
        "with open('token2idx.json', 'w') as f:\n",
        "    json.dump(token2idx, f)\n",
        "\n",
        "    # for labels just make the full dict with all 1042 codes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we create a vocab dictionary for labels, excluding the special tokens, except for 'UNK', or unknown, which refers to an unknown or missing ICD code."
      ],
      "metadata": {
        "id": "ixzcGBUb5kAn"
      },
      "id": "ixzcGBUb5kAn"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5",
      "metadata": {
        "id": "5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed67ed3c-c392-45ba-c7a2-fede86d24e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1043\n"
          ]
        }
      ],
      "source": [
        "#create the label vocab\n",
        "#same as token vocab but no SEP, CLS, PAD, MASk (Leave UNK)\n",
        "# DID NOT reorganize indices, may need to\n",
        "\n",
        "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "labelVocab = {token: idx for token, idx in token2idx.items() if token not in special_tokens}\n",
        "\n",
        "\n",
        "def format_label_vocab(token2idx):\n",
        "    token2idx = token2idx.copy()\n",
        "    del token2idx['[PAD]']\n",
        "    del token2idx['[SEP]']\n",
        "    del token2idx['[CLS]']\n",
        "    del token2idx['[MASK]']\n",
        "    token = list(token2idx.keys())\n",
        "    labelVocab = {}\n",
        "    for i,x in enumerate(token):\n",
        "        labelVocab[x] = i\n",
        "    return labelVocab\n",
        "\n",
        "labelVocab = format_label_vocab(token2idx)\n",
        "\n",
        "# Print the new dictionary to verify\n",
        "print(len(labelVocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label and Feature Splits"
      ],
      "metadata": {
        "id": "tg25otPTxG77"
      },
      "id": "tg25otPTxG77"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First three lines of code used for filtering out patients with ICD-10 code -- commented out since it's not applicable for demo dataset."
      ],
      "metadata": {
        "id": "gqjEHETIBEwP"
      },
      "id": "gqjEHETIBEwP"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7",
      "metadata": {
        "id": "7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba6abcb-2a06-4d4f-e372-258ce59c10b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Features:  ['CLS', '5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582', 'SEP', '07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051', 'SEP', '07054', '78959', 'V462', '5715', '2767', '2761', '496', 'V08', '3051', '78791', 'SEP']\n",
            "length Labels:  11073\n",
            "example label 1043\n"
          ]
        }
      ],
      "source": [
        "#preparing the label and feature splits\n",
        "\n",
        "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
        "\n",
        "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
        "\n",
        "#Finds patients with only icd9 codes\n",
        "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
        "\n",
        "# icd9_only_df = diagnoses_df   # for demo data\n",
        "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
        "\n",
        "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
        "\n",
        "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
        "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
        "\n",
        "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])[icd_code_col_name].apply(list).reset_index()\n",
        "patient_visits = patient_visits.groupby('subject_id')[icd_code_col_name].apply(list)\n",
        "\n",
        "#extract all ICD9 codes but only take first 3 digits\n",
        "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
        "\n",
        "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
        "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
        "\n",
        "def encode_labels(label_codes, labelVocab):\n",
        "    # print(len(label_codes), len(labelVocab))\n",
        "    multi_hot = [0] * len(labelVocab)\n",
        "    #print(len)\n",
        "\n",
        "    for code in label_codes:\n",
        "        if code in labelVocab:\n",
        "            index = labelVocab[code]\n",
        "            multi_hot[index] = 1\n",
        "        else:\n",
        "            print(f\"Warning: Code {code} not found in labelVocab.\")\n",
        "\n",
        "    return multi_hot\n",
        "\n",
        "\n",
        "\n",
        "features = []\n",
        "labels = {}\n",
        "\n",
        "for subject_id, visits in patient_visits.items():\n",
        "    if len(visits) > 3:\n",
        "        # split_index = len(visits) // 2    # splits visits down middle\n",
        "\n",
        "        split_index = random.randint(len(visits)-2, len(visits)-1)    # random split index from 3 to end of visits array\n",
        "\n",
        "\n",
        "        # Initialize the feature list with 'CLS'\n",
        "        feature_visits = ['CLS']\n",
        "\n",
        "        # Append codes and 'SEP' after each visit up to the split index\n",
        "        for sublist in visits[:split_index]:\n",
        "            visit_codes = [code for code in sublist]\n",
        "            visit_codes.append('SEP')\n",
        "            feature_visits.extend(visit_codes)\n",
        "\n",
        "        # Append the feature list to the features\n",
        "        features.append(feature_visits)\n",
        "\n",
        "        # Gather label codes from visits after the split index\n",
        "        label_codes = [code[:3] for sublist in visits[split_index:] for code in sublist]\n",
        "\n",
        "        # Encode the labels and store using subject_id as key\n",
        "        labels[subject_id] = encode_labels(label_codes, labelVocab)\n",
        "\n",
        "\n",
        "#output for one set of features and labels\n",
        "if features and labels:\n",
        "    print(\"Example Features: \", features[0])\n",
        "    print(\"length Labels: \" , len(labels))\n",
        "    print(\"example label\", len(labels[list(patient_visits.items())[0][0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_UevB7i8K6Hi",
      "metadata": {
        "id": "_UevB7i8K6Hi"
      },
      "source": [
        "### Custom Dataset\n",
        "\n",
        "This custom dataset NextVisit prepares input sequences with appropriate tokens, masks, and labels for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "#custom dataset helpers\n",
        "\n",
        "def index_seg(tokens, symbol=2):\n",
        "    flag = 0\n",
        "    seg = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token == symbol:\n",
        "            seg.append(flag)\n",
        "            if flag == 0:\n",
        "                flag = 1\n",
        "            else:\n",
        "                flag = 0\n",
        "        else:\n",
        "            seg.append(flag)\n",
        "    return seg\n",
        "\n",
        "\n",
        "def position_idx(tokens, symbol=2):\n",
        "    pos = []\n",
        "    flag = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        if token == symbol:\n",
        "            pos.append(flag)\n",
        "            flag += 1\n",
        "        else:\n",
        "            pos.append(flag)\n",
        "    return pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "#custom dataset\n",
        "\n",
        "class NextVisit(Dataset):\n",
        "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
        "        self.token2idx = token2idx\n",
        "        self.labels = labels\n",
        "        self.patient_visits = patient_visits\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_visits)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # code_idxs, position idx, mask, segment, label\n",
        "        # Retrieve patient data by index\n",
        "        patient_id = list(self.patient_visits.keys())[index]\n",
        "        codes = self.patient_visits[patient_id]\n",
        "\n",
        "        # Initialize sequence with [CLS] token\n",
        "        sequence = [self.token2idx['[CLS]']]\n",
        "\n",
        "\n",
        "\n",
        "        # Add each code to the sequence and append [SEP] after each visit\n",
        "        for visit in codes:\n",
        "            #change 'code' to be truncated version!!!!!!\n",
        "            sequence.extend([self.token2idx.get(code[:3], self.token2idx['[UNK]']) for code in visit])\n",
        "            sequence.append(self.token2idx['[SEP]'])\n",
        "\n",
        "\n",
        "\n",
        "        # Cut or pad the sequence to the maximum length\n",
        "        if len(sequence) > self.max_len:\n",
        "            sequence = sequence[:self.max_len]\n",
        "        else:\n",
        "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
        "\n",
        "        position_indices = position_idx(sequence)\n",
        "        segment = index_seg(sequence)\n",
        "\n",
        "\n",
        "        # Create a mask for the sequence\n",
        "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
        "\n",
        "        # Prepare the labels\n",
        "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
        "\n",
        "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), torch.tensor(segment, dtype=torch.long), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9",
      "metadata": {
        "id": "9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "4908a944-f9be-403f-c546-1b5046b9fe9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n",
            "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2]\n",
            "512\n",
            "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "512\n",
            "position:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
            "512\n",
            "segment:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "512\n",
            "mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels:  tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\\nNeed to do version matching before feeding into model.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#testint stuff out\n",
        "\n",
        "max_len = 512\n",
        "\n",
        "patient_id = list(patient_visits.keys())[0]\n",
        "codes = patient_visits[patient_id]\n",
        "\n",
        "# Initialize sequence with [CLS] token\n",
        "sequence = [token2idx['[CLS]']]\n",
        "\n",
        "\n",
        "# Add each code to the sequence and append [SEP] after each visit\n",
        "for visit in codes:\n",
        "    sequence.extend([token2idx.get(code[:3], token2idx['[UNK]']) for code in visit])\n",
        "    sequence.append(token2idx['[SEP]'])\n",
        "\n",
        "print(len(sequence))\n",
        "print(sequence)\n",
        "\n",
        "\n",
        "# Cut or pad the sequence to the maximum length\n",
        "if len(sequence) > max_len:\n",
        "    sequence = sequence[:max_len]\n",
        "else:\n",
        "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
        "\n",
        "position_indices = position_idx(sequence)\n",
        "segment = index_seg(sequence)\n",
        "\n",
        "\n",
        "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
        "\n",
        "label = torch.tensor(labels[patient_id], dtype=torch.float)\n",
        "\n",
        "print(len(sequence))\n",
        "print(sequence)\n",
        "\n",
        "print(len(position_indices))\n",
        "print(\"position: \", position_indices)\n",
        "\n",
        "print(len(segment))\n",
        "print(\"segment: \", segment)\n",
        "\n",
        "print(len(mask))\n",
        "print(\"mask: \", mask)\n",
        "\n",
        "# print(len(labels[0]))\n",
        "print(\"labels: \" , label)\n",
        "\n",
        "'''\n",
        "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
        "Need to do version matching before feeding into model.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P4j-PxSk7ABm",
      "metadata": {
        "id": "P4j-PxSk7ABm"
      },
      "source": [
        "##*Model*\n",
        "\n",
        "###Original Paper: TransformEHR:\n",
        "* Citation: Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
        "* Repo: https://github.com/whaleloops/TransformEHR/tree/main\n",
        "\n",
        "\n",
        "###BEHRT\n",
        "* Citation: Li, Y., Rao, S., Solares, J.R.A. et al. BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y\n",
        "\n",
        "\n",
        "* Repo: https://github.com/deepmedicine/BEHRT/tree/master"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Description\n",
        "\n",
        "\n",
        "*   Citation to the original paper\n",
        "*   Link to the original paper’s repo (if applicable)\n",
        "*   Model descriptions\n",
        "\n",
        "###Model Description:\n",
        "\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "\n",
        "* Model architecture: layer number/size/type, activation function, etc\n",
        "* Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "* Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "* The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "\n",
        "## **If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.**"
      ],
      "metadata": {
        "id": "Z_MK-iXT5rBs"
      },
      "id": "Z_MK-iXT5rBs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation Code"
      ],
      "metadata": {
        "id": "PlpC53B95as5"
      },
      "id": "PlpC53B95as5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up parameters and configurations to use in model."
      ],
      "metadata": {
        "id": "acjhC1na5-Xc"
      },
      "id": "acjhC1na5-Xc"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertPreTrainedModel , BertModel\n",
        "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings\n",
        "\n",
        "global_params = {\n",
        "    'batch_size': 64,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'device': 'cuda',\n",
        "    'output_dir': '',  # output dir\n",
        "    'best_name': '', # output model name\n",
        "    'save_model': True,\n",
        "    'max_len_seq': 512,\n",
        "    'max_age': 110,\n",
        "    'month': 1,\n",
        "    'age_symbol': None,\n",
        "    'min_visit': 5\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'age': False,\n",
        "    'seg': False,\n",
        "    'posi': True\n",
        "}\n",
        "\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.02\n",
        "}\n",
        "model_config = {\n",
        "    'vocab_size': len(labelVocab), # number of disease + symbols for word embedding 1047\n",
        "    'hidden_size': 300, # word embedding and seg embedding hidden size\n",
        "    #'seg_vocab_size': 2, # number of vocab for seg embedding\n",
        "    #'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
        "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
        "    'hidden_dropout_prob': 0.3, # dropout rate\n",
        "    'num_hidden_layers': 4, # number of multi-head attention layers required\n",
        "    'num_attention_heads': 12, # number of attention heads\n",
        "    'attention_probs_dropout_prob': 0.45, # multi-head attention dropout rate\n",
        "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'hidden_act': 'relu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'initializer_range': 0.02, # parameter weight initializer range\n",
        "}\n",
        "\n",
        "class BertConfig(BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings = config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        #self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        #self.age_vocab_size = config.get('age_vocab_size')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
        "\n",
        "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "test_dataset = Subset(full_dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=global_params['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=global_params['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ICD9Embeddings:\n",
        "    def __init__(self, filename=\"./ic9_embeddings.txt\"):\n",
        "        self.embedding_filename = filename\n",
        "        # self.icd9_to_embeddings = self._read_embeddings()\n",
        "        self.icd9_to_embeddings = self._read_embeddings_trunc()\n",
        "        self.embedding_size = 300\n",
        "\n",
        "    # this reads the 5 digit code correctly\n",
        "    def _read_embeddings(self):\n",
        "        icd9_to_embeddings = {}\n",
        "        with open(self.embedding_filename, \"r\") as infile:\n",
        "            data = infile.readlines()\n",
        "            for row in data:\n",
        "                eles = row.strip().split(\" \")\n",
        "                name = eles[0]\n",
        "                embedding = eles[1:]\n",
        "                code = name[4:]\n",
        "\n",
        "                code = code.replace(\".\", \"\")\n",
        "                if len(code) > 5 or len(code) < 3:\n",
        "                    print(\"code is bad\")\n",
        "\n",
        "                icd9_to_embeddings[code] = torch.tensor(\n",
        "                    [float(i) for i in embedding], dtype=torch.float32\n",
        "                )\n",
        "        return icd9_to_embeddings\n",
        "\n",
        "    # this does the opposite of greedy it basically just takes the last icd_9 with the first three that match\n",
        "    def _read_embeddings_trunc(self):\n",
        "        icd9_to_embeddings = {}\n",
        "        codes_lost = 0\n",
        "        with open(self.embedding_filename, \"r\") as infile:\n",
        "            data = infile.readlines()\n",
        "            for row in data:\n",
        "                eles = row.strip().split(\" \")\n",
        "                name = eles[0]\n",
        "                embedding = eles[1:]\n",
        "                code = name[4:]\n",
        "\n",
        "                code = code.replace(\".\", \"\")\n",
        "                if len(code) > 5 or len(code) < 3:\n",
        "                    print(\"code is bad\")\n",
        "                trunc_code = code[:3]\n",
        "                # print(trunc_code)\n",
        "                if trunc_code in icd9_to_embeddings:\n",
        "                    codes_lost += 1\n",
        "\n",
        "                icd9_to_embeddings[trunc_code] = torch.tensor(\n",
        "                    [float(i) for i in embedding], dtype=torch.float32\n",
        "                )\n",
        "        # print('codes_lost', codes_lost)\n",
        "        return icd9_to_embeddings\n",
        "\n",
        "    def get(self, code):\n",
        "        if code in self.icd9_to_embeddings:\n",
        "            return self.icd9_to_embeddings[code]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_idx_to_embedding(self, token2idx):\n",
        "        idx2embedding = {}\n",
        "        for code, idx in token2idx.items():\n",
        "            if code in self.icd9_to_embeddings:\n",
        "                idx2embedding[idx] = self.icd9_to_embeddings[code]\n",
        "            else:\n",
        "                idx2embedding[idx] = torch.zeros(\n",
        "                    self.embedding_size, dtype=torch.float32\n",
        "                )\n",
        "                # print(\"code is not in icd9 embeddings\", code)\n",
        "\n",
        "        return idx2embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.feature_dict = feature_dict\n",
        "\n",
        "\n",
        "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups\n",
        "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
        "        self.idx2embedding = self.icd9_embeddings.get_idx_to_embedding(token2idx)\n",
        "\n",
        "\n",
        "        icd9_codes = list(self.idx2embedding.keys())\n",
        "        # print('length of icd9 codes', len(icd9_codes))\n",
        "\n",
        "        embeddings_matrix = torch.stack([self.idx2embedding[code] for code in icd9_codes], dim=0)\n",
        "        # print('embeddings matrix shape', embeddings_matrix.shape)\n",
        "\n",
        "        # Initialize embeddings for CLS and SEP tokens\n",
        "        additional_embeddings = torch.randn(2, config.hidden_size)\n",
        "\n",
        "        # Combine precomputed and additional embeddings\n",
        "        full_embeddings_matrix = torch.cat([embeddings_matrix, additional_embeddings], dim=0)\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=False)\n",
        "        # self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=True)\n",
        "        # self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)      # from scratch\n",
        "\n",
        "\n",
        "\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
        "\n",
        "\n",
        "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, word_ids, posi_ids=None, segment_ids=None):\n",
        "        # print('embeddings forward')\n",
        "        if segment_ids is None:\n",
        "            pass\n",
        "\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(word_ids)\n",
        "        # print('max word id', word_ids.max().item())\n",
        "        # print('embeddings size', self.word_embeddings.weight.shape)\n",
        "        # print(word_ids)\n",
        "        # print('word_ids shape', word_ids.shape)\n",
        "\n",
        "        embeddings = self.word_embeddings(word_ids)\n",
        "        # print('embeddings',embeddings)\n",
        "\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "        if self.feature_dict['posi']:\n",
        "            embeddings = embeddings + posi_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "\n",
        "# this should be fairly hands off we should just need to adjust the config parameters\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, posi_ids=None, attention_mask=None,segment_ids=None,output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(posi_ids)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "# this should be fairly hands off we should just need to adjust the config parameters\n",
        "class BertForMultiLabelPrediction(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, feature_dict):\n",
        "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, feature_dict)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, posi_ids=None, attention_mask=None, segment_ids=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,segment_ids,\n",
        "                                     output_all_encoded_layers=False)\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
        "\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "conf = BertConfig(model_config)\n",
        "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "model = model.to(global_params['device'])\n",
        "# model = model.to(\"cuda\")\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=3e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zx0JTvnb7ZKG",
      "metadata": {
        "id": "zx0JTvnb7ZKG"
      },
      "source": [
        "## *Evaluation Metric Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "def precision(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    label, output=label.cpu(), output.detach().cpu()\n",
        "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, output, label\n",
        "\n",
        "def accuracy(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "\n",
        "    # Apply a threshold to convert probabilities to binary predictions\n",
        "    predictions = (output.numpy() > 0.5).astype(int)\n",
        "    # Calculate accuracy\n",
        "    acc = sklearn.metrics.accuracy_score(label.numpy(), predictions)\n",
        "\n",
        "    # # Calculate accuracy for each label independently\n",
        "    # label_accuracies = []\n",
        "\n",
        "    # for key in labels:\n",
        "    #   label_array = labels[key]  # Get the label array corresponding to the key\n",
        "    #   prediction_array = predictions[key]  # Get the prediction array corresponding to the key\n",
        "\n",
        "    #   # Calculate accuracy for this label category\n",
        "    #   acc = sklearn.metrics.accuracy_score(label_array, prediction_array)\n",
        "    #   label_accuracies.append(acc)\n",
        "    # # Take mean of all accuracies\n",
        "    # overall_acc = np.mean(label_accuracies)\n",
        "\n",
        "\n",
        "\n",
        "    return acc, output, label\n",
        "\n",
        "def print_result(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "\n",
        "    # predictions = (output.numpy() > 0.5).astype(int)\n",
        "    predictions = np.argsort(output.numpy())\n",
        "    predictions = predictions[::-1]\n",
        "\n",
        "\n",
        "    # print('predicted diseases',     np.where(predictions == 1)[:5])\n",
        "    print('predicted diseases',     predictions[:5])\n",
        "    # print(output.numpy()[predictions[:5]])\n",
        "    print('actual diseases',     np.where(label == 1)[:5])\n",
        "    # print('label',     label)\n",
        "\n",
        "\n",
        "def precision_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, roc, output, label,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s45Dvanq7htd",
      "metadata": {
        "id": "s45Dvanq7htd"
      },
      "source": [
        "## *Training*\n",
        "\n",
        "\n",
        "\n",
        "**Hyperparameters**\n",
        "are set in variables `global_params`, `model_config`, and `optim_config`, such as:\n",
        "* Batch Size: 64\n",
        "* Hidden Size: 300\n",
        "* Number of Hidden Layers: 6\n",
        "* Dropout Rate: 0.2\n",
        "  * finetuned to 0.5\n",
        "\n",
        "**Computational Requirements:**\n",
        "* Hardware type: GPU T4\n",
        "* Avg runtime per epoch: 2 min.\n",
        "* GPU units used: 40\n",
        "* Number of training epochs: 25 or 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Code"
      ],
      "metadata": {
        "id": "BZ0OXUuvBXI_"
      },
      "id": "BZ0OXUuvBXI_"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "def train(e):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    temp_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        cnt +=1\n",
        "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
        "\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] >1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "\n",
        "        temp_loss += loss.item()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if step % 50 ==0:\n",
        "            prec, a, b = precision(logits, targets)\n",
        "            acc, a, b = accuracy(logits,targets)\n",
        "            print_result(logits[2], targets[2])\n",
        "            # acc = 0\n",
        "\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\\t| Accuracy: {} \".format(e, cnt,temp_loss/500, prec, acc))\n",
        "            temp_loss = 0\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EKA1_ZCy9MuE",
      "metadata": {
        "id": "EKA1_ZCy9MuE"
      },
      "source": [
        "## *Evaluation*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Description\n",
        "\n",
        "Our model measures the following metrics:\n",
        "* Precision\n",
        "* Accuracy\n",
        "* Evaluation Loss\n",
        "\n",
        "The calculation functions for these precision and accuracy can be found above, while loss is calculated in the `evaluation()` function below."
      ],
      "metadata": {
        "id": "Hkx4sfPUD0Lv"
      },
      "id": "Hkx4sfPUD0Lv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Code"
      ],
      "metadata": {
        "id": "NlEwjnmiC7NQ"
      },
      "id": "NlEwjnmiC7NQ"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "A4XMXPK_9PQk",
      "metadata": {
        "id": "A4XMXPK_9PQk"
      },
      "outputs": [],
      "source": [
        "def evaluation():\n",
        "    model.eval()\n",
        "    y = []\n",
        "    y_label = []\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(test_loader):\n",
        "        model.eval()\n",
        "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
        "\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "          loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
        "        logits = logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    aps, roc, output, label = precision_test(y, y_label)\n",
        "    return aps, roc, tr_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Output"
      ],
      "metadata": {
        "id": "y1CcduZDE7nN"
      },
      "id": "y1CcduZDE7nN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading presaved/trained model:"
      ],
      "metadata": {
        "id": "ojMP4AyGNSLu"
      },
      "id": "ojMP4AyGNSLu"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "AO26sXOFHrX9",
      "metadata": {
        "id": "AO26sXOFHrX9"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint_path = '/content/gdrive/MyDrive/CS 598 Project/598ehrupload/saved_models/best_model.pt'\n",
        "checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is for running the model itself."
      ],
      "metadata": {
        "id": "cPAb1wwQFEk0"
      },
      "id": "cPAb1wwQFEk0"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "19",
      "metadata": {
        "id": "19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17b0767-1233-456d-c192-09948bba867b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training...\n",
            "predicted diseases [204 984 467 122  60]\n",
            "actual diseases (array([ 38, 222, 626, 962, 968]),)\n",
            "epoch: 0\t| Cnt: 1\t| Loss: 0.0013867653608322145\t| precision: 0.016889033862222534\t| Accuracy: 0.0 \n",
            "predicted diseases [296 136 193 935   3]\n",
            "actual diseases (array([248, 373, 382, 390, 485, 725, 751, 752, 941]),)\n",
            "epoch: 0\t| Cnt: 51\t| Loss: 0.0633219518661499\t| precision: 0.01797243947088268\t| Accuracy: 0.0 \n",
            "predicted diseases [284 252 803 950 158]\n",
            "actual diseases (array([272, 660]),)\n",
            "epoch: 0\t| Cnt: 101\t| Loss: 0.04557139217853546\t| precision: 0.028687616394863533\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.09175973328293734\n",
            "starting training...\n",
            "predicted diseases [248 373  95 950 158]\n",
            "actual diseases (array([373, 725]),)\n",
            "epoch: 1\t| Cnt: 1\t| Loss: 0.0005324169397354126\t| precision: 0.05734600303877332\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  178  373 1009  996]\n",
            "actual diseases (array([ 222,  248,  520,  522,  676,  746,  929,  930,  937,  941,  967,\n",
            "       1009]),)\n",
            "epoch: 1\t| Cnt: 51\t| Loss: 0.02263215172290802\t| precision: 0.1429483091962115\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  996  248  226 1009]\n",
            "actual diseases (array([ 261,  306,  375,  384,  391,  392,  393,  394,  473,  485,  509,\n",
            "        529,  530,  646,  927,  940, 1009]),)\n",
            "epoch: 1\t| Cnt: 101\t| Loss: 0.017171246618032456\t| precision: 0.19015942414223397\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.21763478216207796\n",
            "starting training...\n",
            "predicted diseases [ 373  248 1009  226  996]\n",
            "actual diseases (array([ 226,  248,  252,  261,  339,  375,  394,  508,  529,  530,  663,\n",
            "        672,  941, 1005]),)\n",
            "epoch: 2\t| Cnt: 1\t| Loss: 0.00026072543859481813\t| precision: 0.21481681431599764\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  261  382]\n",
            "actual diseases (array([226, 248, 373, 394, 405, 544, 646, 719, 996]),)\n",
            "epoch: 2\t| Cnt: 51\t| Loss: 0.01221245051920414\t| precision: 0.22447246882871477\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248  252 1009  996]\n",
            "actual diseases (array([ 178,  250,  252,  375,  384,  529,  530,  654,  663,  672,  722,\n",
            "       1000]),)\n",
            "epoch: 2\t| Cnt: 101\t| Loss: 0.010648300990462304\t| precision: 0.22754966902886725\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.24784940412993775\n",
            "starting training...\n",
            "predicted diseases [ 373  248 1009  967  261]\n",
            "actual diseases (array([161, 625, 964]),)\n",
            "epoch: 3\t| Cnt: 1\t| Loss: 0.00018084481358528139\t| precision: 0.24947017685176448\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  261  252  996 1009]\n",
            "actual diseases (array([  38,   62,  222,  226,  252,  254,  260,  261,  263,  330,  331,\n",
            "        336,  339,  360,  375,  485,  516,  517,  529,  530,  544,  653,\n",
            "        654,  672,  727,  759,  926,  940,  941,  964,  994, 1009]),)\n",
            "epoch: 3\t| Cnt: 51\t| Loss: 0.008843778476119042\t| precision: 0.24959974277171787\t| Accuracy: 0.0 \n",
            "predicted diseases [373 252 248 996 226]\n",
            "actual diseases (array([154]),)\n",
            "epoch: 3\t| Cnt: 101\t| Loss: 0.00823304782807827\t| precision: 0.25158073172526874\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2576988885441498\n",
            "starting training...\n",
            "predicted diseases [ 373  996  248  252 1009]\n",
            "actual diseases (array([602, 606, 640, 979]),)\n",
            "epoch: 4\t| Cnt: 1\t| Loss: 0.00015010185539722444\t| precision: 0.2408242198104307\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  393  996]\n",
            "actual diseases (array([  38,  222,  242,  250,  252,  261,  303,  373,  375,  378,  380,\n",
            "        382,  384,  390,  393,  394,  455,  485,  530,  544,  568,  662,\n",
            "        663,  672,  962,  964,  968,  996, 1000, 1009]),)\n",
            "epoch: 4\t| Cnt: 51\t| Loss: 0.007320891678333282\t| precision: 0.24473241705108942\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  226  252]\n",
            "actual diseases (array([ 189,  248,  250,  261,  303,  306,  373,  375,  382,  384,  393,\n",
            "        394,  523,  530,  555,  725,  729,  994,  996, 1000, 1004, 1009]),)\n",
            "epoch: 4\t| Cnt: 101\t| Loss: 0.007001812681555748\t| precision: 0.23339788157944927\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.26145902383889025\n",
            "starting training...\n",
            "predicted diseases [ 373  248 1009  226  394]\n",
            "actual diseases (array([ 213,  222,  226,  252,  256,  264,  331,  373,  390,  393,  394,\n",
            "        514,  561,  719,  727,  729,  853,  945,  946,  967,  994, 1009]),)\n",
            "epoch: 5\t| Cnt: 1\t| Loss: 0.00013592086732387544\t| precision: 0.24706040386688322\t| Accuracy: 0.0 \n",
            "predicted diseases [373 996 248 226 261]\n",
            "actual diseases (array([  98,  226,  239,  248,  252,  254,  264,  270,  287,  373,  393,\n",
            "        394,  452,  455,  485,  489,  490,  529,  646,  719,  726,  728,\n",
            "        925,  926,  927,  940,  962,  995,  996, 1000, 1009, 1017, 1036]),)\n",
            "epoch: 5\t| Cnt: 51\t| Loss: 0.006512112222611904\t| precision: 0.2541678733046023\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 226 393 375]\n",
            "actual diseases (array([ 183,  222,  248,  261,  263,  336,  363,  371,  375,  382,  390,\n",
            "        446,  530,  719,  721,  851,  941,  964,  967,  996, 1038]),)\n",
            "epoch: 5\t| Cnt: 101\t| Loss: 0.006326869562268257\t| precision: 0.23895690940824316\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2628331741496353\n",
            "starting training...\n",
            "predicted diseases [ 373 1009  248  393  996]\n",
            "actual diseases (array([ 226,  252,  256,  331,  373,  408,  490,  491,  523,  529,  541,\n",
            "        727,  730,  964, 1000, 1009, 1015]),)\n",
            "epoch: 6\t| Cnt: 1\t| Loss: 0.00011920479685068131\t| precision: 0.2492151825249634\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 226 996 261]\n",
            "actual diseases (array([303, 373, 485, 509, 668, 719, 721, 725]),)\n",
            "epoch: 6\t| Cnt: 51\t| Loss: 0.0059950100928544995\t| precision: 0.24170158769501826\t| Accuracy: 0.0 \n",
            "predicted diseases [373 967 393 226 252]\n",
            "actual diseases (array([261, 269, 270, 339, 375, 381, 382, 390, 394, 419, 509, 529, 530,\n",
            "       663, 725]),)\n",
            "epoch: 6\t| Cnt: 101\t| Loss: 0.005905072271823883\t| precision: 0.24002430922477433\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2636187753680913\n",
            "starting training...\n",
            "predicted diseases [1009  373  248  996  226]\n",
            "actual diseases (array([  98,  222,  239,  252,  261,  375,  420,  488,  516,  517,  530,\n",
            "        649,  672,  719,  725,  726,  728,  732,  964, 1004, 1006, 1036]),)\n",
            "epoch: 7\t| Cnt: 1\t| Loss: 0.00012050196528434753\t| precision: 0.27331497732868637\t| Accuracy: 0.0 \n",
            "predicted diseases [1009  373  248  226  252]\n",
            "actual diseases (array([ 261,  270,  287,  375,  382,  393,  394,  406,  407,  529,  530,\n",
            "        646,  672,  719,  906,  938,  993,  994,  996, 1000]),)\n",
            "epoch: 7\t| Cnt: 51\t| Loss: 0.005704027161002159\t| precision: 0.2835828092285154\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 252 485 382]\n",
            "actual diseases (array([ 182,  226,  263,  375,  382,  393,  394,  530,  658,  719,  839,\n",
            "        941,  996, 1009]),)\n",
            "epoch: 7\t| Cnt: 101\t| Loss: 0.0055903993546962735\t| precision: 0.2524108104663309\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2642808445823058\n",
            "starting training...\n",
            "predicted diseases [373 248 226 261 996]\n",
            "actual diseases (array([ 226,  248,  250,  329,  393,  394,  411,  509,  516,  544,  664,\n",
            "        995, 1000]),)\n",
            "epoch: 8\t| Cnt: 1\t| Loss: 0.0001103571429848671\t| precision: 0.21576246255218218\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  252  226]\n",
            "actual diseases (array([138, 248, 264, 373, 380, 382, 503, 719, 964, 996]),)\n",
            "epoch: 8\t| Cnt: 51\t| Loss: 0.0054717701599001885\t| precision: 0.27761125817902166\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373 1009  252  382]\n",
            "actual diseases (array([248, 373, 393]),)\n",
            "epoch: 8\t| Cnt: 101\t| Loss: 0.0053650129362940785\t| precision: 0.28105821624239014\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2645365461951885\n",
            "starting training...\n",
            "predicted diseases [ 252 1009  226  373  261]\n",
            "actual diseases (array([252, 256, 276, 312, 320, 373, 490, 726, 728, 967]),)\n",
            "epoch: 9\t| Cnt: 1\t| Loss: 0.00010568426549434661\t| precision: 0.24921904661552943\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373  226 1009  393]\n",
            "actual diseases (array([226, 331, 336, 375, 382, 411, 442, 474, 529, 530, 646, 967, 995,\n",
            "       996]),)\n",
            "epoch: 9\t| Cnt: 51\t| Loss: 0.0053328812494874\t| precision: 0.2652062898796602\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  485  252]\n",
            "actual diseases (array([ 226,  248,  252,  303,  322,  373,  393,  463,  466,  473,  514,\n",
            "        529,  544,  625,  719,  726,  962,  964,  995, 1006]),)\n",
            "epoch: 9\t| Cnt: 101\t| Loss: 0.005247665755450726\t| precision: 0.24318095395434486\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.26484065557978653\n",
            "starting training...\n",
            "predicted diseases [373 248 261 226 252]\n",
            "actual diseases (array([ 252,  287,  411,  420,  516,  517,  521,  538,  692,  728,  964,\n",
            "        967, 1009, 1017]),)\n",
            "epoch: 10\t| Cnt: 1\t| Loss: 0.00010933341085910797\t| precision: 0.2765528086694414\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 261 226 996]\n",
            "actual diseases (array([222, 287, 375, 393, 530, 751, 790, 791, 806, 941, 996]),)\n",
            "epoch: 10\t| Cnt: 51\t| Loss: 0.005198059663176537\t| precision: 0.2851328526247919\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248  226  996 1009]\n",
            "actual diseases (array([248, 252, 332, 373, 375, 382, 392, 393, 394, 530, 538, 545, 719,\n",
            "       927, 940, 967, 993, 994, 996]),)\n",
            "epoch: 10\t| Cnt: 101\t| Loss: 0.005185579471290111\t| precision: 0.21276474540103452\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.26543733939058833\n",
            "starting training...\n",
            "predicted diseases [ 373  996 1009  248  226]\n",
            "actual diseases (array([ 166,  175,  248,  261,  373,  375,  381,  382,  417,  485,  530,\n",
            "        538,  719,  728,  946,  962,  967,  995,  996, 1009]),)\n",
            "epoch: 11\t| Cnt: 1\t| Loss: 9.84429270029068e-05\t| precision: 0.26273290823099105\t| Accuracy: 0.0 \n",
            "predicted diseases [248 261 373 996 485]\n",
            "actual diseases (array([ 226,  261,  270,  305,  323,  393,  544, 1009]),)\n",
            "epoch: 11\t| Cnt: 51\t| Loss: 0.005141447268426419\t| precision: 0.2563818304976787\t| Accuracy: 0.0 \n",
            "predicted diseases [1009  252  248  996  373]\n",
            "actual diseases (array([276, 281, 287, 320, 490, 509, 663, 668, 725, 728, 964, 996]),)\n",
            "epoch: 11\t| Cnt: 101\t| Loss: 0.005067794129252434\t| precision: 0.23189218400017922\t| Accuracy: 0.0 \n",
            "aps : 0.2651369864423494\n",
            "starting training...\n",
            "predicted diseases [ 373  248 1009  382  226]\n",
            "actual diseases (array([ 38, 248, 276, 319, 373, 485, 544, 727, 747, 941, 962, 994]),)\n",
            "epoch: 12\t| Cnt: 1\t| Loss: 9.361644834280013e-05\t| precision: 0.27243873464872864\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 996 261 226]\n",
            "actual diseases (array([ 62, 222, 254, 272, 276, 280, 281, 287, 312, 373, 390, 394, 429,\n",
            "       529, 625, 658, 666, 927, 937, 940]),)\n",
            "epoch: 12\t| Cnt: 51\t| Loss: 0.0050652318820357325\t| precision: 0.2506530278734085\t| Accuracy: 0.0 \n",
            "predicted diseases [ 996  373  252 1009  226]\n",
            "actual diseases (array([  8, 373, 452, 654, 658, 663, 728, 927, 934, 937, 940, 996]),)\n",
            "epoch: 12\t| Cnt: 101\t| Loss: 0.004993663795292378\t| precision: 0.25436544434921016\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.26559480546928826\n",
            "starting training...\n",
            "predicted diseases [ 373  248  996  967 1009]\n",
            "actual diseases (array([ 62, 248, 375, 394, 530, 533, 672, 925]),)\n",
            "epoch: 13\t| Cnt: 1\t| Loss: 9.030698239803315e-05\t| precision: 0.23247664989351777\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248 1009  252  373  996]\n",
            "actual diseases (array([ 256,  268,  276,  280,  281,  646,  658,  729, 1013]),)\n",
            "epoch: 13\t| Cnt: 51\t| Loss: 0.004986236892640591\t| precision: 0.2879614550174695\t| Accuracy: 0.0 \n",
            "predicted diseases [373 996 248 530 382]\n",
            "actual diseases (array([248, 251, 374, 393, 394, 651, 654, 729]),)\n",
            "epoch: 13\t| Cnt: 101\t| Loss: 0.0049955201223492625\t| precision: 0.23516757572276184\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2656496804892764\n",
            "starting training...\n",
            "predicted diseases [373 248 226 252 996]\n",
            "actual diseases (array([ 248,  261,  268,  270,  336,  382,  393,  394,  485,  725,  729,\n",
            "        927,  946,  962,  966,  967,  994, 1009]),)\n",
            "epoch: 14\t| Cnt: 1\t| Loss: 9.739318490028381e-05\t| precision: 0.27339892724920606\t| Accuracy: 0.0 \n",
            "predicted diseases [248 373 996 252 967]\n",
            "actual diseases (array([ 48,  80, 222, 248, 252, 276, 373, 429, 473, 485, 529, 530, 544,\n",
            "       719, 723, 925, 940, 946, 962, 967]),)\n",
            "epoch: 14\t| Cnt: 51\t| Loss: 0.0049385991394519805\t| precision: 0.272850029070749\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  996  967]\n",
            "actual diseases (array([ 163,  248,  373,  544,  545,  552, 1009]),)\n",
            "epoch: 14\t| Cnt: 101\t| Loss: 0.004963316015899181\t| precision: 0.2713809689729787\t| Accuracy: 0.0 \n",
            "aps : 0.26526797651427114\n",
            "starting training...\n",
            "predicted diseases [ 373  248  261 1009  252]\n",
            "actual diseases (array([ 373,  393,  964, 1009, 1022]),)\n",
            "epoch: 15\t| Cnt: 1\t| Loss: 9.819585084915161e-05\t| precision: 0.28131456267775107\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373 1009  248  996  226]\n",
            "actual diseases (array([ 222,  250,  254,  287,  303,  375,  393,  394,  442,  450,  473,\n",
            "        474,  530,  726,  928,  940,  954,  961,  962,  994, 1009]),)\n",
            "epoch: 15\t| Cnt: 51\t| Loss: 0.004925183765590191\t| precision: 0.27468593245693645\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 226 996 252]\n",
            "actual diseases (array([ 272,  276,  312,  509,  663,  672,  726,  728,  964,  996, 1009]),)\n",
            "epoch: 15\t| Cnt: 101\t| Loss: 0.0048810155168175695\t| precision: 0.27131729128913146\t| Accuracy: 0.0 \n",
            "aps : 0.265322879879285\n",
            "starting training...\n",
            "predicted diseases [ 248 1009  373  252  226]\n",
            "actual diseases (array([284, 373, 489, 494, 514, 625, 927, 966, 967, 969, 996]),)\n",
            "epoch: 16\t| Cnt: 1\t| Loss: 9.759770333766938e-05\t| precision: 0.22507882496822773\t| Accuracy: 0.0 \n",
            "predicted diseases [373 248 996 261 393]\n",
            "actual diseases (array([248, 251, 252, 264, 266, 373, 390, 392, 393, 403, 485, 651, 655,\n",
            "       660]),)\n",
            "epoch: 16\t| Cnt: 51\t| Loss: 0.004885704509913921\t| precision: 0.27124397317608817\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248  226  996 1009]\n",
            "actual diseases (array([  62,  251,  252,  257,  274,  281,  285,  305,  312,  370,  371,\n",
            "        373,  392,  393,  394,  408,  438,  485,  490,  502,  516,  518,\n",
            "        552,  636,  658,  663,  672,  676,  695,  967,  996, 1009, 1011]),)\n",
            "epoch: 16\t| Cnt: 101\t| Loss: 0.00481155114620924\t| precision: 0.24707961729721634\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2657158145947362\n",
            "starting training...\n",
            "predicted diseases [373 996 226 248 394]\n",
            "actual diseases (array([ 226,  248,  256,  287,  331,  336,  373,  375,  393,  417,  473,\n",
            "        530,  719,  738,  790,  941,  962, 1009]),)\n",
            "epoch: 17\t| Cnt: 1\t| Loss: 9.309972822666168e-05\t| precision: 0.2792638315929478\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373  261 1009  996]\n",
            "actual diseases (array([146, 248, 264, 373, 383, 393, 466, 529, 530, 668, 727, 964]),)\n",
            "epoch: 17\t| Cnt: 51\t| Loss: 0.004865874394774437\t| precision: 0.2967964509055047\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  226  252]\n",
            "actual diseases (array([ 35, 303, 339, 373, 625, 646, 650, 653, 724, 924, 925, 940, 994]),)\n",
            "epoch: 17\t| Cnt: 101\t| Loss: 0.00486751165241003\t| precision: 0.2331831485897076\t| Accuracy: 0.0 \n",
            "aps : 0.2653903139697472\n",
            "starting training...\n",
            "predicted diseases [ 248  373  382  485 1009]\n",
            "actual diseases (array([222, 303, 373, 405, 485, 725, 847, 940, 962, 964, 967]),)\n",
            "epoch: 18\t| Cnt: 1\t| Loss: 8.948967605829239e-05\t| precision: 0.25487406488103476\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248  226  996 1009]\n",
            "actual diseases (array([  38,  226,  248,  261,  262,  276,  287,  311,  320,  336,  375,\n",
            "        394,  442,  473,  485,  488,  491,  529,  530,  544,  586,  589,\n",
            "        591,  592,  597,  672,  724,  735,  968,  975, 1009]),)\n",
            "epoch: 18\t| Cnt: 51\t| Loss: 0.004796634621918201\t| precision: 0.24482987515440188\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373  261 1009  485]\n",
            "actual diseases (array([226, 237, 261, 266, 269, 375, 403, 530, 544, 545, 727, 925, 940]),)\n",
            "epoch: 18\t| Cnt: 101\t| Loss: 0.004869738176465035\t| precision: 0.2658449043668083\t| Accuracy: 0.0 \n",
            "aps : 0.2654660900383233\n",
            "starting training...\n",
            "predicted diseases [373 248 252 996 967]\n",
            "actual diseases (array([ 226,  252,  287,  373,  393,  394,  404,  523,  544,  646,  654,\n",
            "        657,  800,  945,  964,  995,  996, 1009]),)\n",
            "epoch: 19\t| Cnt: 1\t| Loss: 8.925940096378326e-05\t| precision: 0.25919644721718527\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373  996  226 1009]\n",
            "actual diseases (array([261, 393, 519]),)\n",
            "epoch: 19\t| Cnt: 51\t| Loss: 0.004807298444211483\t| precision: 0.27187583884060007\t| Accuracy: 0.0 \n",
            "predicted diseases [373 252 967 248 226]\n",
            "actual diseases (array([  38,  102,  226,  250,  252,  269,  287,  297,  306,  331,  336,\n",
            "        339,  375,  392,  393,  394,  529,  530,  535,  536,  540,  541,\n",
            "        544,  545,  646,  672,  729,  824,  925,  940,  941,  946,  993,\n",
            "       1009]),)\n",
            "epoch: 19\t| Cnt: 101\t| Loss: 0.004807018436491489\t| precision: 0.25559158135047894\t| Accuracy: 0.0 \n",
            "aps : 0.26535510866073037\n",
            "starting training...\n",
            "predicted diseases [ 373  248 1009  226  996]\n",
            "actual diseases (array([226, 336, 591, 597, 979]),)\n",
            "epoch: 20\t| Cnt: 1\t| Loss: 9.500892460346222e-05\t| precision: 0.271849699073883\t| Accuracy: 0.0 \n",
            "predicted diseases [ 248  373  261 1009  226]\n",
            "actual diseases (array([252, 261, 287, 375, 382, 394, 529, 530, 725, 996]),)\n",
            "epoch: 20\t| Cnt: 51\t| Loss: 0.0048167057931423184\t| precision: 0.23460033701479793\t| Accuracy: 0.0 \n",
            "predicted diseases [248 373 226 382 394]\n",
            "actual diseases (array([ 226,  248,  250,  373,  656,  658, 1009]),)\n",
            "epoch: 20\t| Cnt: 101\t| Loss: 0.004783412732183933\t| precision: 0.26001063596817586\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.2657705935346301\n",
            "starting training...\n",
            "predicted diseases [ 373  996  248 1009  226]\n",
            "actual diseases (array([226, 248, 250, 287, 312, 325, 373, 394, 422, 452, 485, 529, 806,\n",
            "       941, 996]),)\n",
            "epoch: 21\t| Cnt: 1\t| Loss: 9.6000075340271e-05\t| precision: 0.23046604966527268\t| Accuracy: 0.0 \n",
            "predicted diseases [248 373 252 226 485]\n",
            "actual diseases (array([  35,   38,  226,  239,  248,  249,  261,  263,  270,  373,  375,\n",
            "        380,  382,  384,  390,  391,  393,  394,  417,  448,  465,  473,\n",
            "        491,  502,  512,  529,  530,  532,  535,  538,  540,  625,  646,\n",
            "        719,  722,  724,  726,  738,  924,  925,  937,  940,  954,  964,\n",
            "        967,  994,  996, 1006, 1009, 1015, 1036]),)\n",
            "epoch: 21\t| Cnt: 51\t| Loss: 0.004773765981197357\t| precision: 0.2639045708622061\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373 1009  967  393  261]\n",
            "actual diseases (array([  8, 181, 206, 251, 252, 262, 263, 373, 390, 491, 509, 719, 946,\n",
            "       993]),)\n",
            "epoch: 21\t| Cnt: 101\t| Loss: 0.004780613541603088\t| precision: 0.2738148325850241\t| Accuracy: 0.0 \n",
            "aps : 0.2657323689220106\n",
            "starting training...\n",
            "predicted diseases [ 373  248  226  996 1009]\n",
            "actual diseases (array([ 248,  252,  261,  263,  320,  373,  393,  416,  422,  485,  504,\n",
            "        508,  654,  719,  926,  940,  995,  996, 1009, 1039]),)\n",
            "epoch: 22\t| Cnt: 1\t| Loss: 9.605176001787186e-05\t| precision: 0.23840249917720255\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  996  248 1009  226]\n",
            "actual diseases (array([  8,  35, 239, 249, 260, 272, 393, 446, 721, 924]),)\n",
            "epoch: 22\t| Cnt: 51\t| Loss: 0.004799872569739819\t| precision: 0.26669983076158627\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  252  248 1009  261]\n",
            "actual diseases (array([226, 314, 324, 373, 382, 455, 473, 544, 545, 625, 646, 657, 719,\n",
            "       762, 834, 941, 946, 996, 997]),)\n",
            "epoch: 22\t| Cnt: 101\t| Loss: 0.00472324113547802\t| precision: 0.2379021182404709\t| Accuracy: 0.0 \n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "aps : 0.26577124060123086\n",
            "starting training...\n",
            "predicted diseases [ 996  248  373 1009  967]\n",
            "actual diseases (array([373, 485, 668, 725, 728, 964, 994]),)\n",
            "epoch: 23\t| Cnt: 1\t| Loss: 9.438838064670562e-05\t| precision: 0.27775887498491597\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  252  248  996 1009]\n",
            "actual diseases (array([ 98, 226, 248, 252, 276, 371, 373, 384, 393, 394, 442, 473, 529,\n",
            "       684, 724, 729, 888, 940, 997]),)\n",
            "epoch: 23\t| Cnt: 51\t| Loss: 0.004719361916184425\t| precision: 0.24881624618730516\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248  996  252 1009]\n",
            "actual diseases (array([ 226,  242,  248,  252,  281,  375,  380,  382,  393,  394,  408,\n",
            "        450,  452,  529,  530,  661,  759,  941,  964,  996, 1000, 1009]),)\n",
            "epoch: 23\t| Cnt: 101\t| Loss: 0.004793178431689739\t| precision: 0.26510747889026703\t| Accuracy: 0.0 \n",
            "aps : 0.2654531006230435\n",
            "starting training...\n",
            "predicted diseases [373 967 248 996 226]\n",
            "actual diseases (array([ 226, 1021]),)\n",
            "epoch: 24\t| Cnt: 1\t| Loss: 8.644770085811615e-05\t| precision: 0.22933561494304855\t| Accuracy: 0.0 \n",
            "predicted diseases [ 373  248 1009  393  226]\n",
            "actual diseases (array([  38,   39,   62,  248,  264,  272,  277,  281,  373,  382,  393,\n",
            "        519,  521,  660,  719,  722,  726,  727,  738, 1015, 1036]),)\n",
            "epoch: 24\t| Cnt: 51\t| Loss: 0.004772570542991161\t| precision: 0.2656824386894485\t| Accuracy: 0.0 \n",
            "predicted diseases [248 252 373 996 226]\n",
            "actual diseases (array([ 38, 226, 252, 312, 331, 373, 529, 625, 646, 660, 956, 967]),)\n",
            "epoch: 24\t| Cnt: 101\t| Loss: 0.004751295991241932\t| precision: 0.2320266148148335\t| Accuracy: 0.0 \n",
            "aps : 0.2654716112919388\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "\n",
        "# # If using Google Drive\n",
        "# base_dir = '/content/gdrive/MyDrive/598ehrupload/saved_models'\n",
        "# # If using local Colab space\n",
        "# # base_dir = '/content/ModelDirectory'\n",
        "\n",
        "# global_params['output_dir'] = base_dir\n",
        "# global_params['best_name'] = 'best_model_unfrozen_embeddings_finetune.pt'\n",
        "\n",
        "# def create_folder(path):\n",
        "#     if not os.path.exists(path):\n",
        "#         os.makedirs(path)\n",
        "\n",
        "# best_pre = 0.0\n",
        "# for e in range(25):\n",
        "#     print(\"starting training...\")\n",
        "#     train(e)\n",
        "#     aps, roc, test_loss = evaluation()\n",
        "#     if aps >best_pre:\n",
        "#         # Save a trained model\n",
        "#         print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "#         model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "#         output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
        "#         create_folder(global_params['output_dir'])\n",
        "\n",
        "#         torch.save(model_to_save.state_dict(), output_model_file)\n",
        "#         best_pre = aps\n",
        "#     print('aps : {}'.format(aps))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUoYWE3t77Rm",
      "metadata": {
        "id": "ZUoYWE3t77Rm"
      },
      "source": [
        "# **Results**\n",
        "\n",
        "Results (15)\n",
        "\n",
        "* Table of results (no need to include additional experiments, but main reproducibility result should be included)\n",
        "* All claims should be supported by experiment results\n",
        "* Discuss with respect to the hypothesis and results from the original paper\n",
        "* Experiments beyond the original paper\n",
        "* Credits for each experiment depend on how hard it is to run the experiments. Each experiment should include results and discussion\n",
        "* Ablation Study.\n",
        "\n",
        "ablations\n",
        "- freeze embeddings\n",
        "- no freeze embeddings\n",
        "- from sratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMUiojNT8MD3",
      "metadata": {
        "id": "mMUiojNT8MD3"
      },
      "source": [
        "# **Discussion**\n",
        "\n",
        "As previously mentioned, due to the missing code from our original paper, TransformEHR, we found that it was *not reproducible*. Since we did not have access to the data collator functions or the tokenizers that the authors used to prepare the data, we were not able to determine the exact format needed to run the model and reproduce results. We would recommend that the authors of this paper upload the `datasets.py` file which includes these missing functions that would allow the model to run.\n",
        "\n",
        "The other paper we followed in order to achieve similar goals, BEHRT, was more reproducible, as it contained all the code necessary to run the model, including the custom dataset NextVisit we modeled ours off of."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Ioh8U32AkG8",
      "metadata": {
        "id": "6Ioh8U32AkG8"
      },
      "source": [
        "# **References**\n",
        "\n",
        "* BEHRT: Transformer for Electronic Health Records\n",
        "  * Paper: https://www.nature.com/articles/s41598-020-62922-y\n",
        "  * GitHub Repo: https://github.com/deepmedicine/BEHRT/tree/master\n",
        "* LLM Embeddings for ICD-10 Data\n",
        "  * https://github.com/whaleloops/TransformEHR/tree/main\n",
        "* TransformEHR\n",
        "  * Paper: https://doi.org/10.1038/s41467-023-43715-z\n",
        "  * GitHub Repo: https://github.com/whaleloops/TransformEHR/tree/main\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lzwOb0ohK7tb",
      "metadata": {
        "id": "lzwOb0ohK7tb"
      },
      "source": [
        "# TODO SINJA\n",
        "\n",
        "\n",
        "*   upload ablation models (3)\n",
        "*   accuracy measure ???????\n",
        "*   \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}