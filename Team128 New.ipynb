{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 89,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "b878e2c7-b91a-4349-8fde-50dd70ff6e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[1;32m<ipython-input-2-a5e49b5c6f79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
=======
      "\u001b[1;32m<ipython-input-89-a5e49b5c6f79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import json\n",
    "import pytorch_pretrained_bert as Bert"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 25,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "38ff7943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'], dtype='object')\n",
      "Patient ID: 10002769\n",
      "num of visits for patient:  2\n",
      "\t0-th visit id: 28314592\n",
      "\t0-th visit diagnosis codes: ['45342', '70713', '45981', '4019', '2724', 'V1251']\n",
      "\t0-th visit diagnosis short titles: ['Ac DVT/emb distl low ext', 'Ulcer of ankle', 'Venous insufficiency NOS', 'Hypertension NOS', 'Hyperlipidemia NEC/NOS', 'Hx-ven thrombosis/embols']\n",
      "\t1-th visit id: 25681387\n",
      "\t1-th visit diagnosis codes: ['45981', '70713', '4019', '2720', 'V1251', '4928', 'V1582', 'V113']\n",
      "\t1-th visit diagnosis short titles: ['Venous insufficiency NOS', 'Ulcer of ankle', 'Hypertension NOS', 'Pure hypercholesterolem', 'Hx-ven thrombosis/embols', 'Emphysema NEC', 'History of tobacco use', 'Hx of alcoholism']\n"
     ]
    }
   ],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 27,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "9e3d9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with icd_version = 10: 1989449\n",
      "Number of rows with icd_version = 9: 2766877\n",
      "Number of unique ICD-9 codes: 9072\n",
      "Number of unique ICD-10 codes: 16757\n",
      "Number of patients with at least one ICD-9 code: 124550\n",
      "Number of patients with both ICD-9 and ICD-10 codes: 24123\n",
      "Number of patients with only ICD-9 codes: 100427\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 9 code\n",
    "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
    "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
    "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 9 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
    "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2eb469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with only ICD-9 codes and more than 3 visits: 11073\n"
     ]
    }
   ],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 166,
   "id": "b0d341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_visits_and_codes(patient_visits):\n",
    "    visit_counts = {}\n",
    "    codes_per_visit = {}\n",
    "\n",
    "    for subject_id, visits in patient_visits.items():\n",
    "        # Count the number of visits for each patient\n",
    "        visit_counts[subject_id] = len(visits)\n",
    "        \n",
    "        # List to store the number of codes per visit for this patient\n",
    "        codes_per_visit[subject_id] = []\n",
    "\n",
    "        for visit in visits:\n",
    "            # Count the number of codes in each visit\n",
    "            codes_per_visit[subject_id].append(len(visit))\n",
    "\n",
    "    return visit_counts, codes_per_visit\n",
    "\n",
    "visit_counts, codes_per_visit = count_visits_and_codes(patient_visits)\n",
    "\n",
    "# Printing the results\n",
    "#print(\"Visit Counts per Patient:\" , sorted(visit_counts.values()))\n",
    "#set total max visits = 40\n",
    "#leave max codes = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "1f6f1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:  ['CLS', '5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582', 'SEP', '07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051', 'SEP']\n",
      "length Labels:  11073\n"
     ]
    }
   ],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "#extract all ICD9 codes but only take first 3 digits\n",
    "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
    "\n",
    "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
    "\n",
<<<<<<< HEAD
    "def encode_labels(label_codes, labelVocab):\n",
    "    multi_hot = [0] * len(token2idx)\n",
    "    #print(len)\n",
    "    \n",
    "    for code in label_codes:\n",
    "        if code in labelVocab:\n",
    "            index = labelVocab[code]\n",
    "            multi_hot[index] = 1\n",
    "        else:\n",
    "            print(f\"Warning: Code {code} not found in labelVocab.\")\n",
    "    \n",
    "    return multi_hot\n",
=======
    "def encode_labels(codes, code_to_index):\n",
    "    label_vector = [0] * len(code_to_index)\n",
    "    for code in codes:\n",
    "        truncated_code = code[:3]\n",
    "        if truncated_code in code_to_index:\n",
    "            label_vector[code_to_index[truncated_code]] = 1\n",
    "    return label_vector\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "labels = {}\n",
    "\n",
    "for subject_id, visits in patient_visits.items():\n",
<<<<<<< HEAD
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "\n",
    "        # Initialize the feature list with 'CLS'\n",
    "        feature_visits = ['CLS']\n",
    "\n",
    "        # Append codes and 'SEP' after each visit up to the split index\n",
=======
    "    if len(visits) > 3:  # Ensuring there are enough visits to split\n",
    "        split_index = len(visits) // 2\n",
    "\n",
    "        # Initializing 'CLS' token at the beginning of each features list\n",
    "        feature_visits = ['CLS']\n",
    "\n",
    "        # Append codes from each visit up to the split index and add 'SEP' after each visit\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "        for sublist in visits[:split_index]:\n",
    "            visit_codes = [code for code in sublist]\n",
    "            visit_codes.append('SEP')\n",
    "            feature_visits.extend(visit_codes)\n",
    "\n",
<<<<<<< HEAD
    "        # Append the feature list to the features\n",
    "        features.append(feature_visits)\n",
    "\n",
    "        # Gather label codes from visits after the split index\n",
    "        label_codes = [code[:3] for sublist in visits[split_index:] for code in sublist]\n",
    "\n",
    "        # Encode the labels and store using subject_id as key\n",
    "        labels[subject_id] = encode_labels(label_codes, labelVocab)\n",
=======
    "        # Store the concatenated feature list in the features list\n",
    "        features.append(feature_visits)\n",
    "\n",
    "        # Concatenate label codes from visits after the split index\n",
    "        label_codes = [code for sublist in visits[split_index:] for code in sublist]\n",
    "\n",
    "        # Use subject_id as a key and store the encoded labels\n",
    "        labels[subject_id] = encode_labels(label_codes, code_to_index)\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"length Labels: \" , len(labels))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
=======
   "execution_count": 216,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "bd6137dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000032\n",
      "[['5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582'], ['07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051'], ['07054', '78959', 'V462', '5715', '2767', '2761', '496', 'V08', '3051', '78791'], ['45829', '07044', '7994', '2761', '78959', '2767', '3051', 'V08', 'V4986', 'V462', '496', '29680', '5715']]\n",
<<<<<<< HEAD
      "1047\n"
=======
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
     ]
    }
   ],
   "source": [
    "print(patient_ids[0])\n",
    "print(patient_visits[10000032])\n",
<<<<<<< HEAD
    "print(len(labels[10000032]))"
=======
    "print(labels[10000032])"
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
=======
   "execution_count": 167,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "4c7ee9ab-3825-43e3-b698-d21591a26c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique truncated codes: 1042\n"
     ]
    }
   ],
   "source": [
    "#Creates Token Vocabulary\n",
    "\n",
    "import json\n",
    "# TODO i need full vocabulary     \n",
    "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
    "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
    "\n",
    "# Define special tokens with a specific order\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
    "\n",
    "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
    "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
    "\n",
    "# Print the number of unique codes to verify\n",
    "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
    "\n",
    "# Print token to index mapping\n",
    "#print(\"Token to Index Mapping:\", token2idx)\n",
    "\n",
    "# Save the token2idx dictionary to a JSON file for later use\n",
    "with open('token2idx.json', 'w') as f:\n",
    "    json.dump(token2idx, f)\n",
    "\n",
    "\n",
    "    # for labels just make the full dict with all 1042 codes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 196,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "bdb4fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[UNK]': 3, '001': 5, '002': 6, '003': 7, '004': 8, '005': 9, '006': 10, '007': 11, '008': 12, '009': 13, '010': 14, '011': 15, '012': 16, '013': 17, '014': 18, '015': 19, '016': 20, '017': 21, '018': 22, '020': 23, '021': 24, '022': 25, '023': 26, '024': 27, '025': 28, '026': 29, '027': 30, '030': 31, '031': 32, '032': 33, '033': 34, '034': 35, '035': 36, '036': 37, '037': 38, '038': 39, '039': 40, '040': 41, '041': 42, '042': 43, '045': 44, '046': 45, '047': 46, '048': 47, '049': 48, '050': 49, '051': 50, '052': 51, '053': 52, '054': 53, '055': 54, '056': 55, '057': 56, '058': 57, '059': 58, '060': 59, '061': 60, '062': 61, '063': 62, '064': 63, '065': 64, '066': 65, '070': 66, '071': 67, '072': 68, '073': 69, '074': 70, '075': 71, '076': 72, '077': 73, '078': 74, '079': 75, '080': 76, '081': 77, '082': 78, '083': 79, '084': 80, '085': 81, '086': 82, '087': 83, '088': 84, '090': 85, '091': 86, '092': 87, '093': 88, '094': 89, '095': 90, '096': 91, '097': 92, '098': 93, '099': 94, '100': 95, '101': 96, '102': 97, '103': 98, '104': 99, '110': 100, '111': 101, '112': 102, '114': 103, '115': 104, '116': 105, '117': 106, '118': 107, '120': 108, '121': 109, '122': 110, '123': 111, '124': 112, '125': 113, '126': 114, '127': 115, '128': 116, '129': 117, '130': 118, '131': 119, '132': 120, '133': 121, '134': 122, '135': 123, '136': 124, '137': 125, '138': 126, '139': 127, '140': 128, '141': 129, '142': 130, '143': 131, '144': 132, '145': 133, '146': 134, '147': 135, '148': 136, '149': 137, '150': 138, '151': 139, '152': 140, '153': 141, '154': 142, '155': 143, '156': 144, '157': 145, '158': 146, '159': 147, '160': 148, '161': 149, '162': 150, '163': 151, '164': 152, '165': 153, '170': 154, '171': 155, '172': 156, '173': 157, '174': 158, '175': 159, '176': 160, '179': 161, '180': 162, '181': 163, '182': 164, '183': 165, '184': 166, '185': 167, '186': 168, '187': 169, '188': 170, '189': 171, '190': 172, '191': 173, '192': 174, '193': 175, '194': 176, '195': 177, '196': 178, '197': 179, '198': 180, '199': 181, '200': 182, '201': 183, '202': 184, '203': 185, '204': 186, '205': 187, '206': 188, '207': 189, '208': 190, '209': 191, '210': 192, '211': 193, '212': 194, '213': 195, '214': 196, '215': 197, '216': 198, '217': 199, '218': 200, '219': 201, '220': 202, '221': 203, '222': 204, '223': 205, '224': 206, '225': 207, '226': 208, '227': 209, '228': 210, '229': 211, '230': 212, '231': 213, '232': 214, '233': 215, '234': 216, '235': 217, '236': 218, '237': 219, '238': 220, '239': 221, '240': 222, '241': 223, '242': 224, '243': 225, '244': 226, '245': 227, '246': 228, '249': 229, '250': 230, '251': 231, '252': 232, '253': 233, '254': 234, '255': 235, '256': 236, '257': 237, '258': 238, '259': 239, '260': 240, '261': 241, '262': 242, '263': 243, '264': 244, '265': 245, '266': 246, '267': 247, '268': 248, '269': 249, '270': 250, '271': 251, '272': 252, '273': 253, '274': 254, '275': 255, '276': 256, '277': 257, '278': 258, '279': 259, '280': 260, '281': 261, '282': 262, '283': 263, '284': 264, '285': 265, '286': 266, '287': 267, '288': 268, '289': 269, '290': 270, '291': 271, '292': 272, '293': 273, '294': 274, '295': 275, '296': 276, '297': 277, '298': 278, '299': 279, '300': 280, '301': 281, '302': 282, '303': 283, '304': 284, '305': 285, '306': 286, '307': 287, '308': 288, '309': 289, '310': 290, '311': 291, '312': 292, '313': 293, '314': 294, '315': 295, '316': 296, '317': 297, '318': 298, '319': 299, '320': 300, '321': 301, '322': 302, '323': 303, '324': 304, '325': 305, '326': 306, '327': 307, '330': 308, '331': 309, '332': 310, '333': 311, '334': 312, '335': 313, '336': 314, '337': 315, '338': 316, '339': 317, '340': 318, '341': 319, '342': 320, '343': 321, '344': 322, '345': 323, '346': 324, '347': 325, '348': 326, '349': 327, '350': 328, '351': 329, '352': 330, '353': 331, '354': 332, '355': 333, '356': 334, '357': 335, '358': 336, '359': 337, '360': 338, '361': 339, '362': 340, '363': 341, '364': 342, '365': 343, '366': 344, '367': 345, '368': 346, '369': 347, '370': 348, '371': 349, '372': 350, '373': 351, '374': 352, '375': 353, '376': 354, '377': 355, '378': 356, '379': 357, '380': 358, '381': 359, '382': 360, '383': 361, '384': 362, '385': 363, '386': 364, '387': 365, '388': 366, '389': 367, '390': 368, '391': 369, '392': 370, '393': 371, '394': 372, '395': 373, '396': 374, '397': 375, '398': 376, '401': 377, '402': 378, '403': 379, '404': 380, '405': 381, '410': 382, '411': 383, '412': 384, '413': 385, '414': 386, '415': 387, '416': 388, '417': 389, '420': 390, '421': 391, '422': 392, '423': 393, '424': 394, '425': 395, '426': 396, '427': 397, '428': 398, '429': 399, '430': 400, '431': 401, '432': 402, '433': 403, '434': 404, '435': 405, '436': 406, '437': 407, '438': 408, '440': 409, '441': 410, '442': 411, '443': 412, '444': 413, '445': 414, '446': 415, '447': 416, '448': 417, '449': 418, '451': 419, '452': 420, '453': 421, '454': 422, '455': 423, '456': 424, '457': 425, '458': 426, '459': 427, '460': 428, '461': 429, '462': 430, '463': 431, '464': 432, '465': 433, '466': 434, '470': 435, '471': 436, '472': 437, '473': 438, '474': 439, '475': 440, '476': 441, '477': 442, '478': 443, '480': 444, '481': 445, '482': 446, '483': 447, '484': 448, '485': 449, '486': 450, '487': 451, '488': 452, '490': 453, '491': 454, '492': 455, '493': 456, '494': 457, '495': 458, '496': 459, '500': 460, '501': 461, '502': 462, '503': 463, '504': 464, '505': 465, '506': 466, '507': 467, '508': 468, '510': 469, '511': 470, '512': 471, '513': 472, '514': 473, '515': 474, '516': 475, '517': 476, '518': 477, '519': 478, '520': 479, '521': 480, '522': 481, '523': 482, '524': 483, '525': 484, '526': 485, '527': 486, '528': 487, '529': 488, '530': 489, '531': 490, '532': 491, '533': 492, '534': 493, '535': 494, '536': 495, '537': 496, '538': 497, '539': 498, '540': 499, '541': 500, '542': 501, '543': 502, '550': 503, '551': 504, '552': 505, '553': 506, '555': 507, '556': 508, '557': 509, '558': 510, '560': 511, '562': 512, '564': 513, '565': 514, '566': 515, '567': 516, '568': 517, '569': 518, '570': 519, '571': 520, '572': 521, '573': 522, '574': 523, '575': 524, '576': 525, '577': 526, '578': 527, '579': 528, '580': 529, '581': 530, '582': 531, '583': 532, '584': 533, '585': 534, '586': 535, '587': 536, '588': 537, '589': 538, '590': 539, '591': 540, '592': 541, '593': 542, '594': 543, '595': 544, '596': 545, '597': 546, '598': 547, '599': 548, '600': 549, '601': 550, '602': 551, '603': 552, '604': 553, '605': 554, '606': 555, '607': 556, '608': 557, '610': 558, '611': 559, '612': 560, '614': 561, '615': 562, '616': 563, '617': 564, '618': 565, '619': 566, '620': 567, '621': 568, '622': 569, '623': 570, '624': 571, '625': 572, '626': 573, '627': 574, '628': 575, '629': 576, '630': 577, '631': 578, '632': 579, '633': 580, '634': 581, '635': 582, '636': 583, '637': 584, '638': 585, '639': 586, '640': 587, '641': 588, '642': 589, '643': 590, '644': 591, '645': 592, '646': 593, '647': 594, '648': 595, '649': 596, '650': 597, '651': 598, '652': 599, '653': 600, '654': 601, '655': 602, '656': 603, '657': 604, '658': 605, '659': 606, '660': 607, '661': 608, '662': 609, '663': 610, '664': 611, '665': 612, '666': 613, '667': 614, '668': 615, '669': 616, '670': 617, '671': 618, '672': 619, '673': 620, '674': 621, '675': 622, '676': 623, '677': 624, '678': 625, '679': 626, '680': 627, '681': 628, '682': 629, '683': 630, '684': 631, '685': 632, '686': 633, '690': 634, '691': 635, '692': 636, '693': 637, '694': 638, '695': 639, '696': 640, '697': 641, '698': 642, '700': 643, '701': 644, '702': 645, '703': 646, '704': 647, '705': 648, '706': 649, '707': 650, '708': 651, '709': 652, '710': 653, '711': 654, '712': 655, '713': 656, '714': 657, '715': 658, '716': 659, '717': 660, '718': 661, '719': 662, '720': 663, '721': 664, '722': 665, '723': 666, '724': 667, '725': 668, '726': 669, '727': 670, '728': 671, '729': 672, '730': 673, '731': 674, '732': 675, '733': 676, '734': 677, '735': 678, '736': 679, '737': 680, '738': 681, '739': 682, '740': 683, '741': 684, '742': 685, '743': 686, '744': 687, '745': 688, '746': 689, '747': 690, '748': 691, '749': 692, '750': 693, '751': 694, '752': 695, '753': 696, '754': 697, '755': 698, '756': 699, '757': 700, '758': 701, '759': 702, '760': 703, '761': 704, '762': 705, '763': 706, '764': 707, '765': 708, '766': 709, '767': 710, '768': 711, '769': 712, '770': 713, '771': 714, '772': 715, '773': 716, '774': 717, '775': 718, '776': 719, '777': 720, '778': 721, '779': 722, '780': 723, '781': 724, '782': 725, '783': 726, '784': 727, '785': 728, '786': 729, '787': 730, '788': 731, '789': 732, '790': 733, '791': 734, '792': 735, '793': 736, '794': 737, '795': 738, '796': 739, '797': 740, '798': 741, '799': 742, '800': 743, '801': 744, '802': 745, '803': 746, '804': 747, '805': 748, '806': 749, '807': 750, '808': 751, '809': 752, '810': 753, '811': 754, '812': 755, '813': 756, '814': 757, '815': 758, '816': 759, '817': 760, '818': 761, '819': 762, '820': 763, '821': 764, '822': 765, '823': 766, '824': 767, '825': 768, '826': 769, '827': 770, '828': 771, '829': 772, '830': 773, '831': 774, '832': 775, '833': 776, '834': 777, '835': 778, '836': 779, '837': 780, '838': 781, '839': 782, '840': 783, '841': 784, '842': 785, '843': 786, '844': 787, '845': 788, '846': 789, '847': 790, '848': 791, '850': 792, '851': 793, '852': 794, '853': 795, '854': 796, '860': 797, '861': 798, '862': 799, '863': 800, '864': 801, '865': 802, '866': 803, '867': 804, '868': 805, '869': 806, '870': 807, '871': 808, '872': 809, '873': 810, '874': 811, '875': 812, '876': 813, '877': 814, '878': 815, '879': 816, '880': 817, '881': 818, '882': 819, '883': 820, '884': 821, '885': 822, '886': 823, '887': 824, '890': 825, '891': 826, '892': 827, '893': 828, '894': 829, '895': 830, '896': 831, '897': 832, '900': 833, '901': 834, '902': 835, '903': 836, '904': 837, '905': 838, '906': 839, '907': 840, '908': 841, '909': 842, '910': 843, '911': 844, '912': 845, '913': 846, '914': 847, '915': 848, '916': 849, '917': 850, '918': 851, '919': 852, '920': 853, '921': 854, '922': 855, '923': 856, '924': 857, '925': 858, '926': 859, '927': 860, '928': 861, '929': 862, '930': 863, '931': 864, '932': 865, '933': 866, '934': 867, '935': 868, '936': 869, '937': 870, '938': 871, '939': 872, '940': 873, '941': 874, '942': 875, '943': 876, '944': 877, '945': 878, '946': 879, '947': 880, '948': 881, '949': 882, '950': 883, '951': 884, '952': 885, '953': 886, '954': 887, '955': 888, '956': 889, '957': 890, '958': 891, '959': 892, '960': 893, '961': 894, '962': 895, '963': 896, '964': 897, '965': 898, '966': 899, '967': 900, '968': 901, '969': 902, '970': 903, '971': 904, '972': 905, '973': 906, '974': 907, '975': 908, '976': 909, '977': 910, '978': 911, '979': 912, '980': 913, '981': 914, '982': 915, '983': 916, '984': 917, '985': 918, '986': 919, '987': 920, '988': 921, '989': 922, '990': 923, '991': 924, '992': 925, '993': 926, '994': 927, '995': 928, '996': 929, '997': 930, '998': 931, '999': 932, 'E00': 933, 'E01': 934, 'E02': 935, 'E03': 936, 'E80': 937, 'E81': 938, 'E82': 939, 'E83': 940, 'E84': 941, 'E85': 942, 'E86': 943, 'E87': 944, 'E88': 945, 'E89': 946, 'E90': 947, 'E91': 948, 'E92': 949, 'E93': 950, 'E94': 951, 'E95': 952, 'E96': 953, 'E97': 954, 'E98': 955, 'E99': 956, 'V01': 957, 'V02': 958, 'V03': 959, 'V04': 960, 'V05': 961, 'V06': 962, 'V07': 963, 'V08': 964, 'V09': 965, 'V10': 966, 'V11': 967, 'V12': 968, 'V13': 969, 'V14': 970, 'V15': 971, 'V16': 972, 'V17': 973, 'V18': 974, 'V19': 975, 'V20': 976, 'V21': 977, 'V22': 978, 'V23': 979, 'V24': 980, 'V25': 981, 'V26': 982, 'V27': 983, 'V28': 984, 'V29': 985, 'V30': 986, 'V31': 987, 'V32': 988, 'V33': 989, 'V34': 990, 'V35': 991, 'V36': 992, 'V37': 993, 'V39': 994, 'V40': 995, 'V41': 996, 'V42': 997, 'V43': 998, 'V44': 999, 'V45': 1000, 'V46': 1001, 'V47': 1002, 'V48': 1003, 'V49': 1004, 'V50': 1005, 'V51': 1006, 'V52': 1007, 'V53': 1008, 'V54': 1009, 'V55': 1010, 'V56': 1011, 'V57': 1012, 'V58': 1013, 'V59': 1014, 'V60': 1015, 'V61': 1016, 'V62': 1017, 'V63': 1018, 'V64': 1019, 'V65': 1020, 'V66': 1021, 'V67': 1022, 'V68': 1023, 'V69': 1024, 'V70': 1025, 'V71': 1026, 'V72': 1027, 'V73': 1028, 'V74': 1029, 'V75': 1030, 'V76': 1031, 'V77': 1032, 'V78': 1033, 'V79': 1034, 'V80': 1035, 'V81': 1036, 'V82': 1037, 'V83': 1038, 'V84': 1039, 'V85': 1040, 'V86': 1041, 'V87': 1042, 'V88': 1043, 'V89': 1044, 'V90': 1045, 'V91': 1046}\n"
     ]
    }
   ],
   "source": [
    "#create the label vocab\n",
    "#same as token vocab but no SEP, CLS, PAD, MASk (Leave UNK)\n",
    "# DID NOT reorganize indices, may need to\n",
    "\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "labelVocab = {token: idx for token, idx in token2idx.items() if token not in special_tokens}\n",
    "\n",
    "# Print the new dictionary to verify\n",
    "print(labelVocab)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
   "id": "489fd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset helpers\n",
    "\n",
    "def index_seg(tokens, symbol=2):\n",
    "    flag = 0\n",
    "    seg = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            seg.append(flag)\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "        else:\n",
    "            seg.append(flag)\n",
    "    return seg\n",
    "\n",
    "\n",
    "def position_idx(tokens, symbol=2):\n",
    "    pos = []\n",
    "    flag = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            pos.append(flag)\n",
    "            flag += 1\n",
    "        else:\n",
    "            pos.append(flag)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
=======
   "execution_count": 217,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "2bacd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset\n",
    "\n",
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
    "        self.token2idx = token2idx\n",
    "        self.labels = labels\n",
    "        self.patient_visits = patient_visits\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_visits)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve patient data by index\n",
    "        patient_id = list(self.patient_visits.keys())[index]\n",
    "        codes = self.patient_visits[patient_id]\n",
    "        \n",
    "        # Initialize sequence with [CLS] token\n",
    "        sequence = [self.token2idx['[CLS]']]\n",
<<<<<<< HEAD
    "\n",
    "\n",
=======
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "        \n",
    "        # Add each code to the sequence and append [SEP] after each visit\n",
    "        for visit in codes:\n",
    "            #change 'code' to be truncated version!!!!!!\n",
<<<<<<< HEAD
    "            sequence.extend([self.token2idx.get(code[:3], self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
    "\n",
    "        position_indices = position_idx(sequence)\n",
    "        segment = index_seg(sequence)\n",
=======
    "            sequence.extend([self.token2idx.get(code, self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "        \n",
    "        # Cut or pad the sequence to the maximum length\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
    "\n",
<<<<<<< HEAD
=======
    "        # Create position indices (positional embeddings)\n",
    "        position_indices = list(range(len(sequence)))\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "\n",
    "        # Create a mask for the sequence\n",
    "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
<<<<<<< HEAD
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), torch.tensor(segment, dtype=torch.long), label"
=======
    "        print(patient_id)\n",
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), label"
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
   "execution_count": 218,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "151fbc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "44\n",
      "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2]\n",
      "512\n",
      "[1, 521, 732, 520, 66, 459, 276, 289, 971, 2, 66, 732, 267, 256, 459, 520, 964, 285, 2, 66, 732, 1001, 520, 256, 256, 459, 964, 285, 730, 2, 426, 66, 742, 256, 732, 256, 285, 964, 1004, 1001, 459, 276, 520, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "44\n",
      "position:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "44\n",
      "segment:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "512\n",
      "mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1042\n",
      "labels:  tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
=======
      "1 Train batch: \n",
      "[1, 3, 3, 3, 3, 459, 3, 3, 3, 2, 3, 3, 3, 3, 459, 3, 964, 3, 2, 3, 3, 3, 3, 3, 3, 459, 964, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 964, 3, 3, 459, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\\nNeed to do version matching before feeding into model.\\n'"
      ]
     },
<<<<<<< HEAD
     "execution_count": 54,
=======
     "execution_count": 218,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testint stuff out\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "patient_id = list(patient_visits.keys())[0]\n",
    "codes = patient_visits[patient_id]\n",
    "\n",
    "# Initialize sequence with [CLS] token\n",
    "sequence = [token2idx['[CLS]']]\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code[:3], token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "position_indices = position_idx(sequence)\n",
    "segment = index_seg(sequence)\n",
    "\n",
=======
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code, token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "# Cut or pad the sequence to the maximum length\n",
    "if len(sequence) > max_len:\n",
    "    sequence = sequence[:max_len]\n",
    "else:\n",
    "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "label = torch.tensor(labels[patient_id], dtype=torch.float)\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "print(len(position_indices))\n",
    "print(\"position: \", position_indices)\n",
    "\n",
    "print(len(segment))\n",
    "print(\"segment: \", segment)\n",
    "\n",
    "print(len(mask))\n",
    "print(\"mask: \", mask)\n",
    "\n",
    "print(len(labels[10000032]))\n",
    "print(\"labels: \" , label)\n",
    "\n",
=======
    "# Create position indices (positional embeddings)\n",
    "position_indices = list(range(len(sequence)))\n",
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "print(\"1 Train batch: \")\n",
    "print(sequence)\n",
    "\n",
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
    "'''\n",
    "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
    "Need to do version matching before feeding into model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 219,
>>>>>>> d85832b72222ffbd8daa86ee5c4a857a89fea87b
   "id": "5178434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bb0169a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertPreTrainedModel , BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cpu',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 512,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(token2idx.keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    #'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    #'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        #self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        #self.age_vocab_size = config.get('age_vocab_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7240e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.get_embeddings import ICD9Embeddings\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups \n",
    "        self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        \n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None,):\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "        \n",
    "        #print(\"this is word ids\", word_ids)\n",
    "\n",
    "        \n",
    "        # TODO how to combine the embeddings of all the words\n",
    "        word_embed = self.icd9_embeddings.get(word_ids)\n",
    "    \n",
    "\n",
    "        # TODO how to pass in positional embeddings?\n",
    "\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        #temp fix\n",
    "        print(\"Embeddings before attempting to create zeros_like:\", embeddings)\n",
    "        embeddings = torch.full((512,), 0.5)\n",
    "        posi_embeddings = torch.zeros_like(embeddings)\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "    \n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(input_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2f90a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ba4038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(global_params['device'])\n",
    "model = model.to(\"cpu\")\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "09a4dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, output, label\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, roc, output, label,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "eca32a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=[3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "                             18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                             31, 32, 33, ...])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "bf484a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        cnt +=1\n",
    "        input_ids, posi_ids, attMask, targets = batch\n",
    "        \n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "\n",
    "        #age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        #segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        loss, logits = model(input_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 500==0:\n",
    "            prec, a, b = precision(logits, targets)\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt,temp_loss/500, prec))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        input_ids, posi_ids, attMask, targets, _ = batch\n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "1ae22948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "17336231\n",
      "16522952\n",
      "14208604\n",
      "17959879\n",
      "14945405\n",
      "18974643\n",
      "16046605\n",
      "15906836\n",
      "10388752\n",
      "13623520\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-325-9b397f7ab511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"starting training...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0maps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maps\u001b[0m \u001b[1;33m>\u001b[0m\u001b[0mbest_pre\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-324-6c90d6139962>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattMask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gradient_accumulation_steps'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-6ff14550fdf2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, posi_ids, attention_mask, labels)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,\n\u001b[0m\u001b[0;32m    111\u001b[0m                                      output_all_encoded_layers=False)\n\u001b[0;32m    112\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-6ff14550fdf2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, posi_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m10000.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[0;32m     90\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-6ff14550fdf2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_ids, posi_ids)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# TODO how to pass in positional embeddings?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mposi_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposi_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposi_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mc:\\Users\\dylan\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2237\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "best_pre = 0.0\n",
    "for e in range(50):\n",
    "    print(\"starting training...\")\n",
    "    train(e)\n",
    "    aps, roc, test_loss = evaluation()\n",
    "    if aps >best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = aps\n",
    "    print('aps : {}'.format(aps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None,):\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "        word_embed = self.word_embeddings(word_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba47b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml3(output_2)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBERTClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mBERTClass.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(BERTClass, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m BertEmbeddings(config\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1 \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.embeddings = \n",
    "\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_0 = self.embeddings(ids, mask, token_type_ids)\n",
    "        _, output_1= self.l1(output_0, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "model = BERTClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
