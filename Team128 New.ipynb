{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b878e2c7-b91a-4349-8fde-50dd70ff6e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-a5e49b5c6f79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import json\n",
    "import pytorch_pretrained_bert as Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38ff7943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version'], dtype='object')\n",
      "Patient ID: 10002769\n",
      "num of visits for patient:  2\n",
      "\t0-th visit id: 28314592\n",
      "\t0-th visit diagnosis codes: ['45342', '70713', '45981', '4019', '2724', 'V1251']\n",
      "\t0-th visit diagnosis short titles: ['Ac DVT/emb distl low ext', 'Ulcer of ankle', 'Venous insufficiency NOS', 'Hypertension NOS', 'Hyperlipidemia NEC/NOS', 'Hx-ven thrombosis/embols']\n",
      "\t1-th visit id: 25681387\n",
      "\t1-th visit diagnosis codes: ['45981', '70713', '4019', '2720', 'V1251', '4928', 'V1582', 'V113']\n",
      "\t1-th visit diagnosis short titles: ['Venous insufficiency NOS', 'Ulcer of ankle', 'Hypertension NOS', 'Pure hypercholesterolem', 'Hx-ven thrombosis/embols', 'Emphysema NEC', 'History of tobacco use', 'Hx of alcoholism']\n"
     ]
    }
   ],
   "source": [
    "#Data stuff\n",
    "diagnoses_file_path = r'mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'TransformerEHR/data/physionet.org/files/mimiciii-demo/1.4/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e3d9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with icd_version = 10: 1989449\n",
      "Number of rows with icd_version = 9: 2766877\n",
      "Number of unique ICD-9 codes: 9072\n",
      "Number of unique ICD-10 codes: 16757\n",
      "Number of patients with at least one ICD-9 code: 124550\n",
      "Number of patients with both ICD-9 and ICD-10 codes: 24123\n",
      "Number of patients with only ICD-9 codes: 100427\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 9 code\n",
    "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
    "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
    "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 9 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
    "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2eb469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with only ICD-9 codes and more than 3 visits: 11073\n"
     ]
    }
   ],
   "source": [
    "# Total num of usable patients\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b0d341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_visits_and_codes(patient_visits):\n",
    "    visit_counts = {}\n",
    "    codes_per_visit = {}\n",
    "\n",
    "    for subject_id, visits in patient_visits.items():\n",
    "        # Count the number of visits for each patient\n",
    "        visit_counts[subject_id] = len(visits)\n",
    "        \n",
    "        # List to store the number of codes per visit for this patient\n",
    "        codes_per_visit[subject_id] = []\n",
    "\n",
    "        for visit in visits:\n",
    "            # Count the number of codes in each visit\n",
    "            codes_per_visit[subject_id].append(len(visit))\n",
    "\n",
    "    return visit_counts, codes_per_visit\n",
    "\n",
    "visit_counts, codes_per_visit = count_visits_and_codes(patient_visits)\n",
    "\n",
    "# Printing the results\n",
    "#print(\"Visit Counts per Patient:\" , sorted(visit_counts.values()))\n",
    "#set total max visits = 40\n",
    "#leave max codes = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1f6f1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:  ['CLS', '5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582', 'SEP', '07071', '78959', '2875', '2761', '496', '5715', 'V08', '3051', 'SEP']\n",
      "length Labels:  1042\n"
     ]
    }
   ],
   "source": [
    "#preparing the label and feature splits \n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])['icd_code'].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')['icd_code'].apply(list)\n",
    "\n",
    "#extract all ICD9 codes but only take first 3 digits\n",
    "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
    "\n",
    "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
    "\n",
    "def encode_labels(codes, code_to_index):\n",
    "    label_vector = [0] * len(code_to_index)\n",
    "    for code in codes:\n",
    "        truncated_code = code[:3]\n",
    "        if truncated_code in code_to_index:\n",
    "            label_vector[code_to_index[truncated_code]] = 1\n",
    "    return label_vector\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for subject_id, visits in patient_visits.items():\n",
    "    if len(visits) > 3:\n",
    "        split_index = len(visits) // 2\n",
    "\n",
    "        \n",
    "        feature_visits = []\n",
    "        feature_visits.append('CLS')\n",
    "\n",
    "        \n",
    "        for sublist in visits[:split_index]:\n",
    "            visit_codes = [code for code in sublist] \n",
    "            visit_codes.append('SEP')\n",
    "            feature_visits.extend(visit_codes) \n",
    "\n",
    "        features.append(feature_visits)\n",
    "\n",
    "       \n",
    "        label_codes = [code for sublist in visits[split_index:] for code in sublist]\n",
    "        labels.append(encode_labels(label_codes, code_to_index))\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"length Labels: \" , len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4c7ee9ab-3825-43e3-b698-d21591a26c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique truncated codes: 1042\n"
     ]
    }
   ],
   "source": [
    "#Creates Token Vocabulary\n",
    "\n",
    "import json\n",
    "# TODO i need full vocabulary     \n",
    "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
    "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
    "\n",
    "# Define special tokens with a specific order\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
    "\n",
    "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
    "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
    "\n",
    "# Print the number of unique codes to verify\n",
    "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
    "\n",
    "# Print token to index mapping\n",
    "#print(\"Token to Index Mapping:\", token2idx)\n",
    "\n",
    "# Save the token2idx dictionary to a JSON file for later use\n",
    "with open('token2idx.json', 'w') as f:\n",
    "    json.dump(token2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2bacd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
    "        self.token2idx = token2idx\n",
    "        self.labels = labels\n",
    "        self.patient_visits = patient_visits\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_visits)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve patient data by index\n",
    "        patient_id = list(self.patient_visits.keys())[index]\n",
    "        codes = self.patient_visits[patient_id]\n",
    "        \n",
    "        # Initialize sequence with [CLS] token\n",
    "        sequence = [self.token2idx['[CLS]']]\n",
    "        \n",
    "        # Add each code to the sequence and append [SEP] after each visit\n",
    "        for visit in codes:\n",
    "            sequence.extend([self.token2idx.get(code, self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
    "        \n",
    "        # Cut or pad the sequence to the maximum length\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
    "\n",
    "        # Create position indices (positional embeddings)\n",
    "        position_indices = list(range(len(sequence)))\n",
    "\n",
    "        # Create a mask for the sequence\n",
    "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "151fbc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 3, 3, 459, 3, 3, 3, 2, 3, 3, 3, 3, 459, 3, 964, 3, 2, 3, 3, 3, 3, 3, 3, 459, 964, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 964, 3, 3, 459, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#testint stuff out\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "patient_id = list(patient_visits.keys())[0]\n",
    "codes = patient_visits[patient_id]\n",
    "\n",
    "# Initialize sequence with [CLS] token\n",
    "sequence = [token2idx['[CLS]']]\n",
    "\n",
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code, token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
    "# Cut or pad the sequence to the maximum length\n",
    "if len(sequence) > max_len:\n",
    "    sequence = sequence[:max_len]\n",
    "else:\n",
    "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
    "\n",
    "# Create position indices (positional embeddings)\n",
    "position_indices = list(range(len(sequence)))\n",
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "print(\"1 Train batch: \")\n",
    "print(sequence)\n",
    "\n",
    "'''\n",
    "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
    "Need to do version matching before feeding into model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5178434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0169a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None,):\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "        word_embed = self.word_embeddings(word_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba47b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml3(output_2)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBERTClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mBERTClass.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(BERTClass, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m BertEmbeddings(config\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1 \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.embeddings = \n",
    "\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_0 = self.embeddings(ids, mask, token_type_ids)\n",
    "        _, output_1= self.l1(output_0, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "model = BERTClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
