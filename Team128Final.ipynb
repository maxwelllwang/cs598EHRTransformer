{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "_OvPWYhA1-jo"
   },
   "source": [
    "# **Project Links**\n",
    "\n",
    "### [Project GitHub Repository](https://github.com/maxwelllwang/cs598EHRTransformer/tree/main)\n",
    "\n",
    "### [Project Video](https://drive.google.com/file/d/1v-igXoba1TicqjT4ozxfhXb0kaP0A9W2/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NjffeLNwPdH",
    "outputId": "847b2fb4-9226-4a92-e743-778f3519296e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4m0lg-gwnHJ",
    "outputId": "5c728f17-f008-400d-9631-efa9fce47119"
   },
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/CS 598 Project/cs598ehrupload\n",
    "# %cd /content/gdrive/MyDrive/cs598ehrupload\n",
    "! ls\n",
    "\n",
    "# change these to the path of your data\n",
    "diagnoses_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/mimic_data/diagnoses_icd.csv.gz'\n",
    "map_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/mimic_data/D_ICD_DIAGNOSES.csv'\n",
    "icd9_embedding_txt = r\"/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/embeddings/ic9_embeddings.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "9ohMXONP1a92"
   },
   "source": [
    "#**Introduction**\n",
    "\n",
    "## *Background:*\n",
    "Current pretraining objectives in predictive EHR-based models are limited to predicting a fraction of ICD codes within a patient’s visit, when in reality, patients usually have multiple, often highly-correlated diseases. In addition, current models are unable to accurately predict the timeline of correlated diagnoses and could lead to missed opportunities in preventative care. Predictive tasks surrounding healthcare data can be challenging due to the complexity of healthcare data, which includes high-dimensional and often incomplete patient data over time. The state of the art methods used to solve similar healthcare related problems have been transformer based deep learning models trained on extensive datasets and fine-tuned for specific tasks. Despite their success, current models are often fine-tuned to focus on predicting a limited set of outcomes, thus overlooking the interconnected nature of various health conditions.\n",
    "\n",
    "## *Original Paper: TransformEHR*\n",
    "The paper presents \"TransformEHR,\" a novel generative encoder-decoder model leveraging transformer architecture, specifically designed for predicting future patient outcomes based on their longitudinal EHRs. The authors utilized techniques like visit-masking and time embedding to achieve results that outperform the other state of the art models. For example, when testing their encoder-decoder model against an encoder only model, the authors were able to achieve an, “improvement of 95%CI: 0.74%–1.16%, p < 0.001 in AUROC across all diseases/outcomes tested.” While the paper boasted strong results on a variety of both common and uncommon diseases, the authors mentioned that their work was related to predictive model studies focused on intentional self-harm. TransformEHR performed exceptionally well within this subspace and esteemed to reduce incremental cost-effective ratio by $109k per quality-adjusted life-years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "uz8n1Nmz3Hjf"
   },
   "source": [
    "#**Scope of Reproducibility**\n",
    "\n",
    "While trying to recreate the TransformEHR model as per the original paper, we discovered a lot of missing code in the project repo, specifically in the helper class `dataset.py`. This file was meant to contain the DataCollator functions and Tokenizers to be used in the model. Since the model required the input formatted a specific way, missing these functions and not knowing how they were implemented or what the expected input format was made it nearly impossible to get the model running.\n",
    "\n",
    "As a result, we decided to base our model off of BEHRT, another type of transformer model used to predict EHR codes. We also integrated this with prelearned ICD code embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "LT-36hWLHq-F"
   },
   "source": [
    "## *BEHRT Overview*\n",
    "\n",
    "BEHRT is a transformer model inspired by BERT, which aims to accomplish a similar goal to TransformEHR of predicting patient's future diseases/outcomes based on EHR data of their past visits. The model takes each diagnosis as a \"word,\" each visit as a \"sentence,\" and the entire medical history as a \"document,\" and uses multi-head self-attention and masked language modeling. BEHRT integrates disease embeddings, positional encodings, age, and visit segment information, and uses deep bidirectional representation to make predictions of a patient's medical journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "bHVnMX-y3fIx"
   },
   "source": [
    "#**Methodology**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "vHsHp_Ka3ygR"
   },
   "source": [
    "##*Environment*\n",
    "\n",
    "Python version: 3.10\n",
    "\n",
    "###Dependencies/Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0",
    "outputId": "20aa8ee2-0398-47e8-a1e1-b931f12e8ba3"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import transformers\n",
    "import json\n",
    "import pytorch_pretrained_bert as Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "MjH2HMCd3_xP"
   },
   "source": [
    "##*Data*\n",
    "\n",
    "The data we use for this model comes from the **[MIMIC-III dataset](https://physionet.org/content/mimiciii/1.4/)**, which can be accessed via PhysioNet to anyone with completed training/permissions. Specifically, we use the files *diagnoses_icd.csv.g* and *D_ICD_DIAGNOSES.csv*.\n",
    "\n",
    "\n",
    "Please note that the filepaths for variables `diagnoses_file_path` (which will read from *diagnoses_icd.csv.gz*) and `map_file_path` (which will read from *D_ICD_DIAGNOSES.csv*) may need to be updated depending on where they are located locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "y8H364z45WNC"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1",
    "outputId": "88dcc690-a91d-4b74-8a1a-78105773cab6"
   },
   "outputs": [],
   "source": [
    "#Data stuff\n",
    "# diagnoses_file_path = r'/content/gdrive/MyDrive/cs598ehrupload/mimic_data/diagnoses_icd.csv.gz'\n",
    "# map_file_path = r'/content/gdrive/MyDrive/cs598ehrupload/mimic_data/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "# diagnoses_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/sample_data/DIAGNOSES_ICD.csv'\n",
    "# map_file_path = r'/content/gdrive/MyDrive/CS 598 Project/cs598ehrupload/sample_data/D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "diagnoses_df = pd.read_csv(diagnoses_file_path)\n",
    "print(diagnoses_df.columns)\n",
    "map_df = pd.read_csv(map_file_path)\n",
    "\n",
    "icd_code_col_name = 'icd_code'   # NOTE: column name 'icd9_code' in DIAGNOSES_ICD.csv from MIMIC demo dataset is called 'icd_code' in full dataset\n",
    "\n",
    "\n",
    "#list of patient id's that have been diagnosed with something\n",
    "#make everything sequential and not patient_id key based\n",
    "patient_ids = diagnoses_df['subject_id'].unique().tolist()\n",
    "\n",
    "#2d array where each nested list is the hadm_id for each visit\n",
    "visits = diagnoses_df.groupby('subject_id')['hadm_id'].apply(lambda x: list(set(x))).tolist()\n",
    "\n",
    "#3d array contains a list of visits with respective ICD9 code per visit\n",
    "# NOTE: column name 'icd9_code' in DIAGNOSES_ICD.csv from MIMIC demo dataset is called 'icd_code' in full dataset\n",
    "patient_visits = (\n",
    "    diagnoses_df.groupby(['subject_id', 'hadm_id'])[icd_code_col_name].apply(list).groupby(level=0).apply(list).tolist()\n",
    ")\n",
    "\n",
    "#dict of {icd9_code : short_title}\n",
    "#not all icd9_codes which are present in DIAGNOSES_ICD.csv are present in D_ICD_DIAGNOSES.csv, so not all codes will have a title\n",
    "icd9_to_title = pd.Series(map_df['short_title'].values, index=map_df['icd9_code']).to_dict()\n",
    "\n",
    "print(\"Patient ID:\", patient_ids[53])\n",
    "print(\"num of visits for patient: \" , len(visits[53]))\n",
    "for visit in range(len(visits[53])):\n",
    "    print(f\"\\t{visit}-th visit id:\", visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", patient_visits[53][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis short titles:\",\n",
    "[icd9_to_title.get(label, label) for label in patient_visits[53][visit]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "zbciMgJuv83B"
   },
   "source": [
    "The following code outputs some descriptive stats on the complete dataset to determine how many patients have ICD-9 vs ICD-10 codes as part of their visit. This is not applicable to the demo dataset because that only contains ICD-9 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2",
    "outputId": "afcf7f26-7b48-4ca3-a9a2-6cc09e53492a"
   },
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "#Total rows with icd 9/10\n",
    "count_icd_version_10 = (diagnoses_df['icd_version'] == 10).sum()\n",
    "count_icd_version_9 = (diagnoses_df['icd_version'] == 9).sum()\n",
    "\n",
    "print(\"Number of rows with icd_version = 10:\", count_icd_version_10)\n",
    "print(\"Number of rows with icd_version = 9:\", count_icd_version_9)\n",
    "\n",
    "#Num of unique ICD 9/10 codes\n",
    "unique_icd9_codes = diagnoses_df[diagnoses_df['icd_version'] == 9]['icd_code'].nunique()\n",
    "unique_icd10_codes = diagnoses_df[diagnoses_df['icd_version'] == 10]['icd_code'].nunique()\n",
    "\n",
    "print(\"Number of unique ICD-9 codes:\", unique_icd9_codes)\n",
    "print(\"Number of unique ICD-10 codes:\", unique_icd10_codes)\n",
    "\n",
    "#num patients with atleast 1 ICD 9 code\n",
    "icd9_df = diagnoses_df[diagnoses_df['icd_version'] == 9]\n",
    "unique_patients_with_icd9 = icd9_df['subject_id'].unique()\n",
    "num_patients_with_icd9 = len(unique_patients_with_icd9)\n",
    "print(\"Number of patients with at least one ICD-9 code:\", num_patients_with_icd9)\n",
    "\n",
    "#num patients with both ICD 9 / 10 codes\n",
    "grouped = diagnoses_df.groupby('subject_id')['icd_version'].agg(set)\n",
    "patients_with_both = grouped[grouped.apply(lambda x: {9, 10}.issubset(x))]\n",
    "print(\"Number of patients with both ICD-9 and ICD-10 codes:\", len(patients_with_both))\n",
    "\n",
    "# num pateients with ONLY ICD 9 codes\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})]\n",
    "num_patients_only_icd9 = len(patients_with_only_icd9)\n",
    "print(\"Number of patients with only ICD-9 codes:\", num_patients_only_icd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "Fszz0eB_uaf3"
   },
   "source": [
    "We only want to consider patients with 4 or more visits so that we can split their visits into label and feature data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "478903a6-93b5-469f-f44b-46651f5becb6"
   },
   "outputs": [],
   "source": [
    "# Total num of usable patients\n",
    "num_unique_patients = diagnoses_df['subject_id'].nunique()\n",
    "print(\"Number of unique patients:\", num_unique_patients)\n",
    "\n",
    "\n",
    "patient_versions = diagnoses_df.groupby('subject_id')['icd_version'].unique()\n",
    "patients_with_only_icd9 = patient_versions[patient_versions.apply(lambda x: set(x) == {9})].index\n",
    "icd9_patients_df = diagnoses_df[diagnoses_df['subject_id'].isin(patients_with_only_icd9)]\n",
    "\n",
    "# visit_counts = diagnoses_df.groupby('subject_id')['hadm_id'].nunique()    # for demo data\n",
    "visit_counts = icd9_patients_df.groupby('subject_id')['hadm_id'].nunique()  # for main data\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "num_patients = len(patients_more_than_three_visits)\n",
    "print(\"Number of patients with only ICD-9 codes and more than 3 visits:\", num_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "smhEyDJBw3QY"
   },
   "source": [
    "### Vocab Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "T9pKpeuY3xbD"
   },
   "source": [
    "We create a token vocabulary to map each ICD code to a unique index. We also add special tokens as padding and separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4",
    "outputId": "c04dc78a-1738-41de-d4ff-6984b3353f1b"
   },
   "outputs": [],
   "source": [
    "#Creates Token Vocabulary\n",
    "\n",
    "import json\n",
    "# TODO i need full vocabulary\n",
    "truncated_codes = {str(code)[:3] for code in map_df['icd9_code']}\n",
    "sorted_truncated_codes = sorted(truncated_codes)  # Sort codes\n",
    "\n",
    "# Define special tokens with a specific order\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']\n",
    "\n",
    "# Create dictionary mapping each code to a unique index, starting with special tokens\n",
    "token2idx = {token: idx for idx, token in enumerate(special_tokens + sorted_truncated_codes)}\n",
    "\n",
    "# Print the number of unique codes to verify\n",
    "print(\"Number of unique truncated codes:\", len(token2idx) - len(special_tokens))\n",
    "\n",
    "# Print token to index mapping\n",
    "#print(\"Token to Index Mapping:\", token2idx)\n",
    "\n",
    "# Save the token2idx dictionary to a JSON file for later use\n",
    "with open('token2idx.json', 'w') as f:\n",
    "    json.dump(token2idx, f)\n",
    "\n",
    "    # for labels just make the full dict with all 1042 codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "ixzcGBUb5kAn"
   },
   "source": [
    "Similarly, we create a vocab dictionary for labels, excluding the special tokens, except for 'UNK', or unknown, which refers to an unknown or missing ICD code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5",
    "outputId": "20247c83-944c-4254-ac7a-fd2276845d94"
   },
   "outputs": [],
   "source": [
    "#create the label vocab\n",
    "#same as token vocab but no SEP, CLS, PAD, MASk (Leave UNK)\n",
    "# DID NOT reorganize indices, may need to\n",
    "\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "labelVocab = {token: idx for token, idx in token2idx.items() if token not in special_tokens}\n",
    "\n",
    "\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    del token2idx['[PAD]']\n",
    "    del token2idx['[SEP]']\n",
    "    del token2idx['[CLS]']\n",
    "    del token2idx['[MASK]']\n",
    "    token = list(token2idx.keys())\n",
    "    labelVocab = {}\n",
    "    for i,x in enumerate(token):\n",
    "        labelVocab[x] = i\n",
    "    return labelVocab\n",
    "\n",
    "labelVocab = format_label_vocab(token2idx)\n",
    "\n",
    "# Print the new dictionary to verify\n",
    "print(len(labelVocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "tg25otPTxG77"
   },
   "source": [
    "### Label and Feature Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "gqjEHETIBEwP"
   },
   "source": [
    "First three lines of code used for filtering out patients with ICD-10 code -- commented out since it's not applicable for demo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7",
    "outputId": "bd1cdb1e-8e3a-418b-c19f-18dfa56aada9"
   },
   "outputs": [],
   "source": [
    "#preparing the label and feature splits\n",
    "\n",
    "patients_with_icd10 = diagnoses_df[diagnoses_df['icd_version'] == 10]['subject_id'].unique()\n",
    "\n",
    "icd9_only_df = diagnoses_df[~diagnoses_df['subject_id'].isin(patients_with_icd10)]\n",
    "\n",
    "#Finds patients with only icd9 codes\n",
    "icd9_only_df = icd9_only_df[icd9_only_df['icd_version'] == 9]\n",
    "\n",
    "# icd9_only_df = diagnoses_df   # for demo data\n",
    "visit_counts = icd9_only_df.groupby('subject_id')['hadm_id'].nunique()\n",
    "\n",
    "patients_more_than_three_visits = visit_counts[visit_counts > 3].index\n",
    "\n",
    "#final DataFrame of patients with only ICD-9 codes and more than three visits\n",
    "final_df = icd9_only_df[icd9_only_df['subject_id'].isin(patients_more_than_three_visits)]\n",
    "\n",
    "patient_visits = final_df.groupby(['subject_id', 'hadm_id'])[icd_code_col_name].apply(list).reset_index()\n",
    "patient_visits = patient_visits.groupby('subject_id')[icd_code_col_name].apply(list)\n",
    "\n",
    "#extract all ICD9 codes but only take first 3 digits\n",
    "map_df['truncated_icd9'] = map_df['icd9_code'].apply(lambda x: x[:3])\n",
    "\n",
    "unique_truncated_codes = sorted(map_df['truncated_icd9'].unique())\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_truncated_codes)}\n",
    "\n",
    "def encode_labels(label_codes, labelVocab):\n",
    "    # print(len(label_codes), len(labelVocab))\n",
    "    multi_hot = [0] * len(labelVocab)\n",
    "    #print(len)\n",
    "\n",
    "    for code in label_codes:\n",
    "        if code in labelVocab:\n",
    "            index = labelVocab[code]\n",
    "            multi_hot[index] = 1\n",
    "        else:\n",
    "            print(f\"Warning: Code {code} not found in labelVocab.\")\n",
    "\n",
    "    return multi_hot\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "labels = {}\n",
    "\n",
    "for subject_id, visits in patient_visits.items():\n",
    "    if len(visits) > 3:\n",
    "        # split_index = len(visits) // 2    # splits visits down middle\n",
    "\n",
    "        split_index = random.randint(len(visits)-2, len(visits)-1)    # random split index from 3 to end of visits array\n",
    "\n",
    "\n",
    "        # Initialize the feature list with 'CLS'\n",
    "        feature_visits = ['CLS']\n",
    "\n",
    "        # Append codes and 'SEP' after each visit up to the split index\n",
    "        for sublist in visits[:split_index]:\n",
    "            visit_codes = [code for code in sublist]\n",
    "            visit_codes.append('SEP')\n",
    "            feature_visits.extend(visit_codes)\n",
    "\n",
    "        # Append the feature list to the features\n",
    "        features.append(feature_visits)\n",
    "\n",
    "        # Gather label codes from visits after the split index\n",
    "        label_codes = [code[:3] for sublist in visits[split_index:] for code in sublist]\n",
    "\n",
    "        # Encode the labels and store using subject_id as key\n",
    "        labels[subject_id] = encode_labels(label_codes, labelVocab)\n",
    "\n",
    "\n",
    "#output for one set of features and labels\n",
    "if features and labels:\n",
    "    print(\"Example Features: \", features[0])\n",
    "    print(\"length Labels: \" , len(labels))\n",
    "    print(\"example label\", len(labels[list(patient_visits.items())[0][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "_UevB7i8K6Hi"
   },
   "source": [
    "### Custom Dataset\n",
    "\n",
    "This custom dataset NextVisit prepares input sequences with appropriate tokens, masks, and labels for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "#custom dataset helpers\n",
    "\n",
    "def index_seg(tokens, symbol=2):\n",
    "    flag = 0\n",
    "    seg = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            seg.append(flag)\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "        else:\n",
    "            seg.append(flag)\n",
    "    return seg\n",
    "\n",
    "\n",
    "def position_idx(tokens, symbol=2):\n",
    "    pos = []\n",
    "    flag = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token == symbol:\n",
    "            pos.append(flag)\n",
    "            flag += 1\n",
    "        else:\n",
    "            pos.append(flag)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "#custom dataset\n",
    "\n",
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, labels, patient_visits, max_len):\n",
    "        self.token2idx = token2idx\n",
    "        self.labels = labels\n",
    "        self.patient_visits = patient_visits\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_visits)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # code_idxs, position idx, mask, segment, label\n",
    "        # Retrieve patient data by index\n",
    "        patient_id = list(self.patient_visits.keys())[index]\n",
    "        codes = self.patient_visits[patient_id]\n",
    "\n",
    "        # Initialize sequence with [CLS] token\n",
    "        sequence = [self.token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "\n",
    "        # Add each code to the sequence and append [SEP] after each visit\n",
    "        for visit in codes:\n",
    "            #change 'code' to be truncated version!!!!!!\n",
    "            sequence.extend([self.token2idx.get(code[:3], self.token2idx['[UNK]']) for code in visit])\n",
    "            sequence.append(self.token2idx['[SEP]'])\n",
    "\n",
    "\n",
    "\n",
    "        # Cut or pad the sequence to the maximum length\n",
    "        if len(sequence) > self.max_len:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        else:\n",
    "            sequence.extend([self.token2idx['[PAD]']] * (self.max_len - len(sequence)))\n",
    "\n",
    "        position_indices = position_idx(sequence)\n",
    "        segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "        # Create a mask for the sequence\n",
    "        mask = [1 if token != self.token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "        # Prepare the labels\n",
    "        label = torch.tensor(self.labels[patient_id], dtype=torch.float)\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(position_indices, dtype=torch.long), torch.tensor(mask, dtype=torch.long), torch.tensor(segment, dtype=torch.long), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "9",
    "outputId": "a76c7dbc-675f-4c79-c7fb-a3750ba00ed9"
   },
   "outputs": [],
   "source": [
    "#testint stuff out\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "patient_id = list(patient_visits.keys())[0]\n",
    "codes = patient_visits[patient_id]\n",
    "\n",
    "# Initialize sequence with [CLS] token\n",
    "sequence = [token2idx['[CLS]']]\n",
    "\n",
    "\n",
    "# Add each code to the sequence and append [SEP] after each visit\n",
    "for visit in codes:\n",
    "    sequence.extend([token2idx.get(code[:3], token2idx['[UNK]']) for code in visit])\n",
    "    sequence.append(token2idx['[SEP]'])\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "\n",
    "# Cut or pad the sequence to the maximum length\n",
    "if len(sequence) > max_len:\n",
    "    sequence = sequence[:max_len]\n",
    "else:\n",
    "    sequence.extend([token2idx['[PAD]']] * (max_len - len(sequence)))\n",
    "\n",
    "position_indices = position_idx(sequence)\n",
    "segment = index_seg(sequence)\n",
    "\n",
    "\n",
    "mask = [1 if token != token2idx['[PAD]'] else 0 for token in sequence]\n",
    "\n",
    "label = torch.tensor(labels[patient_id], dtype=torch.float)\n",
    "\n",
    "print(len(sequence))\n",
    "print(sequence)\n",
    "\n",
    "print(len(position_indices))\n",
    "print(\"position: \", position_indices)\n",
    "\n",
    "print(len(segment))\n",
    "print(\"segment: \", segment)\n",
    "\n",
    "print(len(mask))\n",
    "print(\"mask: \", mask)\n",
    "\n",
    "# print(len(labels[0]))\n",
    "print(\"labels: \" , label)\n",
    "\n",
    "'''\n",
    "A bunch of 3 ([UNK]) because patient_visits includes full ICD Codes but token2idx has truncated version\n",
    "Need to do version matching before feeding into model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "P4j-PxSk7ABm"
   },
   "source": [
    "##*Model*\n",
    "\n",
    "###Original Paper: TransformEHR:\n",
    "* Citation: Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
    "* Repo: https://github.com/whaleloops/TransformEHR/tree/main\n",
    "\n",
    "\n",
    "###BEHRT\n",
    "* Citation: Li, Y., Rao, S., Solares, J.R.A. et al. BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y\n",
    "\n",
    "\n",
    "* Repo: https://github.com/deepmedicine/BEHRT/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "Z_MK-iXT5rBs"
   },
   "source": [
    "###Model Description:\n",
    "\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "\n",
    "* Model architecture: The model uses 6 hidden layers, 12 attention heads, intermediate layer size of 512, and hidden size of 288.\n",
    "* We used relu and gelu activation functions for the encoder and pooler.\n",
    "* Weight decay = 0.02\n",
    "* Model classes:\n",
    "  * BertEmbeddings\n",
    "  * BertModel\n",
    "  * BertForMultiLabelPrediction\n",
    "* Our model utilizes an ICD9Embeddings class for pretrained ICD embeddings.\n",
    "\n",
    "## **If you want to load a trained model search for the section (Loading presaved/trained model)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "PlpC53B95as5"
   },
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "acjhC1na5-Xc"
   },
   "source": [
    "Setting up parameters and configurations to use in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertPreTrainedModel , BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda',\n",
    "    'output_dir': '',  # output dir\n",
    "    'best_name': '', # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 512,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'posi': True\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.02\n",
    "}\n",
    "model_config = {\n",
    "    'vocab_size': len(labelVocab), # number of disease + symbols for word embedding 1047\n",
    "    'hidden_size': 300, # word embedding and seg embedding hidden size\n",
    "    #'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    #'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.3, # dropout rate\n",
    "    'num_hidden_layers': 4, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.45, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'relu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "class BertConfig(BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        #self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        #self.age_vocab_size = config.get('age_vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "full_dataset = NextVisit(token2idx, labels, patient_visits, max_len)\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=global_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=global_params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ICD9Embeddings:\n",
    "    def __init__(self, filename=\"./ic9_embeddings.txt\"):\n",
    "        self.embedding_filename = filename\n",
    "        # self.icd9_to_embeddings = self._read_embeddings()\n",
    "        self.icd9_to_embeddings = self._read_embeddings_trunc()\n",
    "        self.embedding_size = 300\n",
    "\n",
    "    # this reads the 5 digit code correctly\n",
    "    def _read_embeddings(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "\n",
    "                icd9_to_embeddings[code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    # this does the opposite of greedy it basically just takes the last icd_9 with the first three that match\n",
    "    def _read_embeddings_trunc(self):\n",
    "        icd9_to_embeddings = {}\n",
    "        codes_lost = 0\n",
    "        with open(self.embedding_filename, \"r\") as infile:\n",
    "            data = infile.readlines()\n",
    "            for row in data:\n",
    "                eles = row.strip().split(\" \")\n",
    "                name = eles[0]\n",
    "                embedding = eles[1:]\n",
    "                code = name[4:]\n",
    "\n",
    "                code = code.replace(\".\", \"\")\n",
    "                if len(code) > 5 or len(code) < 3:\n",
    "                    print(\"code is bad\")\n",
    "                trunc_code = code[:3]\n",
    "                # print(trunc_code)\n",
    "                if trunc_code in icd9_to_embeddings:\n",
    "                    codes_lost += 1\n",
    "\n",
    "                icd9_to_embeddings[trunc_code] = torch.tensor(\n",
    "                    [float(i) for i in embedding], dtype=torch.float32\n",
    "                )\n",
    "        # print('codes_lost', codes_lost)\n",
    "        return icd9_to_embeddings\n",
    "\n",
    "    def get(self, code):\n",
    "        if code in self.icd9_to_embeddings:\n",
    "            return self.icd9_to_embeddings[code]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_idx_to_embedding(self, token2idx):\n",
    "        idx2embedding = {}\n",
    "        for code, idx in token2idx.items():\n",
    "            if code in self.icd9_to_embeddings:\n",
    "                idx2embedding[idx] = self.icd9_to_embeddings[code]\n",
    "            else:\n",
    "                idx2embedding[idx] = torch.zeros(\n",
    "                    self.embedding_size, dtype=torch.float32\n",
    "                )\n",
    "                # print(\"code is not in icd9 embeddings\", code)\n",
    "\n",
    "        return idx2embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "\n",
    "        # TODO maybe load these as part of the Dataset so we don't have to do extra lookups\n",
    "\n",
    "        # self.icd9_embeddings = ICD9Embeddings(\"./embeddings/ic9_embeddings.txt\")\n",
    "        self.icd9_embeddings = ICD9Embeddings(icd9_embedding_txt)\n",
    "        self.idx2embedding = self.icd9_embeddings.get_idx_to_embedding(token2idx)\n",
    "\n",
    "\n",
    "        icd9_codes = list(self.idx2embedding.keys())\n",
    "        # print('length of icd9 codes', len(icd9_codes))\n",
    "\n",
    "        embeddings_matrix = torch.stack([self.idx2embedding[code] for code in icd9_codes], dim=0)\n",
    "        # print('embeddings matrix shape', embeddings_matrix.shape)\n",
    "\n",
    "        # Initialize embeddings for CLS and SEP tokens\n",
    "        additional_embeddings = torch.randn(2, config.hidden_size)\n",
    "\n",
    "        # Combine precomputed and additional embeddings\n",
    "        full_embeddings_matrix = torch.cat([embeddings_matrix, additional_embeddings], dim=0)\n",
    "        # self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=False)\n",
    "        # self.word_embeddings = nn.Embedding.from_pretrained(full_embeddings_matrix, freeze=True)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)      # from scratch\n",
    "\n",
    "\n",
    "\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "\n",
    "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, posi_ids=None, segment_ids=None):\n",
    "        # print('embeddings forward')\n",
    "        if segment_ids is None:\n",
    "            pass\n",
    "\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "        # print('max word id', word_ids.max().item())\n",
    "        # print('embeddings size', self.word_embeddings.weight.shape)\n",
    "        # print(word_ids)\n",
    "        # print('word_ids shape', word_ids.shape)\n",
    "\n",
    "        embeddings = self.word_embeddings(word_ids)\n",
    "        # print('embeddings',embeddings)\n",
    "\n",
    "        # posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "\n",
    "        # if self.feature_dict['posi']:\n",
    "        #     embeddings = embeddings + posi_embeddings\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None,segment_ids=None,output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(posi_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# this should be fairly hands off we should just need to adjust the config parameters\n",
    "class BertForMultiLabelPrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, feature_dict):\n",
    "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, posi_ids=None, attention_mask=None, segment_ids=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, posi_ids, attention_mask,segment_ids,\n",
    "                                     output_all_encoded_layers=False)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, model_config['vocab_size'], feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": [
    "model = model.to(global_params['device'])\n",
    "# model = model.to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "zx0JTvnb7ZKG"
   },
   "source": [
    "## *Evaluation Metric Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, output, label\n",
    "\n",
    "def auroc_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    tempprc= sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "#     roc = sklearn.metrics.roc_auc_score()\n",
    "    return tempprc\n",
    "\n",
    "def accuracy(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    # Apply a threshold to convert probabilities to binary predictions\n",
    "    # predictions = (output.numpy() > 0.5).astype(int)\n",
    "    # # Calculate accuracy\n",
    "    # acc = sklearn.metrics.accuracy_score(label.numpy(), predictions)\n",
    "\n",
    "    probabilities = torch.sigmoid(logits).detach().cpu()\n",
    "\n",
    "    threshold = 0.5\n",
    "    # Apply threshold to convert probabilities to binary predictions\n",
    "    predictions = (probabilities >= threshold).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = (predictions == label).float()  # Element-wise comparison\n",
    "    # print('where correct: ', np.where(predictions[5] == label[5]))\n",
    "    accuracy = correct_predictions.mean()  # Mean accuracy across all predictions and labels\n",
    "\n",
    "    return accuracy.item()\n",
    "\n",
    "    # # Calculate accuracy for each label independently\n",
    "    # label_accuracies = []\n",
    "\n",
    "    # for key in labels:\n",
    "    #   label_array = labels[key]  # Get the label array corresponding to the key\n",
    "    #   prediction_array = predictions[key]  # Get the prediction array corresponding to the key\n",
    "\n",
    "    #   # Calculate accuracy for this label category\n",
    "    #   acc = sklearn.metrics.accuracy_score(label_array, prediction_array)\n",
    "    #   label_accuracies.append(acc)\n",
    "    # # Take mean of all accuracies\n",
    "    # overall_acc = np.mean(label_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "    return acc, output, label\n",
    "\n",
    "def print_result(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits)\n",
    "    label, output = label.cpu(), output.detach().cpu()\n",
    "\n",
    "    # predictions = (output.numpy() > 0.5).astype(int)\n",
    "    predictions = np.argsort(output.numpy())\n",
    "    predictions = predictions[::-1]\n",
    "\n",
    "\n",
    "    # print('predicted diseases',     np.where(predictions == 1)[:5])\n",
    "    print('predicted diseases',     predictions[:5])\n",
    "    # print(output.numpy()[predictions[:5]])\n",
    "    print('actual diseases',     np.where(label == 1)[:5])\n",
    "    # print('label',     label)\n",
    "\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, roc, output, label,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "s45Dvanq7htd"
   },
   "source": [
    "## *Training*\n",
    "\n",
    "\n",
    "\n",
    "**Hyperparameters**\n",
    "are set in variables `global_params`, `model_config`, and `optim_config`, such as:\n",
    "* Batch Size: 64\n",
    "* Hidden Size: 300\n",
    "* Number of Hidden Layers: 6\n",
    "* Dropout Rate: 0.2\n",
    "  * finetuned to 0.5\n",
    "\n",
    "**Computational Requirements:**\n",
    "* Hardware type: GPU T4\n",
    "* Avg runtime per epoch: 2 min.\n",
    "* GPU units used: 40\n",
    "* Number of training epochs: 25 or 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "BZ0OXUuvBXI_"
   },
   "source": [
    "###Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "18"
   },
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        cnt +=1\n",
    "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
    "\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "\n",
    "        loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 50 ==0:\n",
    "            prec, a, b = precision(logits, targets)\n",
    "            acc = accuracy(logits,targets)\n",
    "            auroc = auroc_test(logits,targets)\n",
    "            print_result(logits[2], targets[2])\n",
    "            # acc = 0\n",
    "\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\\t| Accuracy: {}\\t| AUROC:{} \".format(e, cnt,temp_loss/500, prec, acc, auroc))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "EKA1_ZCy9MuE"
   },
   "source": [
    "## *Evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "Hkx4sfPUD0Lv"
   },
   "source": [
    "### Metric Description\n",
    "\n",
    "Our model measures the following metrics:\n",
    "* Precision\n",
    "* Accuracy\n",
    "* Evaluation Loss\n",
    "\n",
    "The calculation functions for these precision and accuracy can be found above, while loss is calculated in the `evaluation()` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "NlEwjnmiC7NQ"
   },
   "source": [
    "### Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "A4XMXPK_9PQk"
   },
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        input_ids, posi_ids, attMask, segment_ids, targets = batch\n",
    "\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "          loss, logits = model(input_ids, posi_ids,attention_mask=attMask, segment_ids=segment_ids, labels=targets)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, tr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "9uNX31RFUusd"
   },
   "source": [
    "#Loading presaved/trained model:*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "id": "e78Ov__CUsYS"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '/content/gdrive/MyDrive/598ehrupload/saved_models/best_model_unfrozen_embeddings_finetune.pt'\n",
    "checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "y1CcduZDE7nN"
   },
   "source": [
    "# Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "cPAb1wwQFEk0"
   },
   "source": [
    "The following code is for running the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19",
    "outputId": "08202b1b-a599-4107-d364-cae6c538d059"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If using Google Drive\n",
    "base_dir = '/content/gdrive/MyDrive/598ehrupload/saved_models'\n",
    "# If using local Colab space\n",
    "# base_dir = '/content/ModelDirectory'\n",
    "\n",
    "global_params['output_dir'] = base_dir\n",
    "global_params['best_name'] = 'best_model_unfrozen_embeddings_finetune.pt'\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "best_pre = 0.0\n",
    "for e in range(50):\n",
    "    print(\"starting training...\")\n",
    "    train(e)\n",
    "    aps, roc, test_loss = evaluation()\n",
    "    if aps >best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = aps\n",
    "    print('aps : {}'.format(aps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "ZUoYWE3t77Rm"
   },
   "source": [
    "# **Results**\n",
    "\n",
    "Results (15)\n",
    "\n",
    "* Table of results (*These metrics measure the ability for the model to predict subsequent diagnosis based on a patients diagnosis history*)\n",
    "  * Pretrained unfrozen embeddings |  precision: 0.27581077047200264, AUROC:0.8643956994104\n",
    "  * Pretrained frozen embeddings |  precision: 0.22871352401267167, AUROC:0.8595027934194563\n",
    "  * Randomly Initialized embeddings | precision: 0.2644318730070032\tAUROC:0.8797517785185457\n",
    "  * no positional embeddings | precision: precision: 0.2730285026690921\t AUROC:0.8678234320505926\n",
    "\n",
    "* Discuss with respect to the hypothesis and results from the original paper\n",
    "   * We were not able to see a statistically significant difference in adding positional embeddings, nor were we able to reproduce the level of precision that the paper was able to, partially because they had access to 8.1 million patients while we only had 11,000 patients. However following their basic architecture we were able to achieve very high accuracy scores, though a better indication would be the high AUROC score. However analyzing the actual outputs of the model paints a strange picture of overfitting and poor data quality. The model begins to learn the frequency of diseases rather than the corrolation between features and predictions, as it is very keen to predict common diseases, for example in our labeled dataset it is common for later vists to contain \"Facial nerve disorders\" so the model predicts this disease in particular quite often. Which seems to point to class imbalance and this model underfitting the data, or just not enough data to begin with.\n",
    "* Experiments beyond the original paper\n",
    "   * In order to allow the model to converge faster with less training data we attempted to use pretrained icd-9 code embeddings, this also made a neglible difference in terms of performance. We even attempted to see what the different would be if we froze the weights, since the weights are supposed to be pretrained and the relationships between these diseases should already be known. However after performing an ablation study by omitting the loading of the pretrained embeddings weights we are able to see that the preformance, while it does converge slower, its precision remains low until the 3rd epoch, it is able to achieve the same level of precision and AUROC after 10 epochs\n",
    "* Ablation Study.\n",
    "The main ablation we replicated from the paper is the exclusion of positional embeddings. And this change does not seem to affect the performance of the model that much. See experiment section. Similarly the addition of pretrained embeddings while makes the model converge faster ultimatly even with randomly intialized embeddings the model is able to learn the embeddings on its own. See Experiments beyond the original paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "mMUiojNT8MD3"
   },
   "source": [
    "# **Discussion**\n",
    "\n",
    "As previously mentioned, due to the missing code from our original paper, TransformEHR, we found that it was *not reproducible*. Since we did not have access to the data collator functions or the tokenizers that the authors used to prepare the data, we were not able to determine the exact format needed to run the model and reproduce results. We would recommend that the authors of this paper upload the `datasets.py` file which includes these missing functions that would allow the model to run.\n",
    "\n",
    "The other paper we followed in order to achieve similar goals, BEHRT, was more reproducible, as it contained all the code necessary to run the model, including the custom dataset NextVisit we modeled ours off of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "6Ioh8U32AkG8"
   },
   "source": [
    "# **References**\n",
    "\n",
    "* BEHRT: Transformer for Electronic Health Records\n",
    "  * Paper: https://www.nature.com/articles/s41598-020-62922-y\n",
    "  * GitHub Repo: https://github.com/deepmedicine/BEHRT/tree/master\n",
    "* LLM Embeddings for ICD-10 Data\n",
    "  * https://github.com/whaleloops/TransformEHR/tree/main\n",
    "* TransformEHR\n",
    "  * Paper: https://doi.org/10.1038/s41467-023-43715-z\n",
    "  * GitHub Repo: https://github.com/whaleloops/TransformEHR/tree/main\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "lzwOb0ohK7tb"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
