{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Before you use this template\n",
    "\n",
    "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
    "\n",
    "---\n",
    "\n",
    "# FAQ and Attentions\n",
    "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
    "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
    "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
    "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
    "must be within 8 min, otherwise, you may get penalty on the grade.\n",
    "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
    "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
    "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
    "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
    "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
    "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlv6knX04FiY"
   },
   "source": [
    "# Mount Notebook to Google Drive\n",
    "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
    "\n",
    "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
    "\n",
    "*   Background of the problem\n",
    "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
    "  * what is the importance/meaning of solving the problem\n",
    "  * what is the difficulty of the problem\n",
    "  * the state of the art methods and effectiveness.\n",
    "\n",
    "\n",
    "\n",
    "*   Paper explanation\n",
    "  * what did the paper propose\n",
    "  * what is the innovations of the method\n",
    "  * how well the proposed method work (in its own metrics)\n",
    "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "\n",
    "## Background:\n",
    "\n",
    "Current pretraining objectives in predictive EHR-based models are limited to predicting a fraction of ICD codes within a patient’s visit, when in reality, patients usually have multiple, often highly-correlated diseases. In addition, current models are unable to accurately predict the timeline of correlated diagnoses and could lead to missed opportunities in preventative care. Predictive tasks surrounding healthcare data can be challenging due to the complexity of healthcare data, which includes high-dimensional and often incomplete patient data over time. The state of the art methods used to solve similar healthcare related problems have been transformer based deep learning models trained on extensive datasets and fine-tuned for specific tasks. Despite their success, current models are often fine-tuned to focus on predicting a limited set of outcomes, thus overlooking the interconnected nature of various health conditions.\n",
    "\n",
    "\n",
    "\n",
    "## Paper Explanation:\n",
    "\n",
    "The paper presents \"TransformEHR,\" a novel generative encoder-decoder model leveraging transformer architecture, specifically designed for predicting future patient outcomes based on their longitudinal EHRs. The authors utilized techniques like visit-masking and time embedding to achieve results that outperform the other state of the art models. For example, when testing their encoder-decoder model against an encoder only model, the authors were able to achieve an, “improvement of 95%CI: 0.74%–1.16%, p < 0.001 in AUROC across all diseases/outcomes tested.” While the paper boasted strong results on a variety of both common and uncommon diseases, the authors mentioned that their work was related to predictive model studies focused on intentional self-harm. TransformEHR performed exceptionally well within this subspace and esteemed to reduce incremental cost-effective ratio by $109k per quality-adjusted life-years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABD4VhFZbehA"
   },
   "outputs": [],
   "source": [
    "# code comment is used as inline annotations for your coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "\n",
    "1.   Hypothesis 1: Visit Masking vs Code Masking\n",
    "  * This test will see whether masking all ICD codes of a visit has significant results compared to masking only a selected fragment of ICD codes per visit\n",
    "  * Both models will be encoder-decoder\n",
    "  * This would have to be done by changing the training process to mask out data per run\n",
    "    * far more difficult and don't see the code the paper reference to do this\n",
    "2.  Hypothesis 2: Time embedding vs Time Embedding Excluded\n",
    "  * This test will see whether incliding time embedding (for temporal information of prior visits) has significant results compared to excluding them.\n",
    "  * Both models will be encoder-decoder\n",
    "  * This will be done by deleting the self.embed_positions.\n",
    "    * `x = inputs_embeds + embed_pos + embed_visit`\n",
    "    * we should be able to delete embed_pos without changing any config\n",
    "\n",
    "3. (If time allows) encoder-decoder vs encoder only\n",
    "  * This test compares the performance of an enconder-deocder model versus an encoder model trained on the same dataset\n",
    "\n",
    "\n",
    "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
    "\n",
    "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM4WUjz64C3B"
   },
   "source": [
    "\n",
    "You can also use code to display images, see the code below.\n",
    "\n",
    "The images must be saved in Google Drive first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "'''\n",
    "# mount this notebook to your google drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# define dirs to workspace and data\n",
    "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(img_dir)\n",
    "cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch\n",
    "# !pip3 install transformers\n",
    "\n",
    "# import  packages you need\n",
    "import numpy as np\n",
    "# from google.colab import drive\n",
    "import torch\n",
    "import transformers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
    "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    " * We start with the hospital admissions dataset and patient diagnoses dataset from MIMIC-IV. https://physionet.org/content/mimiciv/2.2/\n",
    " * We preprocess this raw data into local files. We format patient data as: `[([icd-version-icd-code], date time)]`\n",
    " * Our sample dataset has 180640 patients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P85Vi0WSH9pV"
   },
   "source": [
    "### Preprocess the data\n",
    " * Start with raw data from mimiciv\n",
    " * map patients with visits and diagnoses codes\n",
    " * add timestamps or relevant time information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aNeZ1q3rH_Xr"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/corpora_alpha/MIMIC/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Maps visit_id's to discahrge dates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/corpora_alpha/MIMIC/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhadm_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m visitid2dischargedate \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, row \u001b[38;5;129;01min\u001b[39;00m data_df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:765\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    763\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m    764\u001b[0m         \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m         handle \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile(\n\u001b[1;32m    772\u001b[0m             \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[1;32m    773\u001b[0m             \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompression_args,\n\u001b[1;32m    777\u001b[0m         )\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/gzip.py:173\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/corpora_alpha/MIMIC/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz'"
     ]
    }
   ],
   "source": [
    "#Maps visit_id's to discahrge dates\n",
    "data_df = pd.read_csv('/data/corpora_alpha/MIMIC/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz', nrows=None, compression='gzip',\n",
    "            dtype={'subject_id': str, 'hadm_id': str} )\n",
    "visitid2dischargedate = {}\n",
    "for ind, row in data_df.iterrows():\n",
    "    visitid2dischargedate[row['hadm_id']] = row['dischtime'][0:10]\n",
    "\n",
    "print(min(visitid2dischargedate.values()))\n",
    "print(max(visitid2dischargedate.values()))\n",
    "\n",
    "#creates a dictionary of patients {patient_id (key) : diagnoses codes}\n",
    "data_df = pd.read_csv('/data/corpora_alpha/MIMIC/physionet.org/files/mimiciv/2.2/hosp/diagnoses_icd.csv.gz', nrows=None, compression='gzip',\n",
    "            dtype={'subject_id': str, 'hadm_id': str, 'icd_code': str, 'icd_version': str} )\n",
    "patients = defaultdict(lambda: defaultdict(list)) #lambda: \"Not Present\"\n",
    "for ind, row in data_df.iterrows():\n",
    "    hadm_id = row['hadm_id']\n",
    "    scrssn = row['subject_id']\n",
    "    visit_date = visitid2dischargedate[hadm_id]\n",
    "    patients[scrssn][visit_date].append(row['icd_version'] +'-'+ row['icd_code']) # 687621183, 912831070\n",
    "\n",
    "num_icd_pat = defaultdict(int)\n",
    "for k,v in patients.items():\n",
    "    for kv, vv in v.items():\n",
    "        for icdcode in vv:\n",
    "            if icdcode.startswith(\"10-\"):\n",
    "                num_icd_pat[k] += 1\n",
    "                break\n",
    "\n",
    "print(len(patients))\n",
    "print(len(num_icd_pat))\n",
    "\n",
    "num_pos = 0\n",
    "for k,v in num_icd_pat.items():\n",
    "    if v > 1:\n",
    "        num_pos += 1\n",
    "print(num_pos)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "#adds timestamps??\n",
    "def icd2cui(patients, logging_step=50000):\n",
    "    dictionary = defaultdict(int)\n",
    "    # cuis_li = []\n",
    "    cuis_di = {}\n",
    "    date_di = {}\n",
    "    num_idx = 0\n",
    "    for pssn,v in patients.items():\n",
    "        num_idx += 1\n",
    "        if num_idx%logging_step == 0:\n",
    "            print(\"|{} - Processed {}\".format(time.asctime(time.localtime(time.time())), num_idx), flush=True)\n",
    "        cuis_di[pssn] = []\n",
    "        cuis_li_tmp = []\n",
    "        date_li_tmp = []\n",
    "        for datetime_str in sorted(v.keys()): # sort by time\n",
    "            datetime_object = datetime.strptime(datetime_str, '%Y-%m-%d') # make sure time str is correct\n",
    "            infos = v[datetime_str]\n",
    "            if len(infos) > 0:\n",
    "                # cuis_di[pssn].append((cuis, ext_cuis, strs))\n",
    "                cuis_li_tmp.append((infos, [], []))\n",
    "                date_li_tmp.append(datetime_str)\n",
    "            for cui_id in infos:\n",
    "                dictionary[cui_id] += 1\n",
    "        if len(cuis_li_tmp) > 0:\n",
    "            cuis_di[pssn] = cuis_li_tmp\n",
    "            date_di[pssn] = date_li_tmp\n",
    "    return cuis_di, date_di, dictionary\n",
    "\n",
    "#putting everything together\n",
    "patients_few = dict(islice(patients.items(), 0, 200))\n",
    "cuis, date, dictionary = icd2cui(patients, logging_step=50000)\n",
    "\n",
    "#Saving the preprocessed data into 3 seperate files\n",
    "dir_apth = 'path/to/file' #file to save preprocessed data\n",
    "print(\"Number of cui in dictionary: {}\".format(len(dictionary)), flush=True)\n",
    "with open(dir_apth + '/dict.txt', 'w') as handle: #TODO\n",
    "    handle.write(\"[PAD]\"+\"\\n\")\n",
    "    for i in range(99):\n",
    "        handle.write(\"[unused{}]\".format(i)+\"\\n\")\n",
    "    handle.write(\"[UNK]\"+\"\\n\")\n",
    "    handle.write(\"[CLS]\"+\"\\n\")\n",
    "    handle.write(\"[SEP]\"+\"\\n\")\n",
    "    handle.write(\"[MASK]\"+\"\\n\")\n",
    "    for i in range(99,194):\n",
    "        handle.write(\"[unused{}]\".format(i)+\"\\n\")\n",
    "    for k,v in dictionary.items():\n",
    "        handle.write(\"{}\\n\".format(k))\n",
    "# save data\n",
    "print(\"Saving patient data...\", flush=True)\n",
    "f1 = open(dir_apth + '/value.pickle', 'wb')\n",
    "f3 = open(dir_apth + '/dates.pickle', 'wb')\n",
    "f2 = open(dir_apth + '/key.txt', 'w')\n",
    "for k,v in cuis.items():\n",
    "    pickle.dump(v, f1, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(date[k], f3, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f2.write(\"{}\\n\".format(k))\n",
    "f1.close()\n",
    "f3.close()\n",
    "f2.close()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h2D4j5CQRit"
   },
   "source": [
    "### Loading the Data from 3 previously saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "suHKWTBVQT3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients in the sample dataset:  180640\n",
      "[((['9-5723', '9-78959', '9-5715', '9-07070', '9-496', '9-29680', '9-30981', '9-V1582'], [], []), '2180-05-07'), ((['9-07071', '9-78959', '9-2875', '9-2761', '9-496', '9-5715', '9-V08', '9-3051'], [], []), '2180-06-27'), ((['9-45829', '9-07044', '9-7994', '9-2761', '9-78959', '9-2767', '9-3051', '9-V08', '9-V4986', '9-V462', '9-496', '9-29680', '9-5715'], [], []), '2180-07-25'), ((['9-07054', '9-78959', '9-V462', '9-5715', '9-2767', '9-2761', '9-496', '9-V08', '9-3051', '9-78791'], [], []), '2180-08-07')]\n",
      "format of the data is as follows [([icd-version-icd-code], date time)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dir_path = \"./train_data\" #edit with path file\n",
    "do_date = True\n",
    "\n",
    "\n",
    "f1 = open(dir_path+ '/value.pickle', 'rb')\n",
    "f3 = open(dir_path+ '/dates.pickle', 'rb')\n",
    "f2 = open(dir_path+ '/key.txt', 'r')\n",
    "keys = f2.readlines()\n",
    "\n",
    "patients = {}\n",
    "for key in keys:\n",
    "    patient_idd = key.strip()\n",
    "    each_visit = pickle.load(f1)\n",
    "    f1obj = []\n",
    "    for (cuis, ext_cuis, strs) in each_visit:\n",
    "        f1obj.append((cuis, [], []))\n",
    "    f3obj = pickle.load(f3)\n",
    "    assert len(f1obj) == len(f3obj)\n",
    "    if do_date:\n",
    "        if patient_idd in patients:\n",
    "            patients[patient_idd] += list(zip(f1obj, f3obj))\n",
    "        else:\n",
    "            patients[patient_idd] = list(zip(f1obj, f3obj))\n",
    "    else:\n",
    "        if patient_idd in patients:\n",
    "            patients[patient_idd] += f1obj\n",
    "        else:\n",
    "            patients[patient_idd] = f1obj\n",
    "\n",
    "\n",
    "print(\"number of patients in the sample dataset: \", len(patients))\n",
    "\n",
    "# print(patients[list(patients.keys())[0]])\n",
    "# print(\"format of the data is as follows [([icd-version-icd-code], date time)]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Descriptions\n",
    "\n",
    "* We will use the `ICDBartForPreTraining` from `icdmodelbart.py`. `ICDBartForPreTraining` is an extension of `BartModel` used for pretraining in clinical data processing, and includes features for resizing token embeddings and handling masked language modeling objectives. \n",
    "* The model contains multiple forward-pass encoder and decoder layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation/Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartEncoder: SinusoidalPositionalEmbedding(config.max_position_embeddings, embed_dim, self.padding_idx)\n",
      "nn.Embedding(1460, embed_dim)\n",
      "self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n",
      "self.layer_norm = LayerNorm(config.d_model) if config.normalize_before else None\n",
      "BartDecoder: self.embed_positions = LearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, self.padding_idx)\n",
      "self.layernorm_embedding = LayerNorm(config.d_model) if config.normalize_embedding else nn.Identity()\n",
      "self.layer_norm = LayerNorm(config.d_model) if config.normalize_before else None\n",
      "world master -- one process only\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Initialize distributed training\n",
    "# dist.init_process_group(backend='nccl',rank=0,world_size=1)\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import icdmodelbart\n",
    "import trainer\n",
    "reload(icdmodelbart)\n",
    "reload(trainer)\n",
    "\n",
    "\n",
    "from icdmodelbart import ICDBartForPreTraining\n",
    "from trainer import Trainer \n",
    "import torch\n",
    "from transformers import BartConfig, BartModel, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "\n",
    "\n",
    "config = BartConfig()\n",
    "# Create an instance of the model\n",
    "model = ICDBartForPreTraining(config)\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=3,  # number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",  # evaluation strategy to adopt during training\n",
    "    logging_dir=\"./logs\",  # directory for storing logs\n",
    "    logging_steps=100,  # log every 100 steps\n",
    "    save_steps=100,  # save checkpoint every 100 steps\n",
    "    save_total_limit=3,  # limit the total amount of checkpoints\n",
    "    prediction_loss_only=True, # When enabled, only the prediction loss is calculated\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "# class my_model():\n",
    "#   # use this class to define your model\n",
    "#   pass\n",
    "\n",
    "# model = my_model()\n",
    "# loss_func = None\n",
    "# optimizer = None\n",
    "\n",
    "# def train_model_one_iter(model, loss_func, optimizer):\n",
    "#   pass\n",
    "\n",
    "# num_epoch = 10\n",
    "# # model training loop: it is better to print the training/validation losses during the training\n",
    "# for i in range(num_epoch):\n",
    "#   train_model_one_iter(model, loss_func, optimizer)\n",
    "#   train_loss, valid_loss = None, None\n",
    "#   print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# -- instantiating blank model \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#    -- import model from file first later import sinja's model from the box above\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mICDBartForPreTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# -- setting up training values \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "from icdmodelbart import ICDBartForPreTraining\n",
    "from trainer import Trainer\n",
    "\n",
    "# -- instantiating blank model \n",
    "#    -- import model from file first later import sinja's model from the box above\n",
    "model=ICDBartForPreTraining()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    test_collator=test_data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    test_datasets=test_datasets,\n",
    "    prediction_loss_only=False\n",
    ")\n",
    "\n",
    "# -- setting up training values \n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "if trainer.args.max_steps > 0:\n",
    "    t_total = trainer.args.max_steps\n",
    "    num_train_epochs = (\n",
    "        trainer.args.max_steps // (len(train_dataloader) // trainer.args.gradient_accumulation_steps) + 1\n",
    "    )\n",
    "else:\n",
    "    t_total = int(len(train_dataloader) // trainer.args.gradient_accumulation_steps * trainer.args.num_train_epochs)\n",
    "    num_train_epochs = trainer.args.num_train_epochs\n",
    "\n",
    "optimizer, scheduler = trainer.get_optimizers(num_training_steps=t_total)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  -- training loop \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Metrics Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainer.evaluate()` will return a dict containing:\n",
    "* the eval loss\n",
    "* the potential metrics computed from the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    eval_output = trainer.evaluate()\n",
    "\n",
    "    perplexity = math.exp(eval_output[\"eval_00_valid_loss\"])\n",
    "    eval_output[\"perplexity\"] = perplexity\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "\n",
    "    results.update(eval_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
    "  * Make assessment that the paper is reproducible or not.\n",
    "  * Explain why it is not reproducible if your results are kind negative.\n",
    "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
    "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    "  * What will you do in next phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the paper is missing large parts of relevant code, specifically functions from `dataset.py` such as the data collator functions and `prepare_dataset` function, it is difficult to reproduce. Moving forward, we plan to implement our own versions of these functions to use for running and training the models. To improve reproducibility of the paper, ideally the authors could publish their `dataset.py` with the completed working functions.\n",
    "\n",
    "**Plan for ablation studies:**\n",
    "\n",
    "The `DataTrainingArguments` class has flags pertaining to the data we provide as input to our model for training and evaluation, which we can use for analyzing each ablation and how it affects the model. \n",
    "\n",
    "1) ***Visit Masking:*** For this ablation, we will adjust the `mlm` flag and/or `do_poisoon_random_masking`. When set to True, the `mlm` argument will train with bart masking by natural visit, instead of to mask with poisoon_random lambda 4. Similarly, the `do_poisoon_random_masking` decides whether to mask with poisoon_random lambda 4 instead of by natural visit.\n",
    "\n",
    "2) ***Time Embedding:*** To test this ablation, we will adjust the `do_pos_emb` argument, which decides whether or not to do positional embedding, and/or the `do_date_visit` argument, which decides whether or not to treat each visit embedding as a date (time embedding implemented) or a sequence number (like BERT, time embedding not implemented).\n",
    "\n",
    "3) ***Encoder-Decoder vs Encoder Only:*** This ablation would be much more complex to do, since it will involve modifying the architecture of the model. Currently, the model consists of several forward-pass (unidirectional) encoder and decoder layers. To compare this with an encoder-only model, we would need to remove the use of the decoder layers and implement bidirectional encoder layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2VDXo5F4Frm"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can read and plot it here like the Scope of Reproducibility\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1. Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmVuzQ724HbO"
   },
   "source": [
    "# Feel free to add new sections"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
